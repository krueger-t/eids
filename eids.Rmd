--- 
title: "Einf√ºhrung in die Statistik"
author: "Tobias Krueger"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: book
bibliography: eids.bib
biblio-style: apalike
link-citations: yes
github-repo: krueger-t/eids
description: "This is the script of the course 'Einf√ºhrung in die Statistik' (in German) run at the Geography Department of Humboldt-Universit√§t zu Berlin."
---
--- 
title: "Einf√ºhrung in die Statistik"
author: "Tobias Krueger"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: book
bibliography: eids.bib
biblio-style: apalike
link-citations: yes
github-repo: krueger-t/eids
description: "This is the script of the course 'Einf√ºhrung in die Statistik' (in German) run at the Geography Department of Humboldt-Universit√§t zu Berlin."
---

# Vorwort {-}

Dies ist das Skript f√ºr den Kurs 'Einf√ºhrung in die Statistik' am Geographischen Institut der Humboldt-Universit√§t zu Berlin.

<!--chapter:end:index.Rmd-->


# Einf√ºhrung {#einfuehrung}

Placeholder


## Statistik im empirischen Forschungsprozess
## Warum Statistik?
## Organisatorisches {#orga}
## Mathematische Notation und Grundlagen
### Exponential- und Logarithmusfunktion
### Quadratische Funktion und Wurzelfunktion

<!--chapter:end:01-einfuehrung.Rmd-->


# Grundbegriffe und Datenerhebung {#begriffe}

Placeholder


## Statistische Grundbegriffe
## Datenerhebung
## Skalenniveaus

<!--chapter:end:02-begriffe.Rmd-->


# H√§ufigkeiten und Lageparameter {#haeufigkeit}

Placeholder


## Ziel der deskriptiven Statistik
## H√§ufigkeiten
## Lageparameter

<!--chapter:end:03-haeufigkeit.Rmd-->


# Streuungsparameter, Schiefe und W√∂lbung {#streuung}

Placeholder


## Streuungsparameter
## Schiefe und W√∂lbung von H√§ufigkeitsverteilungen

<!--chapter:end:04-streuung.Rmd-->


# Korrelationsanalyse {#korrelation}

Placeholder


## Nominalskalierte Merkmale: Kontingenztabelle und Chi-Quadrat Statistik
## Ordinalskalierte Merkmale: Rangkorrelationskoeffizient nach Spearman
## Metrische Merkmale: Scatterplot (Streudiagramm) und Korrelationskoeffizient nach Bravais-Pearson

<!--chapter:end:05-korrelation.Rmd-->

# Grundlagen der Wahrscheinlichkeitsrechnung {#wahrscheinlichkeit}



<!--chapter:end:06-wahrscheinlichkeit.Rmd-->

# Verteilungen {#verteilungen}



<!--chapter:end:07-verteilungen.Rmd-->

# Schaetzen von Verteilungsparametern {#schaetzen}



<!--chapter:end:08-schaetzen.Rmd-->

# T-Test {#ttest}



<!--chapter:end:09-ttest.Rmd-->

# F-Test {#ftest}



<!--chapter:end:10-ftest.Rmd-->

# Chi2- und Kolmogorow-Smirnow-Test {#chi2testkstest}



<!--chapter:end:11-chi2test_kstest.Rmd-->

# Lineare Regression {#regression}

## Warum lineare Regression?

Die Fragen, die wir mit linearer Regression beantworten m√∂chten, sind von der in Abbildung \@ref(fig:abb12-1) dargestellten Art: Kann man Ihre Reisezeit mit der Entfernung zu Ihrem Wohnort statistisch vorhersagen? (vgl. Kapitel \@ref(korrelation))

```{r, include=FALSE}
library(readxl)
library(dplyr)
library(bookdown)
library(ggplot2)
library(car)

# Daten laden
daten <- read_excel("data/Daten.xlsx", na = "-999")
daten <- as.data.frame(daten)
names(daten) <- c("distanz","zeit")

# umwandeln in km und redundante Dezimalstellen entfernen
daten <- mutate(daten, distanz = distanz/100)
daten <- round(daten, digits = 1)

```



```{r abb12-1, echo=TRUE, fig.align='center', fig.cap='Reisezeit in Abh√§ngigkeit zur Entfernung. Korrelationskoeffizient nach Bravais-Pearson <br>r~$x, y$~=0'}

ggplot(data = daten, 
       aes(x = distanz, y = zeit)) + 
  geom_point() + 
  xlab("Entfernung (km)") + 
  ylab("Reisezeit (min)")
```

Das **Ziel** ist, eine Gerade durch die Punktwolke zu legen, die den *Trend* beschreibt, so dass der Abstand der Punkte von der Geraden minimal ist.

Es geht um *2 Variablen* (Merkmale):

- die **abh√§ngige Variable** $y$ (im Bsp. Reisezeit)
- die **unabh√§ngige Variable** $x$ (im Bsp. Entfernung)

Die Variablen m√ºssen *metrisch* skaliert sein. Wir wollen das generelle Verhalten von $y$ mit $y$ beschreiben. Eine Gerade stellt dabei das einfachste lineare Modell dar.

## Definitionen

Im Falle einer einzigen unabh√§ngigen Variable ist die Gleichung des **linearen Models** wie folgt:

$$\begin{equation}
y_i = \beta_0 + \beta_1 \cdot x_i + \epsilon_i
(\#eq:linmodsingle)
\end{equation}$$

Wobei $i$ eine positive nat√ºrliche Zahl darstellt, also $i$ = 1,2,...,$n$

$y_i$ beschreibt den Wert der **abh√§ngigen Variable** f√ºr Datenpunkt $i$, und $x_i$ den Wert der **unabh√§ngigen Variable** f√ºr Datenpunkt $i$. Der Parameter $\beta_0$ beschreibt den **Achsenabschnitt** der Geraden, also wo die Gerade die y-Achse schneidet. Der Parameter $\beta_1$ beschreibt die **Steigung** der Geraden (ein Parameter). $\epsilon_i$ schlie√ülich stellt das **Residuum** (also den Fehler) f√ºr Datenpunkt $i$ dar (Abbildung \@ref(fig:abb12-2)).

```{r abb12-2, echo=FALSE, fig.align='center', fig.cap='Definitionen'}

knitr::include_graphics('figs/abb12-2.png')
```

## Beschreibung vs. Vorhersage

Der prim√§re Zweck einer Regressionsanalyse ist die Beschreibung (oder Erkl√§rung) der Daten im Sinne einer allgemeinen Beziehung, die sich auf die Population √ºbertragen l√§sst, aus der diese Daten entnommen werden. Da diese Beziehung eine Eigenschaft der Population ist, sollte diese demnach auch Vorhersagen erm√∂glichen. Hierbei ist jedoch Vorsicht geboten. Betrachten Sie das Verh√§ltnis zwischen Jahr und Weltrekordzeit f√ºr die in Abbildung \@ref(fig:mile) dargestellte Herrenmeile. Wenn, wie hier, die Zeit als Pr√§diktor dient, wird die Regression zu einer Form der Trendanalyse, die in diesem Fall eine Abnahme der Rekordzeit im Hinblick auf das Jahr angibt.

```{r echo=TRUE}
# Daten aus entferntem Repository laden
dat <- read.csv("https://raw.githubusercontent.com/avehtari/ROS-Examples/master/Mile/data/mile.csv", header=TRUE)
# lineares Modell and Daten aus 1. H√§lfte des 20. JH. anpassen
fit1 <- lm(seconds ~ year, data = dat[dat$year<1950,])
# Informationen zu Parametersch√§tzungen extrahieren
coef(summary(fit1))
# lineares Modell an kompletten Datensatz anpassen
fit2 <- lm(seconds ~ year, data = dat)
coef(summary(fit2))
```

```{r mile, echo=TRUE, fig.align='center', fig.cap= 'Links: Trend des Weltrekords der Herrenmeile in der ersten H√§lfte des 20. Jahrhunderts (Beschreibung). Mitte: Extrapolierung des Trends auf die zweite H√§lfte des 20. Jahrhunderts (Vorhersage). Rechts: Extrapolierung des Trends bis zum Jahr 2050 (l√§ngere Vorhersage). Quelle: @wainer2009', fig.show='hold', out.width='33%'}
# Modellanpassung f√ºr 1. H√§lfte des 20. JH. plotten
plot(dat$year[dat$year<1950], dat$seconds[dat$year<1950],
     xlim = c(1900, 2000), ylim = c(200, 260),
     pch = 19, type = 'p',
     xlab = "Jahr", ylab = "Weltrekord, Herrenmeile (Sekunden)")
abline(coef(fit1), lwd = 3, col = "red")
# Extrapolierung f√ºr 2. H√§lfte des 20. JH. plotten
plot(dat$year, dat$seconds,
     xlim = c(1900, 2000), ylim = c(200, 260),
     pch = 19, type = 'p',
     xlab = "Jahr", ylab = "Weltrekord, Herrenmeile (Sekunden)")
abline(coef(fit1), lwd = 3, col = "red")
# Modellanpassung f√ºr Gesamtdaten bis 2050 plotten
plot(dat$year, dat$seconds,
     xlim = c(1900, 2050), ylim = c(200, 260),
     pch = 19, type = 'p',
     xlab = "Jahr", ylab = "Weltrekord, Herrenmeile (Sekunden)")
abline(coef(fit2), lwd = 3, col = "red")
```

Der Weltrekord bei der Herrenmeile verbesserte sich in der ersten H√§lfte des 20. Jahrhunderts linear (Abbildung \@ref(fig:mile), links). Diese Hochrechnung bietet auch f√ºr die zweite H√§lfte eine bemerkenswert genaue Passung (Abbildung \@ref(fig:mile), Mitte). Wie lange kann sich der Weltrekord jedoch noch mit der gleichen Geschwindigkeit verbessern (Abbildung \@ref(fig:mile), rechts)? Dieses Beispiel zeigt deutlich die Anwendbarkeit von Regressionen f√ºr Vorhersagen innerhalb bestimmter Grenzen, kennzeichnet jedoch gleichzeitig die Begrenzung dieser einfachen Modelle f√ºr Fernvorhersagen (z.B. √ºber Zeit und Raum). Im Falle des Weltrekords w√ºrden wir erwarten, dass die Verbesserungsrate mit der Zeit abnimmt, d.h. dass sich der Weltrekord abflacht, was ein nichtlineares Modell erfordert.

## Ausblick: weiterf√ºhrende lineare Modelle

Wenn wir √ºber das lineare Modell sprechen, ist die abh√§ngige Variable immer metrisch, w√§hrend die unabh√§ngigen Variablen metrisch, nominal/ordinal oder gemischt sein k√∂nnen. Im Prinzip kann jede dieser Varianten mathematisch gleich behandelt werden, d.h. alle k√∂nnen z.B. mit der lm-Funktion in R analysiert werden. Allerdings sind historisch gesehen unterschiedliche Bezeichnungen f√ºr diese Varianten festgelegt worden, die hier erw√§hnt werden sollten, um Verwirrung zu vermeiden (Tabelle \@ref(tab:varianten1) und \@ref(tab:varianten2)).


| | Abh√§ngige Variable(n) metrisch / <br>unabh√§ngige Variable(n)... |
| :---: | :---: | :---: |
| ...metrisch | ...nominal/ ordinal | ...gemischt |
| Regression | Varianzanalyse<br>(ANOVA) | Kovarianz Analyse<br>(ANCOVA) |
Tabelle: (\#tab:varianten1) Historische Namen f√ºr die Varianten des linearen Modells, je nachdem, ob die unabh√§ngigen Variablen metrisch, nominal oder gemischt sind. Die abh√§ngige Variable ist immer nominal.


| | 1 unabh√§ngige Variable  | >1 unabh√§ngige Variable |
| :---: | :---: | :---: |
| **1 unabh√§ngige Variable** | Regression  | Multiple Regression |
| **>1 unabh√§ngige Variable** | Multivariate Regression |
Tabelle: (\#tab:varianten2) Historische Namen f√ºr die Regression, je nachdem, ob wir eine oder mehrere unabh√§ngige Variablen und eine oder mehrere abh√§ngige Variablen haben.


## Lineare Regression

Wie soll nun die Gerade durch die Punktwolke gelegt werden, d.h. welche Werte sollen Achsenabschnitt $\beta_0$ und Steigung $\beta_1$ annehmen?

Typischerweise werden Regressionsprobleme gel√∂st, d.h. die Linien in den Abbildungen \@ref(fig:abb12-1) und \@ref(fig:abb12-2) werden an die Daten angepasst, indem die Summe der quadratischen Fehler zwischen der Regressionsgeraden und den Datenpunkten minimiert wird (kleinste-Quadrate-Sch√§tzung). Diese Differenz wird auch als SSE bezeichnet (Sum of Squared Errors). Grafisch gesehen, probieren wir in Abbildung \@ref(fig:abb12-1) verschiedene Linien mit unterschiedlichen Schnittpunkten ($\beta_0$) und Steigungen ($\beta_1$) aus und w√§hlen diejenige, bei der die Summe √ºber alle vertikalen Abst√§nde zum Quadrat ($\epsilon_i$) am kleinsten ist. Mathematisch ist SSE definiert als:

$$\begin{equation}
SSE=\sum_{i=1}^{n}\left(\epsilon_i\right)^2=\sum_{i=1}^{n}\left(y_i-\left(\beta_0+\beta_1 \cdot x_i\right)\right)^2
(\#eq:sse)
\end{equation}$$

Das **Residuum**, also der Teil der Varianz, der durch das lineare Modell nicht erkl√§rt werden kann, wird durch die folgende Gleichung beschrieben: $\epsilon_i=y_i-\left(\beta_0+\beta_1 \cdot x_i\right)$

Im Falle der linearen Regression kann SSE analytisch minimiert werden, was z.B. bei nichtlinearen Modellen nicht der Fall ist. Analytisch finden wir das Minimum von SSE, wenn deren partielle Ableitungen in Bezug auf die beiden Modellparameter beide Null sind:  $\frac{\partial SSE}{\partial \beta_0}=0$ und $\frac{\partial SSE}{\partial \beta_1}=0$. Unter Anwendung der Definition von SEE aus Gleichung \@ref(eq:sse) beginnen wir also mit einem System von zwei Differentialgleichungen:

$$\begin{equation}
\frac{\partial SSE}{\partial \beta_0}=-2 \cdot \sum_{i=1}^{n}\left(y_i-\beta_0-\beta_1 \cdot x_i\right)=0
(\#eq:sseb0)
\end{equation}$$
$$\begin{equation}
\frac{\partial SSE}{\partial \beta_1}=-2 \cdot \sum_{i=1}^{n}x_i \cdot \left(y_i-\beta_0-\beta_1 \cdot x_i\right)=0
(\#eq:sseb1)
\end{equation}$$

Diese Ableitungen k√∂nnen wir anhand der **Summenregel** (wenn $ùë¶=ùë¢¬±ùë£$, dann
$ùë¶^‚Ä≤=ùë¢‚Ä≤¬±ùë£‚Ä≤$ ) und der **Kettenregel** (wenn $ùë¶=ùëì[ùëî(ùë•)]$ dann
$ùë¶^‚Ä≤=ùëì‚Ä≤[ùëî(ùë•)]*ùëî‚Ä≤(ùë•)$ ) l√∂sen.
Mit den Gleichungen \@ref(eq:sseb0) und \@ref(eq:sseb1) haben wir ein Gleichungssystem mit 2 Gleichungen und 2 Unbekannten, das wir eindeutig l√∂sen k√∂nnen.

Zuerst l√∂sen wir Gleichung \@ref(eq:sseb0) nach $\beta_0$ (nachdem wir durch -2 geteilt haben):

$$\begin{equation}
\sum_{i=1}^{n}y_i-n \cdot \beta_0-\beta_1 \cdot \sum_{i=1}^{n}x_i=0
(\#eq:b01)
\end{equation}$$
$$\begin{equation}
n \cdot \hat\beta_0=\sum_{i=1}^{n}y_i-\hat\beta_1 \cdot \sum_{i=1}^{n}x_i
(\#eq:b02)
\end{equation}$$
$$\begin{equation}
\hat\beta_0=\bar{y}-\hat\beta_1 \cdot \bar{x}
(\#eq:b03)
\end{equation}$$

Beachten Sie, dass wir irgendwann $\beta_0$ in $\hat\beta_0$ und $\beta_1$ in $\hat\beta_1$ umbenannt haben, um diese als **Sch√§tzwerte** zu bezeichnen. Die Parameter-Notation war bisher allgemein, aber wenn wir uns den tats√§chlichen numerischen Werten f√ºr die vorliegenden Daten n√§hern, verwenden wir das "Dach"-Symbol, um zu signalisieren, dass wir jetzt Sch√§tzungen dieser allgemeinen Parameter f√ºr einen bestimmten Datensatz berechnen.

Als zweites, setzen wir Gleichung \@ref(eq:b03) in Gleichung \@ref(eq:sseb1) ein (nachdem wir durch -2 geteilt haben):

$$\begin{equation}
\sum_{i=1}^{n}\left(x_i \cdot y_i-\beta_0 \cdot x_i-\beta_1 \cdot x_i^2\right)=0
(\#eq:insert1)
\end{equation}$$
$$\begin{equation}
\sum_{i=1}^{n}\left(x_i \cdot y_i-\bar{y} \cdot x_i+\hat\beta_1 \cdot \bar{x} \cdot x_i-\hat\beta_1 \cdot x_i^2\right)=0
(\#eq:insert2)
\end{equation}$$

Drittens, l√∂sen wir Gleichung Third \@ref(eq:insert2) nach $\beta_1$:

$$\begin{equation}
\sum_{i=1}^{n}\left(x_i \cdot y_i-\bar{y} \cdot x_i\right)-\hat\beta_1 \cdot \sum_{i=1}^{n}\left(x_i^2-\bar{x} \cdot x_i\right)=0
(\#eq:b11)
\end{equation}$$
$$\begin{equation}
\hat\beta_1=\frac{\sum_{i=1}^{n}\left(x_i \cdot y_i-\bar{y} \cdot x_i\right)}{\sum_{i=1}^{n}\left(x_i^2-\bar{x} \cdot x_i\right)}
(\#eq:b12)
\end{equation}$$

√úber eine Reihe von Schritten, die ich hier √ºberspringe, gelangen wir zum folgenden Schritt:

$$\begin{equation}
\hat\beta_1=\frac{SSXY}{SSX}
(\#eq:b13)
\end{equation}$$
Wo $SSX=\sum_{i=1}^{n}\left(x_i-\bar{x}\right)^2$ und $SSXY=\sum_{i=1}^{n}\left(x_i-\bar{x}\right) \cdot \left(y_i-\bar{y}\right)$. Beachten Sie, dass entsprechend gilt: $SSY=\sum_{i=1}^{n}\left(y_i-\bar{y}\right)^2$. Dies ist eine exakte L√∂sung f√ºr $\hat\beta_1$.

Wir setzen nun Gleichung \@ref(eq:b13) in Gleichung \@ref(eq:sse) ein und haben eine exakte L√∂sung f√ºr $\hat\beta_0$.


## Statistische Signifikanz

Wenn wir Sch√§tzungen f√ºr die Regressionsparameter haben, m√ºssen wir uns fragen, ob diese Sch√§tzungen statistisch signifikant sind oder ob sie entstanden sind durch Zufall aus dem (angenommenen) Zufallsprozess der Stichprobenziehung der Daten. Wir tun dies √ºber die **Varianzanalyse (ANOVA)**, die mit der Erstellung der ANOVA-Tabelle (Tabelle \@ref(tab:anova)) beginnt. Dies geschieht oft im Hintergrund in Software wie R und wird eigentlich nicht so viel betrachtet.


| Varianzquelle | Quadrat-<br>summe | Freiheitsgrad ($df$) | Varianz | F-Statistic ($F_s$) | p-Wert |
| :---: | :---: | :---: | :---: | :---: | :---: |
| Regression | $SSR=\\SSY-SSE$ | $1$ | $\frac{SSR}{df_{SSR}}$ | $\frac{\frac{SSR}{df_{SSR}}}{s^2}$ | $1-F\left(F_s,1,n-2\right)$ |
| Fehler | $SSE$ | $n-2$ | $\frac{SSE}{df_{SSE}}=s^2$ | | |
| Gesamt | $SSY$ | $n-1$ | | | |
Tabelle: (\#tab:anova) ANOVA-Tabelle f√ºr lineare Regression.

In der zweiten Spalte von Tabelle (\#tab:anova) ist $SSY=\sum_{i=1}^{n}\left(y_i-\bar{y}\right)^2$ ein Ma√ü f√ºr die Gesamtvarianz der Daten, d.h. wie stark die Datenpunkte um den Gesamtmittelwert variieren (Abbildung \@ref(fig:ssysse), links). $SSE=\sum_{i=1}^{n}\left(\epsilon_i\right)^2=\sum_{i=1}^{n}\left(y_i-\left(\beta_0+\beta_1 \cdot x_i\right)\right)^2$ ist ein Ma√ü f√ºr die Fehlervarianz, d.h. wie stark die Datenpunkte um die Regressionsgerade variieren (Abbildung \@ref(fig:ssysse), rechts). Dies ist die Varianz, die nicht durch das Modell erkl√§rt wird. $SSR=SSY-SSE$ ist folglich ein Ma√ü f√ºr die Varianz, die durch das Modell erkl√§rt wird.


```{r ssysse, echo=FALSE, fig.align='center', fig.cap='Variation der Datenpunkte um den Mittelwert, gekennzeichnet mit $SSY$ (links) und um die Regressionsgerade, gekennzeichnet mit $SSE$ (rechts).', fig.show='hold', out.width='50%'}
knitr::include_graphics(c('figs/ssy.jpg','figs/sse.jpg'))
```

In der dritten Spalte der Tabelle \@ref(tab:anova) sind die sogenannten **Freiheitsgrade** der drei Varianzterme aufgef√ºhrt, die als die Anzahl der freien Parameter f√ºr den jeweiligen Term verstanden werden k√∂nnen, die durch den (angenommenen) Zufallsprozess der Stichprobenziehung der Daten kontrolliert wird (vgl. Kapitel \@ref(streuung)). Die Freiheitsgrade werden verwendet, um die Varianzterme in der vierten Spalte der Tabelle \@ref(tab:anova) zu normalisieren, wobei $s^2$ die **Fehlervarianz** genannt wird.

In der f√ºnften Spalte der Tabelle \@ref(tab:anova) finden wir das Verh√§ltnis von zwei Varianzen; Regressionsvarianz √ºber Fehlervarianz. Nat√ºrlich wollen wir f√ºr eine signifikante Regression, dass die (durch das Modell erkl√§rte) Regressionsvarianz viel gr√∂√üer ist als die (durch das Modell nicht erkl√§rte) Fehlervarianz. Dies ist ein **F-Test** Problem, bei dem getestet wird, ob sich die durch das Modell erkl√§rte Varianz signifikant von der durch das Modell nicht erkl√§rten Varianz unterscheidet. Das Verh√§ltnis der beiden Varianzen dient als F-Statistik ($F_s$). 

Die sechste Spalte der Tabelle \@ref(tab:anova) zeigt dann den **p-Wert** des F-Tests, d.h. die Wahrscheinlichkeit, $F_s$ oder einen gr√∂√üeren Wert (d.h. ein noch besseres Modell) zuf√§llig zu erhalten, wenn die Nullhypothese ($H_0$) wahr ist. $H_0$ bedeutet hier, dass die beiden Varianzen gleich sind. $F_s$ folgt einer F-Verteilung mit den Parametern $1$ und $n-2$ unter der Nullhypothese (Abbildung \@ref(abb:fcdf)). Die rote Linie in Abbildung \@ref(fig:fcdf) markiert einen bestimmten Wert von $F_s$ (zwischen 10 und 11) und den entsprechenden Wert der kumulativen Verteilungsfunktion der F-Verteilung ($F\left(F_s,1,n-2\right)$). Der p-Wert ist $\Pr\left(Z\geq F_s\right)=1-F\left(F_s,1,n-2\right)$, und beschreibt die Wahrscheinlichkeit, dieses oder ein gr√∂√üeres Varianzverh√§ltnis zuf√§llig (aufgrund des Zufallsstichprobenverfahrens) zu erhalten, selbst wenn die beiden Varianzen tats√§chlich gleich sind.


```{r fcdf, echo=FALSE, fig.align='center', fig.cap='Kumulative Verteilungsfunktion (CDF) der F-Verteilung der F-Statistik of the F statistic ($F_s$) mit bestimmtem Wert und dem entsprechenden rot markierten Wert der CDF.', out.width='80%'}
knitr::include_graphics('figs/cdf_f.jpg')
```


Hier ist der p-Wert sehr klein, und daher k√∂nnen wir schlussfolgern, dass die Regression signifikant ist (Tabelle \@ref(tab:reisezeit)).

| Varianzquelle | Quadrat-<br>summe | Freiheitsgrad ($df$) | Varianz | F-Statistik <br>($F_s$) | p-Wert |
| :---: | :---: | :---: | :---: | :---: | :---: |
| Regression | 26253 | 1 | 26253 | 275,32 | 8,12e-30 |
| Fehler | 9059 | 95 | 95,36 | | |
| Gesamt | 35312 | 96 | | | |
Tabelle: (\#tab:reisezeit) Werte f√ºr unser Beispiel der Reisezeit.


## Konfidenzintervalle auf Signifikanz der Parameter

Nachdem wir die statistische Signifikanz der Regression festgestellt haben, sollten wir uns die Unsicherheit um die Parametersch√§tzungen ansehen. In der klassischen linearen Regression wird diese Unsicherheit so konzeptualisiert, dass sie sich rein aus dem Zufallsstichprobenverfahren ergibt; die vorliegenden Daten sind nur eine M√∂glichkeit von vielen, und in jedem alternativen Fall w√§ren die Parametersch√§tzungen etwas anders ausgefallen. Das lineare Modell selbst wird als korrekt angenommen.

Der erste Schritt, um festzustellen, wie zuversichtlich wir sein sollten, dass die Parametersch√§tzungen korrekt sind, ist die Berechnung von **Standardfehlern**. F√ºr $\hat\beta_0$ geht das wie folgt:

$$\begin{equation}
s_{\hat\beta_0}=\sqrt{\frac{\sum_{i=1}^{n}x_i^2}{n} \cdot \frac{s^2}{SSX}}
(\#eq:seb0)
\end{equation}$$

Wenn wir diese Formel in ihre einzelnen Teile zerlegen, sehen wir: Je mehr Datenpunkte $n$ wir haben, desto kleiner ist der Standardfehler, d.h. desto mehr Vertrauen haben wir in die Sch√§tzung. Au√üerdem gilt, je gr√∂√üer die Abweichung in $x$ ($SSX$), desto kleiner der Standardfehler. Beide Effekte machen intuitiv Sinn: je mehr Datenpunkte wir haben und je mehr M√∂glichkeiten f√ºr $x$ wir abgedeckt haben, desto sicherer k√∂nnen wir sein, dass wir in unserer Zufallsstichprobe nicht viel verpasst haben. Umgekehrt gilt: je gr√∂√üer die Fehlervarianz $s^2$, d.h. je kleiner die Erkl√§rungskraft unseres Modells, desto gr√∂√üer der Standardfehler. Und je mehr $x$-Datenpunkte wir von Null entfernt haben, d.h. je gr√∂√üer $\sum_{i=1}^{n}x_i^2$, desto geringer ist unser Vertrauen in den Schnittpunkt (wo $x=0$) und damit steigt der Standardfehler.

Der Standardfehler f√ºr $\hat\beta_1$ ist:

$$\begin{equation}
s_{\hat\beta_1}=\sqrt{\frac{s^2}{SSX}}
(\#eq:seb1)
\end{equation}$$

Die gleiche Interpretation gilt, au√üer dass es keinen Einfluss der Gr√∂√üe der $x$-Datenpunkte gibt.

Wir k√∂nnen auch einen Standardfehler f√ºr neue Vorhersagen $\hat y$ festlegen f√ºr gegebene $\hat x$:

$$\begin{equation}
s_{\hat y}=\sqrt{s^2 \cdot \left(\frac{1}{n}+\frac{\left(\hat x-\bar x\right)^2}{SSX}\right)}
(\#eq:sey)
\end{equation}$$

Dieselbe Interpretation gilt auch hier, nur dass jetzt ein zus√§tzlicher Begriff $\left(\hat x-\bar x\right)^2$ hinzugef√ºgt wurde, der besagt, je weiter der neue $x$-Wert vom Zentrum der urspr√ºnglichen Daten (den Trainings- oder Kalibrierdaten) entfernt ist, desto gr√∂√üer ist der Standardfehler der neuen Vorhersage, d.h. desto geringer ist die Zuversicht, dass sie korrekt ist.

Anmerkung: Die Formel f√ºr die Standardfehler ergibt sich aus den grundlegenden Annahmen der linearen Regression, auf die weiter unten eingegangen wird. Dies kann mathematisch dargestellt werden, wird hier aber ausgelassen.

Aus den Standardfehlern k√∂nnen wir **Konfidenzintervalle** f√ºr die Parametersch√§tzungen wie folgt berechnen:

$$\begin{equation}
\Pr\left(\hat\beta_0-t_{n-2;0.975} \cdot s_{\hat\beta_0}\leq \beta_0\leq \hat\beta_0+t_{n-2;0.975} \cdot s_{\hat\beta_0}\right)=0.95
(\#eq:cib01)
\end{equation}$$

Das Symbol $\Pr(\cdot)$ bedeutet Wahrscheinlichkeit. Das Symbol $t_{n-2;0.975}$ steht f√ºr das 0.975-Perzentil der t-Verteilung mit $n-2$ Freiheitsgraden. Die Gleichung \@ref(eq:cib01) ist das zentrale 95%-Konfidenzintervall, das als die Grenzen definiert ist, in denen der wahre Parameter, hier $\beta_0$, mit einer Wahrscheinlichkeit von 0,95 liegt. Wir k√∂nnen das Intervall wie folgt schreiben:

$$\begin{equation}
CI=\left[\hat\beta_0-t_{n-2;0.975} \cdot s_{\hat\beta_0};\hat\beta_0+t_{n-2;0.975} \cdot s_{\hat\beta_0}\right]
(\#eq:cib02)
\end{equation}$$

Wie zu sehen ist, liegt das Konfidenzintervall symmetrisch um den Parametersch√§tzwert $\hat\beta_0$ und ergibt sich aus einer t-Verteilung mit dem Parameter $n-2$, dessen Breite durch den Standardfehler $s_{\hat\beta_0}$ moduliert wird. Beachten Sie, dass die Breite der t-Verteilung ebenfalls durch den Stichprobenumfang kontrolliert wird und mit zunehmendem $n$ immer schmaler wird.

Die gleichen Formeln gelten f√ºr $\beta_1$ und $y$:

$$\begin{equation}
\Pr\left(\hat\beta_1-t_{n-2;0.975} \cdot s_{\hat\beta_1}\leq \beta_1\leq \hat\beta_1+t_{n-2;0.975} \cdot s_{\hat\beta_1}\right)=0.95
(\#eq:cib1)
\end{equation}$$
$$\begin{equation}
\Pr\left(\hat y-t_{n-2;0.975} \cdot s_{\hat y}\leq y\leq \hat y+t_{n-2;0.975} \cdot s_{\hat y}\right)=0.95
(\#eq:ciy)
\end{equation}$$

Wie bei den p-Werten m√ºssen wir uns auch hier √ºber die Bedeutung der Wahrscheinlichkeit klar werden, die in der klassischen Statistik auf dem **wiederholten Stichprobenprinzip** beruht. Die Bedeutung des 95%-Konfidenzintervalls ist dann, dass bei einer angenommenen unendlichen Anzahl von Regressionsexperimenten das 95%-Konfidenzintervall in 95% der F√§lle den wahren Parameterwert erfasst. Auch hier handelt es sich **nicht** um die Wahrscheinlichkeit, dass der wahre Parameterwert f√ºr ein beliebiges Experiment innerhalb des Konfidenzintervalls liegt!

Die Formeln f√ºr die Konfidenzintervalle (Gleichungen \@ref(eq:cib01), \@ref(eq:cib1) und \@ref(eq:ciy)) ergeben sich aus den Grundannahmen der linearen Regression; die Residuen sind **unabh√§ngig identisch verteilt (u.i.v.)** gem√§√ü einer **Normalverteilung** und **das lineare Modell ist korrekt**. Dann l√§sst sich mathematisch zeigen, dass $\frac{\hat\beta_0-\beta_0}{s_{\hat\beta_0}}$, $\frac{\hat\beta_1-\beta_1}{s_{\hat\beta_1}}$ und $\frac{\hat y-y}{s_{\hat y}}$ $t_{n-2}$-verteilt sind (t-Verteilung mit $n-2$ Freiheitsgraden). Da das zentrale 95%-Konfidenzintervall einer willk√ºrlich $t_{n-2}$-verteilten Zufallsvariablen $Z$ $\Pr\left(-t_{n-2;0.975}\leq Z\leq t_{n-2;0. 975}\right)=0,95$ (Abbildung \@ref(fig:tpdfcdf)), k√∂nnen wir jeden der oben genannten drei Terme durch $Z$ ersetzen und neu anordnen, um zu den Gleichungen \@ref(eq:cib01), \@ref(eq:cib1) und \@ref(eq:ciy) zu gelangen.

```{r tpdfcdf, echo=FALSE, fig.align='center', fig.cap='Links: Wahrscheinlichkeitsdichtefunktion (PDF) einer t-verteilten Zufallsvariablen $Z$, wobei das zentrale 95%-Konfidenzintervall rot markiert ist. 95% der PDF liegen zwischen den beiden Schranken, 2,5% liegen links von der unteren Schranke und 2,5% rechts von der oberen Schranke. Rechts: Kumulative Verteilungsfunktion (CDF) der gleichen t-verteilten Zufallsvariablen $Z$. Die obere Grenze des 95%-Konfidenzintervalls ist als $t_{n-2;0,975}$ definiert, d.h. das 0,975-Perzentil der Verteilung, w√§hrend die untere Grenze als $t_{n-2;0,025}$ definiert ist, was aufgrund der Symmetrie der Verteilung $-t_{n-2;0,975}$ entspricht.', fig.show='hold', out.width='50%'}
knitr::include_graphics(c('figs/pdf_t.jpg','figs/cdf_t.jpg'))
```

Die t-Verteilungseigenschaft der Parametersch√§tzungen kann weiter ausgenutzt werden, um jede Parametersch√§tzung separat auf ihre statistische Signifikanz zu testen. Dies wird besonders wichtig bei multiplen Regressionsproblemen, bei denen wir mehr als einen m√∂glichen Pr√§diktor haben, von denen nicht alle einen statistisch signifikanten Effekt haben werden. Die **Signifikanz der Parametersch√§tzungen** wird durch einen **t-Test** bestimmt. Die **NullHypothese** ist, dass die wahren Parameter gleich Null sind, d.h. die Parametersch√§tzungen sind nicht signifikant:

$$\begin{equation}
H_0:\beta_0=0
(\#eq:h0b0)
\end{equation}$$
$$\begin{equation}
H_0:\beta_1=0
(\#eq:h0b1)
\end{equation}$$

Diese Hypothese wird gegen die **Alternativhypothese** getestet, dass die wahren Parameter ungleich Null sind, d.h. dass die Parametersch√§tzungen signifikant sind:

$$\begin{equation}
H_1:\beta_0\neq 0
(\#eq:h1b0)
\end{equation}$$
$$\begin{equation}
H_1:\beta_1\neq 0
(\#eq:h1b1)
\end{equation}$$

Die statistischen Tests sind:

$$\begin{equation}
t_s=\frac{\hat\beta_0-0}{s_{\hat\beta_0}}\sim t_{n-2}
(\#eq:tsb0)
\end{equation}$$
$$\begin{equation}
t_s=\frac{\hat\beta_1-0}{s_{\hat\beta_1}}\sim t_{n-2}
(\#eq:tsb1)
\end{equation}$$

Das "Tilde"-Symbol ($\sim$) bedeutet, dass die Teststatistik einer bestimmten Verteilung folgt, hier der t-Verteilung. Dies ergibt sich wiederum aus den oben erw√§hnten Regressionsannahmen. Die Annahmen sind die gleichen wie beim √ºblichen t-Test der Mittelwerte, au√üer dass im Fall der linearen Regression die Residuen als u.i.v. normal angenommen werden, w√§hrend im Fall der Mittelwerte die tats√§chlichen Datenpunkte $y$ als **u.i.v. normal** angenommen werden.

Analog zum √ºblichen 2-seitigen t-Test ist der p-Wert definiert als:

$$\begin{equation}
2 \cdot \Pr\left(t>|t_s|\right)=2 \cdot \left(1-F_t\left(|t_s|\right)\right)
(\#eq:pv)
\end{equation}$$

Das Symbol $F_t\left(|t_s|\right)$ bezeichnet den Wert der kumulativen Verteilungsfunktion der t-Verteilung an der Stelle des absoluten Wertes der Teststatistik ($|t_s|$, Abbildung \@ref(fig:tc)). Mit einem Signifikanzniveau von z.B. $\alpha=0,05$ gelangen wir zu einem kritischen Wert der Teststatistik $t_c=t_{n-2;0,975}$, bei dessen √úberschreitung wir die Nullhypothese verwerfen und die Parametersch√§tzungen als signifikant bezeichnen (Abbildung \@ref(fig:tc)).

```{r tc, echo=FALSE, fig.align='center', fig.cap='Schema des t-Tests der Signifikanz von Parametersch√§tzungen. Die Teststatistik folgt einer t-Verteilung unter der Nullhypothese. Der tats√§chliche Wert der Teststatistik $t_s$ ist blau markiert und wird f√ºr den 2-seitigen Test bei Null gespiegelt. Der kritische Wert der Teststatistik $t_c$, den wir von einem Signifikanzniveau von $\\alpha=0.05$ erhalten, ist rot markiert; auch dieser wird f√ºr den 2-seitigen Test gespiegelt. Wir lehnen die Nullhypothese ab, wenn $|t_s|>t_c$, d.h. f√ºr Werte von $t_s$ unter $-t_c$ und √ºber $t_c$, und nennen diese Parametersch√§tzung dann signifikant. Wir behalten die Nullhypothese if $|t_s|\\leq t_c$ bei, d.h. f√ºr Werte von $t_s$ zwischen $-t_c$ und $t_c$, und nennen diesen Parametersch√§tzwert dann (vorl√§ufig) unbedeutend. In dem gezeigten Beispiel ist die Parametersch√§tzung unbedeutend.', out.width='80%'}
knitr::include_graphics('figs/tc.jpg')
```

## G√ºte der Modellanpassung

Der letzte Schritt bei der Regressionsanalyse ist die Beurteilung der G√ºte der Modellanpassung. In erster Linie kann dies durch das **Bestimmtheitsma√ü** ($r^2$) erfolgen, welches als der Anteil der Variation (in y-Richtung) definiert ist, der durch das Modell erkl√§rt wird. Das Bestimmtheitsma√ü ist der Korrelationskoeffizient nach Bravais-Pearson zum Quadrat (vgl. Kapitel \@ref(korrelation)).

$$\begin{equation}
r^2=\frac{SSY-SSE}{SSY}=1-\frac{SSE}{SSY}
(\#eq:r2)
\end{equation}$$

Wie man sehen kann, wenn das Modell nicht mehr Variation als die Gesamtvariation um den Mittelwert herum erkl√§rt, d.h. $SSE=SSY$, dann $r^2=0$. Umgekehrt, wenn das Modell perfekt zu den Daten passt, d.h. $SSE=0$, dann ist $r^2=1$. Jeder Wert dazwischen stellt unterschiedliche Grade der Anpassungsg√ºte dar. Dies kann wiederum mit Abbildung \@ref(fig:ssysse) veranschaulicht werden, wobei der linke Teil $SSY$ und der rechte Teil $SSE$ bedeuten.

Wenn es darum geht, Modelle unterschiedlicher Komplexit√§t (d.h. mit mehr oder weniger Parametern) mit $r^2$ zu vergleichen, dann ist es sinnvoll, die Metrik durch die Anzahl der Modellparameter zu bestrafen, da komplexere Modelle (mehr Parameter) automatisch zu besseren Anpassungen f√ºhren, einfach aufgrund der gr√∂√üeren Freiheitsgrade, die komplexere Modelle bei der Anpassung der Daten haben. Dies f√ºhrt zum **korregierten $r^2$**:

$$\begin{equation}
\bar r^2=1-\frac{\frac{SSE}{df_{SSE}}}{\frac{SSY}{df_{SSY}}}=1-\frac{SSE}{SSY} \cdot \frac{df_{SSY}}{df_{SSE}}
(\#eq:adjr2)
\end{equation}$$

Das Bestimmungsma√ü allein reicht jedoch nicht aus, um die Anpassungsg√ºte zu beurteilen. Wir f√ºhren nun eine lineare Regression mit dem Datensatz zu Reisezeiten durch (Abbildung \@ref(fig:abb12-3))


```{r echo=TRUE}

# lineare Regression der Daten ausf√ºhren
lm_dat <- lm(formula = zeit ~ distanz, data = daten)
coef(summary(lm_dat))

```

In diese zusammenfassenden Tabelle steht "(Intercept)" f√ºr $\beta_0$, w√§hrend "distanz" f√ºr $\beta_1$ steht. Die Spalte "Estimate" gibt $\hat\beta_0$ und $\hat\beta_1$ an, die Spalte "Std. Error" gibt $s_{\hat\beta_0}$ und $s_{\hat\beta_1}$ an, die Spalte "t value" gibt das Individuum $t_s$ an und die Spalte "Pr(>|t|)" gibt den jeweiligen p-Wert an.


```{r abb12-3, echo=TRUE, fig.align='center', fig.cap='Lineares Modell der Reisezeit in Abh√§ngigkeit zur Entfernung'}

# Daten mit Regressionslinie plotten
ggplot(data = daten, 
       aes(x = distanz, y = zeit)) + 
  geom_point() + 
  xlab("Entfernung (km)") + 
  ylab("Reisezeit (min)") +
  geom_abline(intercept = 23.5475832, slope = 0.1931053, col = "red")
```

Das Bestimmtheitsma√ü ist unempfindlich gegen√ºber systematischen Abweichungen von der Regressionsgeraden. Aber wir k√∂nnen diese Unzul√§nglichkeiten des Modells erkennen, indem wir ganz allgemein eine **Residualdiagnostik** durchf√ºhren, die die **Modellannahmen √ºberpr√ºft**.

Folgende Annahmen ergeben sich aus der **Maximum-Likelihood-Theorie** (vgl. Sch√§tzen von Verteilungsparametern, Kapitel \@ref(schaetzen)):

- Die Residuen sind **unabh√§ngig**, in diesem Fall gibt es keine serielle Korrelation in der Residuengraphik - dies kann mit dem Durbin-Watson-Test getestet werden
- Die Residuen sind **normal verteilt** - dies kann visuell mit Hilfe der Quantil-Quantil-Darstellung (QQ-PLot) und dem Residuen-Histogramm/Boxplot beurteilt werden und kann mit dem Kolmogorov-Smirnov-Test und dem Shapiro-Wilk-Test getestet werden.
- Die Varianz ist f√ºr alle Residuen konstant (= die Residuen sind **homoskedastisch**), d.h. es erfolgt keine "Auff√§cherung" der Residuen

Sind diese Annahmen nicht erf√ºllt, k√∂nnen wir auf Datentransformation, gewichtete Regression oder allgemeine lineare Modelle zur√ºckgreifen (Letzteres ist die bevorzugte Option und wird im Master *Global Change Geography* unterrichtet).


Eine erste n√ºtzliche diagnostische Darstellung ist die der Residuen in Serie, d.h. nach Index $i$, um zu sehen, ob es ein Muster aufgrund des Datenerfassungsprozesses gibt (Abbildung \@ref(fig:abb12-4)). Dieser Datensatz zeigt kein erkennbares Muster an, was f√ºr eine Unabh√§ngigkeit der Residuen spricht.

```{r abb12-4, echo=TRUE, fig.align='center', fig.cap='Reisezeit. Darstellung ist die der Residuen in Serie, d.h. nach Index $i$', fig.show='hold', out.width='50%'}
# Residuen gegen Index plotten
plot(residuals(lm_dat), pch = 19, type = 'p') +
abline(h = 0, lwd = 3, col = "red")

```

Wir sollten auch die Residuen nach dem vorhergesagten Wert von $y$ plotten, um zu sehen, ob es ein Muster als Funktion der Gr√∂√üe gibt (Abbildung \@ref(fig:abb12-5)). Die Graphik deutet auf eine leichte Verzerrung der Punkte nach links an. Dies k√∂nnte auf eine Nichterf√ºllung der Unabh√§ngigkeit und Homoskedastizit√§t hindeuten.

```{r abb12-5, echo=TRUE, fig.align='center', fig.cap='Reisezeit. Darstellung der Residuen nach dem vorausgesagten Wert von $y$.', fig.show='hold', out.width='50%'}

# Residuen gegen vorausgesagten Wert von y plotten
plot(fitted.values(lm_dat),residuals(lm_dat), pch = 19, type = 'p') +
abline(h = 0, lwd = 3, col = "red")
```

Die Annahme, dass die Residuen normalverteilt sind, kann anhand des QQ-Plots beurteilt werden (Abbildung \@ref(fig:abb12-6)).

```{r abb12-6, echo=TRUE, fig.align='center', fig.cap='Reisezeit. Quantil-Quantil-Darstellung (QQ-Plot) der Residuen.', fig.show='hold', out.width='50%'}

qqnorm(residuals(lm_dat))
qqline(residuals(lm_dat))

```
Im QQ-Plot stellt jeder Datenpunkt ein bestimmtes Quantil der empirischen Verteilung dar. Dieses Quantil (nach Standardisierung) wird gegen den Wert dieses Quantils geplotted (vertikale Achse), der unter einer Standard-Normalverteilung (horizontale Achse) erwartet wird. Die resultierenden Formen sagen etwas √ºber die Verteilung der Residuen aus (Abbildung \@ref(abb:qq)). Im Falle einer Normalverteilung zum Beispiel, fallen alle Residuen auf eine gerade Linie. In unserem Fall deutet der QQ-Plot eine leichte Verzerrung der Residuen nach links ("skewed to the left") an.

```{r qq, echo=FALSE, fig.align='center', fig.cap='Charakteristische Formen des QQ-Plots und was sie f√ºr die Residuen bedeuten. Quelle: https://condor.depaul.edu/sjost/it223/documents/normal-plot.htm.', out.width='80%'}
knitr::include_graphics('figs/qq.gif')
```

Das Histogramm der Residuen deutet ebenfalls auf eine leichte Verzerrung nach links oder zu niedrideren Werten hin (Abbildung \@ref(fig:abb12-7)).

```{r abb12-7, echo=TRUE, fig.align='center', fig.cap='Reisezeit. Histogramm der Residuen.', fig.show='hold', out.width='50%'}

# Histogramm der Residuen plotten
hist(residuals(lm_dat))

```

<!--chapter:end:12-regression.Rmd-->

`r if (knitr:::is_html_output()) '
# Literatur {-}
'`

<!--chapter:end:13-refs.Rmd-->

