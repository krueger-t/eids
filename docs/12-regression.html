<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Kapitel 12 Lineare Regression | Einführung in die Statistik</title>
  <meta name="description" content="This is the script of the course ‘Einführung in die Statistik’ (in German) run at the Geography Department of Humboldt-Universität zu Berlin." />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Kapitel 12 Lineare Regression | Einführung in die Statistik" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is the script of the course ‘Einführung in die Statistik’ (in German) run at the Geography Department of Humboldt-Universität zu Berlin." />
  <meta name="github-repo" content="krueger-t/eids" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Kapitel 12 Lineare Regression | Einführung in die Statistik" />
  
  <meta name="twitter:description" content="This is the script of the course ‘Einführung in die Statistik’ (in German) run at the Geography Department of Humboldt-Universität zu Berlin." />
  

<meta name="author" content="Tobias Krueger" />


<meta name="date" content="2020-11-20" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="11-chi2test_kstest.html"/>
<link rel="next" href="13-refs.html"/>
<script src="libs/jquery/jquery.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block/empty-anchor.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Einführung in die Statistik</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Vorwort</a></li>
<li class="part"><span><b>Grundlagen</b></span></li>
<li class="chapter" data-level="1" data-path="01-einfuehrung.html"><a href="01-einfuehrung.html"><i class="fa fa-check"></i><b>1</b> Einführung</a></li>
<li class="chapter" data-level="2" data-path="02-begriffe.html"><a href="02-begriffe.html"><i class="fa fa-check"></i><b>2</b> Grundbegriffe und Datenerhebung</a></li>
<li class="part"><span><b>Deskriptive Statistik</b></span></li>
<li class="chapter" data-level="3" data-path="03-haeufigkeit.html"><a href="03-haeufigkeit.html"><i class="fa fa-check"></i><b>3</b> Häufigkeiten und Lageparameter</a></li>
<li class="chapter" data-level="4" data-path="04-streuung.html"><a href="04-streuung.html"><i class="fa fa-check"></i><b>4</b> Streuungsparameter, Schiefe und Wölbung</a></li>
<li class="chapter" data-level="5" data-path="05-korrelation.html"><a href="05-korrelation.html"><i class="fa fa-check"></i><b>5</b> Korrelationsanalyse</a></li>
<li class="part"><span><b>Wahrscheinlichkeitstheorie</b></span></li>
<li class="chapter" data-level="6" data-path="06-wahrscheinlichkeit.html"><a href="06-wahrscheinlichkeit.html"><i class="fa fa-check"></i><b>6</b> Grundlagen der Wahrscheinlichkeitsrechnung</a></li>
<li class="chapter" data-level="7" data-path="07-verteilungen.html"><a href="07-verteilungen.html"><i class="fa fa-check"></i><b>7</b> Verteilungen</a></li>
<li class="part"><span><b>Induktive Statistik</b></span></li>
<li class="chapter" data-level="8" data-path="08-schaetzen.html"><a href="08-schaetzen.html"><i class="fa fa-check"></i><b>8</b> Schätzen von Verteilungsparametern</a></li>
<li class="chapter" data-level="9" data-path="09-ttest.html"><a href="09-ttest.html"><i class="fa fa-check"></i><b>9</b> t-Test</a></li>
<li class="chapter" data-level="10" data-path="10-ftest.html"><a href="10-ftest.html"><i class="fa fa-check"></i><b>10</b> F-Test</a></li>
<li class="chapter" data-level="11" data-path="11-chi2test_kstest.html"><a href="11-chi2test_kstest.html"><i class="fa fa-check"></i><b>11</b> Chi-Quadrat- und Kolmogorow-Smirnow-Test</a></li>
<li class="chapter" data-level="12" data-path="12-regression.html"><a href="12-regression.html"><i class="fa fa-check"></i><b>12</b> Lineare Regression</a></li>
<li class="appendix"><span><b>Referenzen</b></span></li>
<li class="chapter" data-level="" data-path="13-refs.html"><a href="13-refs.html"><i class="fa fa-check"></i>Literatur</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Einführung in die Statistik</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="regression" class="section level1">
<h1><span class="header-section-number">Kapitel 12</span> Lineare Regression</h1>
<p>Für die lineare Regression kehren wir zu einer Frage aus Kapitel <a href="05-korrelation.html#korrelation">5</a> zurück: <em>Kann man die Entfernung zu Ihrem Wohnort mit der Anzahl Stationen, die Sie bis Adlershof brauchen statistisch vorhersagen?</em></p>
<div class="sourceCode" id="cb45"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb45-1"><a href="12-regression.html#cb45-1"></a><span class="kw">plot</span>(reisedat<span class="op">$</span>stationen, reisedat<span class="op">$</span>distanz, <span class="dt">xlab=</span><span class="st">&quot;Anzahl Stationen&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;Entfernung (km)&quot;</span>)</span></code></pre></div>
<p><img src="eids_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<p>Wir erinnern uns, dass der Korrelationskoeffizient nach Bravais-Pearson aus Kapitel <a href="05-korrelation.html#korrelation">5</a> 0.81 war. Das <strong>Ziel</strong> ist nun, eine Gerade durch die Punktwolke zu legen, die den <em>Trend</em> beschreibt, so dass der Abstand der Punkte von der Geraden minimal ist.</p>
<p>Es geht um <em>2 Variablen</em> (Merkmale):</p>
<ul>
<li>die <strong>abhängige Variable</strong> <span class="math inline">\(y\)</span> (im Bsp. Entfernung)</li>
<li>die <strong>unabhängige Variable</strong> <span class="math inline">\(x\)</span> (im Bsp. Anzahl Stationen)</li>
</ul>
<p>Die Variablen müssen <em>metrisch</em> skaliert sein.<a href="#fn10" class="footnote-ref" id="fnref10"><sup>10</sup></a> Wir wollen das generelle Verhalten von <span class="math inline">\(y\)</span> mit <span class="math inline">\(x\)</span> beschreiben. Eine Gerade stellt dabei das einfachste lineare Modell dar.</p>
<div id="definitionen" class="section level2">
<h2><span class="header-section-number">12.1</span> Definitionen</h2>
<p>Im Falle einer einzigen unabhängigen Variable lautet die Gleichung des <strong>linearen Models</strong>:</p>
<p><span class="math display" id="eq:linmodsingle">\[\begin{equation}
y_i = \beta_0 + \beta_1 \cdot x_i + \epsilon_i \quad \text{mit} \quad i=1,2,\ldots,n
\tag{12.1}
\end{equation}\]</span></p>
<p><span class="math inline">\(y_i\)</span> bezeichnet den Wert der <strong>abhängigen Variable</strong> für Datenpunkt <span class="math inline">\(i\)</span>, und <span class="math inline">\(x_i\)</span> den Wert der <strong>unabhängigen Variable</strong> für Datenpunkt <span class="math inline">\(i\)</span>. Der Parameter <span class="math inline">\(\beta_0\)</span> beschreibt den <strong>Achsenabschnitt</strong> der Geraden, also der Punkt an dem die Gerade die y-Achse schneidet. Der Parameter <span class="math inline">\(\beta_1\)</span> beschreibt die <strong>Steigung</strong> der Geraden. <span class="math inline">\(\epsilon_i\)</span> stellt das <strong>Residuum</strong> (also den Fehler) für Datenpunkt <span class="math inline">\(i\)</span> dar (Abbildung <a href="12-regression.html#fig:linreg">12.1</a>).</p>
<div class="figure" style="text-align: center"><span id="fig:linreg"></span>
<img src="figs/abb12-2.png" alt="Lineare Regression: Definitionen."  />
<p class="caption">
Abbildung 12.1: Lineare Regression: Definitionen.
</p>
</div>
</div>
<div id="beschreibung-vs.-vorhersage" class="section level2">
<h2><span class="header-section-number">12.2</span> Beschreibung vs. Vorhersage</h2>
<p>Der primäre Zweck einer Regressionsanalyse ist die Beschreibung (oder Erklärung) der Daten im Sinne einer allgemeinen Beziehung, die sich auf die Grundgesamtheit übertragen lässt, aus der diese Daten entnommen wurden. Da diese Beziehung eine Eigenschaft der Grundgesamtheit ist, sollte diese demnach auch Vorhersagen ermöglichen. Hierbei ist jedoch Vorsicht geboten. Betrachten Sie den Zusammenhang von Jahr und Weltrekordzeit für die in Abbildung <a href="12-regression.html#fig:mile">12.2</a> dargestellten Daten (Meile, Herren). Wenn, wie hier, die Zeit die unabhängige Variable ist, wird die Regression zu einer Form der Trendanalyse, die in diesem Fall eine Verringerung der Rekordzeit mit den Jahren anzeigt. (Die <code>lm()</code> Funktion und ihren Output werden wir weiter unten kennenlernen, hier geht es um die Grafiken.)</p>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb46-1"><a href="12-regression.html#cb46-1"></a><span class="co"># Daten laden</span></span>
<span id="cb46-2"><a href="12-regression.html#cb46-2"></a>mile &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;https://raw.githubusercontent.com/avehtari/ROS-Examples/master/Mile/data/mile.csv&quot;</span>, <span class="dt">header=</span><span class="ot">TRUE</span>)</span>
<span id="cb46-3"><a href="12-regression.html#cb46-3"></a><span class="co"># lineares Modell an Daten aus 1. Hälfte des 20. Jahrh. anpassen</span></span>
<span id="cb46-4"><a href="12-regression.html#cb46-4"></a>mile_fit1 &lt;-<span class="st"> </span><span class="kw">lm</span>(seconds <span class="op">~</span><span class="st"> </span>year, <span class="dt">data =</span> mile[mile<span class="op">$</span>year<span class="op">&lt;</span><span class="dv">1950</span>,])</span>
<span id="cb46-5"><a href="12-regression.html#cb46-5"></a><span class="co"># Informationen zu Parameterschätzern extrahieren</span></span>
<span id="cb46-6"><a href="12-regression.html#cb46-6"></a><span class="kw">coef</span>(<span class="kw">summary</span>(mile_fit1))</span></code></pre></div>
<pre><code>##             Estimate Std. Error t value  Pr(&gt;|t|)
## (Intercept) 912.2340   67.90140  13.435 3.615e-08
## year         -0.3439    0.03509  -9.798 9.059e-07</code></pre>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb48-1"><a href="12-regression.html#cb48-1"></a><span class="co"># lineares Modell an kompletten Datensatz anpassen</span></span>
<span id="cb48-2"><a href="12-regression.html#cb48-2"></a>mile_fit2 &lt;-<span class="st"> </span><span class="kw">lm</span>(seconds <span class="op">~</span><span class="st"> </span>year, <span class="dt">data =</span> mile)</span>
<span id="cb48-3"><a href="12-regression.html#cb48-3"></a><span class="kw">coef</span>(<span class="kw">summary</span>(mile_fit2))</span></code></pre></div>
<pre><code>##             Estimate Std. Error t value  Pr(&gt;|t|)
## (Intercept) 1006.876     21.532   46.76 1.361e-29
## year          -0.393      0.011  -35.73 3.780e-26</code></pre>
<div class="sourceCode" id="cb50"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb50-1"><a href="12-regression.html#cb50-1"></a><span class="co"># Modellanpassung für 1. Hälfte des 20. Jahrh. plotten</span></span>
<span id="cb50-2"><a href="12-regression.html#cb50-2"></a><span class="kw">plot</span>(mile<span class="op">$</span>year[mile<span class="op">$</span>year<span class="op">&lt;</span><span class="dv">1950</span>], mile<span class="op">$</span>seconds[mile<span class="op">$</span>year<span class="op">&lt;</span><span class="dv">1950</span>],</span>
<span id="cb50-3"><a href="12-regression.html#cb50-3"></a>     <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="dv">1900</span>, <span class="dv">2000</span>), <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="dv">200</span>, <span class="dv">260</span>),</span>
<span id="cb50-4"><a href="12-regression.html#cb50-4"></a>     <span class="dt">pch =</span> <span class="dv">19</span>, <span class="dt">type =</span> <span class="st">&#39;p&#39;</span>,</span>
<span id="cb50-5"><a href="12-regression.html#cb50-5"></a>     <span class="dt">xlab =</span> <span class="st">&quot;Jahr&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;Weltrekord, Meile, Herren (Sekunden)&quot;</span>)</span>
<span id="cb50-6"><a href="12-regression.html#cb50-6"></a><span class="kw">abline</span>(<span class="kw">coef</span>(mile_fit1), <span class="dt">lwd =</span> <span class="dv">3</span>, <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>)</span>
<span id="cb50-7"><a href="12-regression.html#cb50-7"></a><span class="co"># Extrapolation für 2. Hälfte des 20. Jahrh. plotten</span></span>
<span id="cb50-8"><a href="12-regression.html#cb50-8"></a><span class="kw">plot</span>(mile<span class="op">$</span>year, mile<span class="op">$</span>seconds,</span>
<span id="cb50-9"><a href="12-regression.html#cb50-9"></a>     <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="dv">1900</span>, <span class="dv">2000</span>), <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="dv">200</span>, <span class="dv">260</span>),</span>
<span id="cb50-10"><a href="12-regression.html#cb50-10"></a>     <span class="dt">pch =</span> <span class="dv">19</span>, <span class="dt">type =</span> <span class="st">&#39;p&#39;</span>,</span>
<span id="cb50-11"><a href="12-regression.html#cb50-11"></a>     <span class="dt">xlab =</span> <span class="st">&quot;Jahr&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;Weltrekord, Meile, Herren (Sekunden)&quot;</span>)</span>
<span id="cb50-12"><a href="12-regression.html#cb50-12"></a><span class="kw">abline</span>(<span class="kw">coef</span>(mile_fit1), <span class="dt">lwd =</span> <span class="dv">3</span>, <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>)</span>
<span id="cb50-13"><a href="12-regression.html#cb50-13"></a><span class="co"># Modellanpassung für Gesamtdaten bis 2050 plotten</span></span>
<span id="cb50-14"><a href="12-regression.html#cb50-14"></a><span class="kw">plot</span>(mile<span class="op">$</span>year, mile<span class="op">$</span>seconds,</span>
<span id="cb50-15"><a href="12-regression.html#cb50-15"></a>     <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="dv">1900</span>, <span class="dv">2050</span>), <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="dv">200</span>, <span class="dv">260</span>),</span>
<span id="cb50-16"><a href="12-regression.html#cb50-16"></a>     <span class="dt">pch =</span> <span class="dv">19</span>, <span class="dt">type =</span> <span class="st">&#39;p&#39;</span>,</span>
<span id="cb50-17"><a href="12-regression.html#cb50-17"></a>     <span class="dt">xlab =</span> <span class="st">&quot;Jahr&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;Weltrekord, Meile, Herren (Sekunden)&quot;</span>)</span>
<span id="cb50-18"><a href="12-regression.html#cb50-18"></a><span class="kw">abline</span>(<span class="kw">coef</span>(mile_fit2), <span class="dt">lwd =</span> <span class="dv">3</span>, <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:mile"></span>
<img src="eids_files/figure-html/mile-1.png" alt="Links: Trend des Weltrekords &quot;Meile, Herren&quot; in der ersten Hälfte des 20. Jahrhunderts (Beschreibung). Mitte: Extrapolation des Trends für die zweite Hälfte des 20. Jahrhunderts (Vorhersage). Rechts: Extrapolation des Trends bis zum Jahr 2050 (längere Vorhersage). Nach: @wainer2009" width="33%" /><img src="eids_files/figure-html/mile-2.png" alt="Links: Trend des Weltrekords &quot;Meile, Herren&quot; in der ersten Hälfte des 20. Jahrhunderts (Beschreibung). Mitte: Extrapolation des Trends für die zweite Hälfte des 20. Jahrhunderts (Vorhersage). Rechts: Extrapolation des Trends bis zum Jahr 2050 (längere Vorhersage). Nach: @wainer2009" width="33%" /><img src="eids_files/figure-html/mile-3.png" alt="Links: Trend des Weltrekords &quot;Meile, Herren&quot; in der ersten Hälfte des 20. Jahrhunderts (Beschreibung). Mitte: Extrapolation des Trends für die zweite Hälfte des 20. Jahrhunderts (Vorhersage). Rechts: Extrapolation des Trends bis zum Jahr 2050 (längere Vorhersage). Nach: @wainer2009" width="33%" />
<p class="caption">
Abbildung 12.2: Links: Trend des Weltrekords “Meile, Herren” in der ersten Hälfte des 20. Jahrhunderts (Beschreibung). Mitte: Extrapolation des Trends für die zweite Hälfte des 20. Jahrhunderts (Vorhersage). Rechts: Extrapolation des Trends bis zum Jahr 2050 (längere Vorhersage). Nach: <span class="citation">Wainer (<a href="#ref-wainer2009" role="doc-biblioref">2009</a>)</span>
</p>
</div>
<p>Wir sehen, dass sich der Weltrekord in der ersten Hälfte des 20. Jahrhunderts linear verbesserte (Abbildung <a href="12-regression.html#fig:mile">12.2</a>, links). Dieser Trend passt auch für die zweite Hälfte des 20. Jahrhunderts bemerkenswert gut (Abbildung <a href="12-regression.html#fig:mile">12.2</a>, Mitte). Wie lange kann sich der Weltrekord jedoch noch mit der gleichen Rate verbessern (Abbildung <a href="12-regression.html#fig:mile">12.2</a>, rechts)?</p>
<p>Dieses Beispiel zeigt deutlich die Anwendbarkeit von Regressionen für Vorhersagen innerhalb bestimmter Grenzen, zeigt jedoch gleichzeitig die Grenzen dieser einfachen Modelle für längere Vorhersagen (z.B. in Zeit und Raum). Im Falle des Weltrekords würden wir erwarten, dass die Verbesserungsrate mit der Zeit abnimmt, d.h. dass die Kurve abflacht, was ein nichtlineares Modell erfordert.</p>
</div>
<div id="ausblick-weiterführende-lineare-modelle" class="section level2">
<h2><span class="header-section-number">12.3</span> Ausblick: Weiterführende lineare Modelle</h2>
<p>Wenn wir über das lineare Modell sprechen, ist die abhängige Variable immer metrisch skaliert, während die unabhängigen Variablen metrisch, nominal/ordinal oder gemischt sein können. Im Prinzip kann jede dieser Varianten mathematisch gleich behandelt werden, d.h. alle können z.B. mit der <code>lm()</code> Funktion in <em>R</em> analysiert werden. Allerdings haben sich historisch gesehen unterschiedliche Bezeichnungen für diese Varianten etabliert, die hier erwähnt werden sollen, um Verwirrung zu vermeiden (Tabellen <a href="12-regression.html#tab:varianten1">12.1</a> und <a href="12-regression.html#tab:varianten2">12.2</a>).</p>
<table>
<caption><span id="tab:varianten1">Tabelle 12.1: </span> Historische Namen für die Varianten des linearen Modells, je nachdem, ob die unabhängigen Variablen metrisch, nominal/ordinal oder gemischt sind. Die abhängige Variable ist immer metrisch skaliert.</caption>
<colgroup>
<col width="33%" />
<col width="33%" />
<col width="33%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">unabhängige Variable(n)<br>metrisch</th>
<th align="center">unabhängige Variable(n)<br>nominal/ordinal</th>
<th align="center">unabhängige Variable(n)<br>gemischt</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Regression</td>
<td align="center">Varianzanalyse<br>(ANOVA)</td>
<td align="center">Kovarianzanalyse<br>(ANCOVA)</td>
</tr>
</tbody>
</table>
<table>
<caption><span id="tab:varianten2">Tabelle 12.2: </span> Historische Namen für Regression, je nachdem, ob wir eine oder mehrere unabhängige Variablen und eine oder mehrere abhängige Variablen haben.</caption>
<colgroup>
<col width="33%" />
<col width="33%" />
<col width="33%" />
</colgroup>
<thead>
<tr class="header">
<th align="center"></th>
<th align="center">1 unabhängige Variable</th>
<th align="center">&gt;1 unabhängige Variable</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><strong>1 abhängige Variable</strong></td>
<td align="center">Regression</td>
<td align="center">Multiple Regression</td>
</tr>
<tr class="even">
<td align="center"><strong>&gt;1 abhängige Variable</strong></td>
<td align="center">Multivariate Regression</td>
<td align="center">Multivariate multiple Regression</td>
</tr>
</tbody>
</table>
</div>
<div id="lineare-regression" class="section level2">
<h2><span class="header-section-number">12.4</span> Lineare Regression</h2>
<p>Wie soll nun die Gerade durch die Punktwolke gelegt werden, d.h. welche Werte sollen Achsenabschnitt <span class="math inline">\(\beta_0\)</span> und Steigung <span class="math inline">\(\beta_1\)</span> annehmen? Typischerweise werden Regressionsprobleme gelöst, indem die Summe der quadratischen Abweichungen zwischen der Regressionsgeraden und den Datenpunkten minimiert wird - die sogenannte <strong>Kleinste-Quadrate-Schätzung</strong>.</p>
<p>Die Summe der quadratischen Abweichungen wird auch als <span class="math inline">\(SSE\)</span> bezeichnet (Sum of Squared Errors). Grafisch gesehen probieren wir in Abbildung <a href="12-regression.html#fig:linreg">12.1</a> verschiedene Geraden mit unterschiedlichen Achsenabschnitten <span class="math inline">\(\beta_0\)</span> und Steigungen <span class="math inline">\(\beta_1\)</span> aus und wählen diejenige, bei der die Summe aller vertikalen Abstände <span class="math inline">\(\epsilon_i\)</span> zum Quadrat am kleinsten ist. Mathematisch ist <span class="math inline">\(SSE\)</span> definiert als:</p>
<p><span class="math display" id="eq:sse">\[\begin{equation}
SSE=\sum_{i=1}^{n}\left(\epsilon_i\right)^2=\sum_{i=1}^{n}\left(y_i-\left(\beta_0+\beta_1 \cdot x_i\right)\right)^2
\tag{12.2}
\end{equation}\]</span></p>
<p>Das Residuum <span class="math inline">\(\epsilon_i\)</span> ist also gleich <span class="math inline">\(y_i-\left(\beta_0+\beta_1 \cdot x_i\right)\)</span>, dem vertikalen Abstand zwischen Datenpunkt und Regressionsgerade.</p>
<p>Im Fall der linearen Regression kann <span class="math inline">\(SSE\)</span> analytisch minimiert werden, was z.B. bei nichtlinearen Modellen nicht der Fall ist. Analytisch finden wir das Minimum von <span class="math inline">\(SSE\)</span> wo dessen partielle Ableitungen in Bezug auf die beiden Modellparameter beide Null sind: <span class="math inline">\(\frac{\partial SSE}{\partial \beta_0}=0\)</span> und <span class="math inline">\(\frac{\partial SSE}{\partial \beta_1}=0\)</span>. Unter Anwendung der Definition von <span class="math inline">\(SEE\)</span> aus Gleichung <a href="12-regression.html#eq:sse">(12.2)</a> und der Summenregel<a href="#fn11" class="footnote-ref" id="fnref11"><sup>11</sup></a> und der Kettenregel<a href="#fn12" class="footnote-ref" id="fnref12"><sup>12</sup></a>, die Sie noch aus der Schule kennen werden, erhalten wir:</p>
<p><span class="math display" id="eq:sseb0">\[\begin{equation}
\frac{\partial SSE}{\partial \beta_0}=-2 \cdot \sum_{i=1}^{n}\left(y_i-\beta_0-\beta_1 \cdot x_i\right)=0
\tag{12.3}
\end{equation}\]</span>
<span class="math display" id="eq:sseb1">\[\begin{equation}
\frac{\partial SSE}{\partial \beta_1}=-2 \cdot \sum_{i=1}^{n}x_i \cdot \left(y_i-\beta_0-\beta_1 \cdot x_i\right)=0
\tag{12.4}
\end{equation}\]</span></p>
<p>Gleichungen <a href="12-regression.html#eq:sseb0">(12.3)</a> und <a href="12-regression.html#eq:sseb1">(12.4)</a> bilden ein Gleichungssystem mit zwei Gleichungen und zwei Unbekannten, das wir eindeutig lösen können. Zuerst lösen wir Gleichung <a href="12-regression.html#eq:sseb0">(12.3)</a> nach <span class="math inline">\(\beta_0\)</span> auf (nachdem wir durch -2 geteilt haben):</p>
<p><span class="math display" id="eq:b01">\[\begin{equation}
\sum_{i=1}^{n}y_i-n \cdot \beta_0-\beta_1 \cdot \sum_{i=1}^{n}x_i=0
\tag{12.5}
\end{equation}\]</span>
<span class="math display" id="eq:b02">\[\begin{equation}
n \cdot \beta_0=\sum_{i=1}^{n}y_i-\beta_1 \cdot \sum_{i=1}^{n}x_i
\tag{12.6}
\end{equation}\]</span>
<span class="math display" id="eq:b03">\[\begin{equation}
\beta_0=\bar{y}-\beta_1 \cdot \bar{x}
\tag{12.7}
\end{equation}\]</span></p>
<p>Formal sind das jetzt Parameter<em>schätzer</em> (das “Dach”-Symbol bezeichnet Schätzer):
<span class="math display" id="eq:b04">\[\begin{equation}
\hat\beta_0=\bar{y}-\hat\beta_1 \cdot \bar{x}
\tag{12.8}
\end{equation}\]</span></p>
<p>Sodann setzen wir Gleichung <a href="12-regression.html#eq:b04">(12.8)</a> in Gleichung <a href="12-regression.html#eq:sseb1">(12.4)</a> ein (nachdem wir durch -2 geteilt haben):</p>
<p><span class="math display" id="eq:insert1">\[\begin{equation}
\sum_{i=1}^{n}\left(x_i \cdot y_i-\beta_0 \cdot x_i-\beta_1 \cdot x_i^2\right)=0
\tag{12.9}
\end{equation}\]</span>
<span class="math display" id="eq:insert2">\[\begin{equation}
\sum_{i=1}^{n}\left(x_i \cdot y_i-\bar{y} \cdot x_i+\hat\beta_1 \cdot \bar{x} \cdot x_i-\hat\beta_1 \cdot x_i^2\right)=0
\tag{12.10}
\end{equation}\]</span></p>
<p>Schließlich lösen wir Gleichung <a href="12-regression.html#eq:insert2">(12.10)</a> nach <span class="math inline">\(\beta_1\)</span> auf:</p>
<p><span class="math display" id="eq:b11">\[\begin{equation}
\sum_{i=1}^{n}\left(x_i \cdot y_i-\bar{y} \cdot x_i\right)-\hat\beta_1 \cdot \sum_{i=1}^{n}\left(x_i^2-\bar{x} \cdot x_i\right)=0
\tag{12.11}
\end{equation}\]</span>
<span class="math display" id="eq:b12">\[\begin{equation}
\hat\beta_1=\frac{\sum_{i=1}^{n}\left(x_i \cdot y_i-\bar{y} \cdot x_i\right)}{\sum_{i=1}^{n}\left(x_i^2-\bar{x} \cdot x_i\right)}
\tag{12.12}
\end{equation}\]</span></p>
<p>Über eine Reihe von Schritten, die ich hier überspringe, erhalten wir:</p>
<p><span class="math display" id="eq:b13">\[\begin{equation}
\hat\beta_1=\frac{SSXY}{SSX}
\tag{12.13}
\end{equation}\]</span></p>
<p><span class="math inline">\(SSX=\sum_{i=1}^{n}\left(x_i-\bar{x}\right)^2\)</span> ist ein Maß für die Varianz der Daten in <span class="math inline">\(x\)</span>-Richtung. <span class="math inline">\(SSXY=\sum_{i=1}^{n}\left(x_i-\bar{x}\right) \cdot \left(y_i-\bar{y}\right)\)</span> ist ein Maß für die Kovarianz der Daten. Es gibt auch <span class="math inline">\(SSY=\sum_{i=1}^{n}\left(y_i-\bar{y}\right)^2\)</span>, das entsprechend ein Maß für die Varianz der Daten in <span class="math inline">\(y\)</span>-Richtung ist. Gleichung <a href="12-regression.html#eq:b13">(12.13)</a> ist eine exakte Lösung für <span class="math inline">\(\hat\beta_1\)</span>.</p>
<p>Wir setzen nun Gleichung <a href="12-regression.html#eq:b13">(12.13)</a> in Gleichung <a href="12-regression.html#eq:b04">(12.8)</a> ein und haben eine exakte Lösung für <span class="math inline">\(\hat\beta_0\)</span>. Berechnen wir nun die Parameter für unsere Reisedaten mit der <code>lm()</code> Funktion:</p>
<div class="sourceCode" id="cb51"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb51-1"><a href="12-regression.html#cb51-1"></a><span class="co"># lineare Regression der Reisedaten</span></span>
<span id="cb51-2"><a href="12-regression.html#cb51-2"></a>reise_fit &lt;-<span class="st"> </span><span class="kw">lm</span>(distanz <span class="op">~</span><span class="st"> </span>stationen, <span class="dt">data =</span> reisedat)</span>
<span id="cb51-3"><a href="12-regression.html#cb51-3"></a><span class="co"># Informationen über geschätzte Parameterwerte ausgeben</span></span>
<span id="cb51-4"><a href="12-regression.html#cb51-4"></a><span class="kw">coef</span>(<span class="kw">summary</span>(reise_fit))</span></code></pre></div>
<pre><code>##             Estimate Std. Error t value  Pr(&gt;|t|)
## (Intercept)   4.9232    1.01874   4.833 7.533e-06
## stationen     0.6934    0.05976  11.604 4.641e-18</code></pre>
<p>In der ersten Spalte (“Estimate”) dieses Outputs finden Sie die Werte der Parameterschätzer, wobei “(Intercept)” für <span class="math inline">\(\beta_0\)</span> steht und “stationen” für <span class="math inline">\(\beta_1\)</span>. Anhand von <span class="math inline">\(\beta_1\)</span> können wir ablesen, dass die Entfernung zwischen zwei Stationen im Mittel 0.7km beträgt. Der Achsenabschnitt <span class="math inline">\(\beta_0\)</span> hat keine direkte Entsprechung.<a href="#fn13" class="footnote-ref" id="fnref13"><sup>13</sup></a> Auf die anderen Spalten werden wir weiter unten zu sprechen kommen. Plotten wir nun die so ermittelte Regressionsgerade <span class="math inline">\(y_i=4.9+0.7\cdot x_i+\epsilon_i\)</span>:</p>
<div class="sourceCode" id="cb53"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb53-1"><a href="12-regression.html#cb53-1"></a><span class="co"># Modellanpassung plotten</span></span>
<span id="cb53-2"><a href="12-regression.html#cb53-2"></a><span class="kw">plot</span>(reisedat<span class="op">$</span>stationen, reisedat<span class="op">$</span>distanz,</span>
<span id="cb53-3"><a href="12-regression.html#cb53-3"></a>     <span class="dt">pch =</span> <span class="dv">19</span>, <span class="dt">type =</span> <span class="st">&#39;p&#39;</span>,</span>
<span id="cb53-4"><a href="12-regression.html#cb53-4"></a>     <span class="dt">xlab =</span> <span class="st">&quot;Anzahl Stationen&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;Entfernung (km)&quot;</span>)</span>
<span id="cb53-5"><a href="12-regression.html#cb53-5"></a><span class="kw">abline</span>(<span class="kw">coef</span>(reise_fit), <span class="dt">lwd =</span> <span class="dv">3</span>, <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<p><img src="eids_files/figure-html/unnamed-chunk-27-1.png" width="672" /></p>
</div>
<div id="signifikanz-der-regression" class="section level2">
<h2><span class="header-section-number">12.5</span> Signifikanz der Regression</h2>
<p>Nun, dass wir Werte für die Regressionsparameter haben, müssen wir uns fragen, ob diese Werte statistisch signifikant sind oder ob sie durch Zufall aus dem (angenommenen) Zufallsprozess der Stichprobenziehung entstanden sein könnten. Dazu testen wir formal, ob die vom Modell erklärte Varianz in den Daten signifikant größer als die nicht erklärte Varianz ist. Das ist ein F-Test-Problem, das wir über die sogenannte Varianzanalyse (ANOVA) angehen. ANOVA beginnt mit der Erstellung der ANOVA-Tabelle (Tabelle <a href="12-regression.html#tab:anova">12.3</a>). Dies geschieht in <em>R</em> im Hintergrund und wird selten explizit betrachtet; tun wir es hier aber trotzdem, damit wir verstehen was passiert.</p>
<table style="width:100%;">
<caption><span id="tab:anova">Tabelle 12.3: </span> ANOVA-Tabelle der linearen Regression.</caption>
<colgroup>
<col width="16%" />
<col width="16%" />
<col width="16%" />
<col width="16%" />
<col width="16%" />
<col width="16%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">Varianz-<br>quelle</th>
<th align="center">Quadrat-<br>summe</th>
<th align="center">Freiheits-<br>grad (<span class="math inline">\(df\)</span>)</th>
<th align="center">Varianz</th>
<th align="center">F-Statistik (<span class="math inline">\(F_s\)</span>)</th>
<th align="center">p-Wert</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Regression</td>
<td align="center"><span class="math inline">\(SSR=\\SSY-SSE\)</span></td>
<td align="center"><span class="math inline">\(1\)</span></td>
<td align="center"><span class="math inline">\(\frac{SSR}{df_{SSR}}\)</span></td>
<td align="center"><span class="math inline">\(\frac{\frac{SSR}{df_{SSR}}}{s^2}\)</span></td>
<td align="center"><span class="math inline">\(1-F\left(F_s,1,n-2\right)\)</span></td>
</tr>
<tr class="even">
<td align="center">Fehler</td>
<td align="center"><span class="math inline">\(SSE\)</span></td>
<td align="center"><span class="math inline">\(n-2\)</span></td>
<td align="center"><span class="math inline">\(\frac{SSE}{df_{SSE}}=s^2\)</span></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="center">Gesamt</td>
<td align="center"><span class="math inline">\(SSY\)</span></td>
<td align="center"><span class="math inline">\(n-1\)</span></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
</tbody>
</table>
<p>Schauen wir uns zunächst die zweiten Spalte der Tabelle <a href="12-regression.html#tab:anova">12.3</a> an: <span class="math inline">\(SSY=\sum_{i=1}^{n}\left(y_i-\bar{y}\right)^2\)</span> ist ein Maß für die Gesamtvarianz der Daten (in <span class="math inline">\(y\)</span>-Richtung), d.h. wie stark die Datenpunkte um den Gesamtmittelwert streuen (Abbildung <a href="12-regression.html#fig:ssysse">12.3</a>, links). <span class="math inline">\(SSE=\sum_{i=1}^{n}\left(\epsilon_i\right)^2=\sum_{i=1}^{n}\left(y_i-\left(\beta_0+\beta_1 \cdot x_i\right)\right)^2\)</span> ist ein Maß für die Fehlervarianz, d.h. wie stark die Datenpunkte um die Regressionsgerade streuen (Abbildung <a href="12-regression.html#fig:ssysse">12.3</a>, rechts). Das ist die Varianz, die nach der Modellanpassung übrig ist (“nicht erklärt”). <span class="math inline">\(SSR=SSY-SSE\)</span> ist folglich ein Maß für die vom Modell erklärte Varianz.</p>
<div class="figure" style="text-align: center"><span id="fig:ssysse"></span>
<img src="figs/ssy.jpg" alt="Variation der Datenpunkte um den Mittelwert, zusammengefasst durch $SSY$ (links), und um die Regressionsgerade, zusammengefasst durch $SSE$ (rechts)." width="50%" /><img src="figs/sse.jpg" alt="Variation der Datenpunkte um den Mittelwert, zusammengefasst durch $SSY$ (links), und um die Regressionsgerade, zusammengefasst durch $SSE$ (rechts)." width="50%" />
<p class="caption">
Abbildung 12.3: Variation der Datenpunkte um den Mittelwert, zusammengefasst durch <span class="math inline">\(SSY\)</span> (links), und um die Regressionsgerade, zusammengefasst durch <span class="math inline">\(SSE\)</span> (rechts).
</p>
</div>
<p>In der dritten Spalte der Tabelle <a href="12-regression.html#tab:anova">12.3</a> stehen die Freiheitsgrade der drei Varianzterme. Diese können als Anzahl der Werte in einer Stichprobe, die für die Berechnung der jeweiligen Parameter frei zur Verfügung stehen, verstanden werden (vgl. Kapitel <a href="04-streuung.html#streuung">4</a>): In die Berechnung von <span class="math inline">\(SSY\)</span> geht <span class="math inline">\(\bar y\)</span> ein, für dessen Berechnung die Werte der Stichprobe bereits einmal verwendet wurden; dadurch ist die Anzahl Freiheitsgrade <span class="math inline">\(df_{SSY}=n-1\)</span>. In die Berechnung von <span class="math inline">\(SSE\)</span> gehen <span class="math inline">\(\beta_0\)</span> und <span class="math inline">\(\beta_1\)</span> ein (Gleichung <a href="12-regression.html#eq:sse">(12.2)</a>), d.h. die Anzahl Freiheitsgrade ist <span class="math inline">\(df_{SSE}=n-2\)</span>. Für <span class="math inline">\(SSR\)</span> gilt dann einfach <span class="math inline">\(df_{SSR}=df_{SSY}-df_{SSE}=1\)</span>. Die Freiheitsgrade werden verwendet, um die Varianzterme in der vierten Spalte der Tabelle <a href="12-regression.html#tab:anova">12.3</a> zu normalisieren, wobei <span class="math inline">\(s^2\)</span> Fehlervarianz genannt wird.</p>
<p>In der fünften Spalte der Tabelle <a href="12-regression.html#tab:anova">12.3</a> finden wir das Verhältnis von zwei Varianzen; Regressionsvarianz über Fehlervarianz. Von einer signifikanten Regression erwarten wir, dass die (durch das Modell erklärte) Regressionsvarianz viel größer ist als die (durch das Modell nicht erklärte) Fehlervarianz. Dies ist ein F-Test Problem, bei dem getestet wird, ob sich die durch das Modell <em>erklärte</em> Varianz <strong>signifikant</strong> von der durch das Modell <em>nicht erklärten</em> Varianz unterscheidet. Das Verhältnis der beiden Varianzen dient als F-Statistik <span class="math inline">\(F_s\)</span>.</p>
<p>Die sechste Spalte der Tabelle <a href="12-regression.html#tab:anova">12.3</a> gibt dann den p-Wert des F-Tests an, d.h. die Wahrscheinlichkeit, <span class="math inline">\(F_s\)</span> oder einen größeren Wert (d.h. <strong>ein noch besseres Modell</strong>) zufällig zu erhalten, wenn die Nullhypothese <span class="math inline">\(H_0\)</span> wahr ist (vgl. Kapitel <a href="10-ftest.html#ftest">10</a>). Im Fall der linearen Regression ist <span class="math inline">\(H_0:\frac{SSR}{df_{SSR}}=s^2\)</span>, d.h. die beiden Varianzen sind gleich, und <span class="math inline">\(H_1:\frac{SSR}{df_{SSR}}&gt;s^2\)</span>, d.h. die erklärte Varianz ist größer als die nicht erklärte.</p>
Wie in Kapitel <a href="10-ftest.html#ftest">10</a> bereits diskutiert folgt <span class="math inline">\(F_s\)</span> einer F-Verteilung mit den Parametern <span class="math inline">\(1\)</span> und <span class="math inline">\(n-2\)</span> unter der Nullhypothese (Abbildung <a href="12-regression.html#fig:fcdf">12.4</a>). Die rote Linie in Abbildung <a href="12-regression.html#fig:fcdf">12.4</a> markiert einen bestimmten Wert von <span class="math inline">\(F_s\)</span> (hier zwischen 10 und 11) und den entsprechenden Wert der Verteilungsfunktion der F-Verteilung (<span class="math inline">\(F\left(F_s,1,n-2\right)\)</span>). Der p-Wert ist <span class="math inline">\(\Pr\left(Z&gt; F_s\right)=1-F\left(F_s,1,n-2\right)\)</span> und beschreibt die Wahrscheinlichkeit, dieses oder ein größeres Varianzverhältnis zufällig (aufgrund der zufälligen Stichprobenziehung) zu erhalten, selbst wenn die beiden Varianzen tatsächlich gleich sind.
<div class="figure" style="text-align: center"><span id="fig:fcdf"></span>
<img src="figs/cdf_f.jpg" alt="Verteilungsfunktion der F-Verteilung der F-Statistik $F_s$. Rot: Bestimmter Wert für $F_s$ und entsprechender Wert der Verteilungsfunktion." width="80%" />
<p class="caption">
Abbildung 12.4: Verteilungsfunktion der F-Verteilung der F-Statistik <span class="math inline">\(F_s\)</span>. Rot: Bestimmter Wert für <span class="math inline">\(F_s\)</span> und entsprechender Wert der Verteilungsfunktion.
</p>
</div>
<p>Für die Regression der Reisedaten ist der p-Wert wesentlich kleiner als das konventionelle Signifikanzniveau <span class="math inline">\(\alpha=0.01\)</span>, daher lehnen wir die die Nullhypothese ab und bezeichnen die Regression als statistisch signifikant. Schauen wir uns die ANOVA-Tabelle fuer das Beispiel an:</p>
<div class="sourceCode" id="cb54"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb54-1"><a href="12-regression.html#cb54-1"></a><span class="kw">anova</span>(reise_fit)</span></code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Response: distanz
##           Df Sum Sq Mean Sq F value Pr(&gt;F)    
## stationen  1   3309    3309     135 &lt;2e-16 ***
## Residuals 71   1745      25                   
## ---
## Signif. codes:  
## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Im Vergleich zu Tabelle <a href="12-regression.html#tab:anova">12.3</a> lässt <em>R</em> die letzte Zeile (<span class="math inline">\(SSY\)</span>) weg und tauscht die Spalten “Quadratsumme” und “Freiheitsgrad”.</p>
</div>
<div id="konfidenzintervalle-und-signifikanz-der-parameter" class="section level2">
<h2><span class="header-section-number">12.6</span> Konfidenzintervalle und Signifikanz der Parameter</h2>
<p>Da die Modellanpassung nicht perfekt ist, haben die Parameterschätzer Standardfehler, d.h. sie werden wie andere statistische Kennzahlen als Realisationen eines Zufallsprozesses interpretiert. Das führt uns zu Konfidenzintervallen und t-Tests auf Signifikanz der einzelnen Parameter.</p>
<p>Der <strong>Standardfehlern</strong> für <span class="math inline">\(\hat\beta_0\)</span> ist:
<span class="math display" id="eq:seb0">\[\begin{equation}
s_{\hat\beta_0}=\sqrt{\frac{\sum_{i=1}^{n}x_i^2}{n} \cdot \frac{s^2}{SSX}}
\tag{12.14}
\end{equation}\]</span></p>
<p>Wenn wir diese Formel in ihre einzelnen Teile zerlegen, sehen wir: Je mehr Datenpunkte <span class="math inline">\(n\)</span> wir haben, desto kleiner ist der Standardfehler, d.h. desto mehr Vertrauen haben wir in die Schätzung. Außerdem gilt, je größer die Variation in <span class="math inline">\(x\)</span> (<span class="math inline">\(SSX\)</span>), desto kleiner der Standardfehler. Beide Effekte machen intuitiv Sinn: Je mehr Datenpunkte wir haben und je mehr Ausprägungen von <span class="math inline">\(x\)</span> wir abgedeckt haben, desto sicherer können wir sein, dass unsere Stichprobe repräsentativ für die Grundgesamtheit ist. Umgekehrt gilt: Je größer die Fehlervarianz <span class="math inline">\(s^2\)</span>, d.h. je kleiner die Erklärungskraft unseres Modells, desto größer der Standardfehler. Und je mehr <span class="math inline">\(x\)</span>-Datenpunkte von Null entfernt sind, d.h. je größer <span class="math inline">\(\sum_{i=1}^{n}x_i^2\)</span>, desto geringer ist unser Vertrauen in den Achsenabschnitt (wo <span class="math inline">\(x=0\)</span> ist) und damit steigt der Standardfehler.</p>
<p>Der Standardfehler für <span class="math inline">\(\hat\beta_1\)</span> ist:
<span class="math display" id="eq:seb1">\[\begin{equation}
s_{\hat\beta_1}=\sqrt{\frac{s^2}{SSX}}
\tag{12.15}
\end{equation}\]</span>
Hier gilt die gleiche Interpretation wie zuvor, außer dass es keinen Einfluss der Größe der <span class="math inline">\(x\)</span>-Datenpunkte gibt.</p>
<p>Wir können auch einen Standardfehler für neue Vorhersagen <span class="math inline">\(\hat y\)</span> für gegebene <span class="math inline">\(\hat x\)</span> festlegen:
<span class="math display" id="eq:sey">\[\begin{equation}
s_{\hat y}=\sqrt{s^2 \cdot \left(\frac{1}{n}+\frac{\left(\hat x-\bar x\right)^2}{SSX}\right)}
\tag{12.16}
\end{equation}\]</span></p>
<p>Dieselbe Interpretation gilt auch hier, nur dass jetzt ein zusätzlicher Term <span class="math inline">\(\left(\hat x-\bar x\right)^2\)</span> auftaucht, der besagt, je weiter der neue <span class="math inline">\(x\)</span>-Wert vom Zentrum der ursprünglichen Daten (den Trainings- oder Kalibrierungsdaten) entfernt ist, desto größer ist der Standardfehler der neuen Vorhersage, d.h. desto geringer ist die Zuversicht, dass sie korrekt ist.</p>
<p>Anmerkung: Die Formeln für die Standardfehler ergeben sich aus den grundlegenden Annahmen der linearen Regression, auf die wir weiter unten eingehen werden. Die mathematische Herleitung lassen wir hier aus.</p>
<p>Aus den Standardfehlern können wir <strong>Konfidenzintervalle</strong> für die Parameterschätzer wie folgt berechnen:
<span class="math display" id="eq:cib01">\[\begin{equation}
\Pr\left(\hat\beta_0-t_{n-2;0.975} \cdot s_{\hat\beta_0}\leq \beta_0\leq \hat\beta_0+t_{n-2;0.975} \cdot s_{\hat\beta_0}\right)=0.95
\tag{12.17}
\end{equation}\]</span></p>
<p>Gleichung <a href="12-regression.html#eq:cib01">(12.17)</a> ist das zentrale 95%-Konfidenzintervall, in dem der wahre Parameterwert, hier <span class="math inline">\(\beta_0\)</span>, mit einer Wahrscheinlichkeit von 0.95 liegt. Vgl. Konfidenzintervall des Mittelwertschätzers (Kapitel <a href="08-schaetzen.html#schaetzen">8</a>).</p>
<p>Wir können das Intervall auch wie folgt schreiben:
<span class="math display" id="eq:cib02">\[\begin{equation}
KI=\left[\hat\beta_0-t_{n-2;0.975} \cdot s_{\hat\beta_0};\hat\beta_0+t_{n-2;0.975} \cdot s_{\hat\beta_0}\right]
\tag{12.18}
\end{equation}\]</span></p>
<p>Wie bei dem Konfidenzintervall des Mittelwertschätzers (Kapitel <a href="08-schaetzen.html#schaetzen">8</a>) liegt das Konfidenzintervall symmetrisch um den Parameterschätzwert <span class="math inline">\(\hat\beta_0\)</span> und ergibt sich aus einer t-Verteilung mit dem Parameter <span class="math inline">\(n-2\)</span>, deren Breite durch den Standardfehler <span class="math inline">\(s_{\hat\beta_0}\)</span> moduliert wird. Erinnern Sie sich, dass die Breite der t-Verteilung ebenfalls durch den Stichprobenumfang kontrolliert wird und mit zunehmendem <span class="math inline">\(n\)</span> immer schmaler wird.</p>
<p>Die gleichen Formeln gelten für <span class="math inline">\(\beta_1\)</span> und <span class="math inline">\(y\)</span>:
<span class="math display" id="eq:cib1">\[\begin{equation}
\Pr\left(\hat\beta_1-t_{n-2;0.975} \cdot s_{\hat\beta_1}\leq \beta_1\leq \hat\beta_1+t_{n-2;0.975} \cdot s_{\hat\beta_1}\right)=0.95
\tag{12.19}
\end{equation}\]</span>
<span class="math display" id="eq:ciy">\[\begin{equation}
\Pr\left(\hat y-t_{n-2;0.975} \cdot s_{\hat y}\leq y\leq \hat y+t_{n-2;0.975} \cdot s_{\hat y}\right)=0.95
\tag{12.20}
\end{equation}\]</span></p>
<p>Die Formeln für die Konfidenzintervalle (Gleichungen <a href="12-regression.html#eq:cib01">(12.17)</a>, <a href="12-regression.html#eq:cib1">(12.19)</a> und <a href="12-regression.html#eq:ciy">(12.20)</a>) ergeben sich aus den Grundannahmen der linearen Regression (vgl. Kapitel <a href="08-schaetzen.html#schaetzen">8</a>): Die Residuen sind <em>unabhängig identisch verteilt (u.i.v.)</em> gemäß einer <em>Normalverteilung</em>, d.h. <span class="math inline">\(\epsilon_i\sim N(0,\sigma)\)</span>, und <em>das lineare Modell ist korrekt</em>. Dann lässt sich mathematisch zeigen, dass <span class="math inline">\(\frac{\hat\beta_0-\beta_0}{s_{\hat\beta_0}}\)</span>, <span class="math inline">\(\frac{\hat\beta_1-\beta_1}{s_{\hat\beta_1}}\)</span> und <span class="math inline">\(\frac{\hat y-y}{s_{\hat y}}\)</span> <span class="math inline">\(t_{n-2}\)</span>-verteilt sind (t-Verteilung mit <span class="math inline">\(n-2\)</span> Freiheitsgraden).</p>
<p>Da das zentrale 95%-Konfidenzintervall einer <span class="math inline">\(t_{n-2}\)</span>-verteilten Zufallsvariablen <span class="math inline">\(Z\)</span> <span class="math inline">\(\Pr\left(-t_{n-2;0.975}\leq Z\leq t_{n-2;0. 975}\right)=0.95\)</span> ist (Abbildung <a href="12-regression.html#fig:tpdfcdf">12.5</a>), können wir jeden der oben genannten drei Terme für <span class="math inline">\(Z\)</span> einsetzen und die Ungleichung umstellen, um zu den Gleichungen <a href="12-regression.html#eq:cib01">(12.17)</a>, <a href="12-regression.html#eq:cib1">(12.19)</a> und <a href="12-regression.html#eq:ciy">(12.20)</a> zu gelangen.</p>
<div class="figure" style="text-align: center"><span id="fig:tpdfcdf"></span>
<img src="figs/pdf_t.jpg" alt="Links: Dichtefunktion einer t-verteilten Zufallsvariablen $Z$, wobei das zentrale 95%-Konfidenzintervall rot markiert ist. 95% der Dichtefunktion liegen zwischen den beiden Grenzen, 2.5% liegen links von der unteren Grenze und 2.5% rechts von der oberen Grenze. Rechts: Verteilungsfunktion der gleichen t-verteilten Zufallsvariablen $Z$. Die obere Grenze des 95%-Konfidenzintervalls ist als $t_{n-2;0.975}$ definiert, d.h. das 0.975-Perzentil der Verteilung, während die untere Grenze als $t_{n-2;0.025}$ definiert ist, was aufgrund der Symmetrie der Verteilung $-t_{n-2;0.975}$ entspricht." width="50%" /><img src="figs/cdf_t.jpg" alt="Links: Dichtefunktion einer t-verteilten Zufallsvariablen $Z$, wobei das zentrale 95%-Konfidenzintervall rot markiert ist. 95% der Dichtefunktion liegen zwischen den beiden Grenzen, 2.5% liegen links von der unteren Grenze und 2.5% rechts von der oberen Grenze. Rechts: Verteilungsfunktion der gleichen t-verteilten Zufallsvariablen $Z$. Die obere Grenze des 95%-Konfidenzintervalls ist als $t_{n-2;0.975}$ definiert, d.h. das 0.975-Perzentil der Verteilung, während die untere Grenze als $t_{n-2;0.025}$ definiert ist, was aufgrund der Symmetrie der Verteilung $-t_{n-2;0.975}$ entspricht." width="50%" />
<p class="caption">
Abbildung 12.5: Links: Dichtefunktion einer t-verteilten Zufallsvariablen <span class="math inline">\(Z\)</span>, wobei das zentrale 95%-Konfidenzintervall rot markiert ist. 95% der Dichtefunktion liegen zwischen den beiden Grenzen, 2.5% liegen links von der unteren Grenze und 2.5% rechts von der oberen Grenze. Rechts: Verteilungsfunktion der gleichen t-verteilten Zufallsvariablen <span class="math inline">\(Z\)</span>. Die obere Grenze des 95%-Konfidenzintervalls ist als <span class="math inline">\(t_{n-2;0.975}\)</span> definiert, d.h. das 0.975-Perzentil der Verteilung, während die untere Grenze als <span class="math inline">\(t_{n-2;0.025}\)</span> definiert ist, was aufgrund der Symmetrie der Verteilung <span class="math inline">\(-t_{n-2;0.975}\)</span> entspricht.
</p>
</div>
<p>Die Signifikanz der Parameterschätzer wird mit Hilfe eines t-Tests ermittelt (vgl. Kapitel <a href="09-ttest.html#ttest">9</a> und <a href="10-ftest.html#ftest">10</a>). Die <strong>Nullhypothese</strong> ist, dass die wahren Parameterwerte gleich Null sind, d.h. die Parameterschätzer <em>nicht</em> signifikant sind:
<span class="math display" id="eq:h0b0">\[\begin{equation}
H_0:\beta_0=0
\tag{12.21}
\end{equation}\]</span>
<span class="math display" id="eq:h0b1">\[\begin{equation}
H_0:\beta_1=0
\tag{12.22}
\end{equation}\]</span></p>
<p>Diese Hypothese wird gegen die <strong>Alternativhypothese</strong> getestet, dass die wahren Parameterwerte <em>un</em>gleich Null sind, d.h. dass die Parameterschätzer signifikant sind:
<span class="math display" id="eq:h1b0">\[\begin{equation}
H_1:\beta_0\neq 0
\tag{12.23}
\end{equation}\]</span>
<span class="math display" id="eq:h1b1">\[\begin{equation}
H_1:\beta_1\neq 0
\tag{12.24}
\end{equation}\]</span></p>
<p>Die Teststatistiken sind:
<span class="math display" id="eq:tsb0">\[\begin{equation}
t_s=\frac{\hat\beta_0-0}{s_{\hat\beta_0}}\sim t_{n-2}
\tag{12.25}
\end{equation}\]</span>
<span class="math display" id="eq:tsb1">\[\begin{equation}
t_s=\frac{\hat\beta_1-0}{s_{\hat\beta_1}}\sim t_{n-2}
\tag{12.26}
\end{equation}\]</span></p>
<p>Das “Tilde”-Symbol (<span class="math inline">\(\sim\)</span>) bedeutet, dass die Teststatistik einer bestimmten Verteilung folgt, hier der t-Verteilung. Dies ergibt sich wiederum aus den oben erwähnten Regressionsannahmen. Die Annahmen sind die gleichen wie beim üblichen t-Test der Mittelwerte (Kapitel <a href="09-ttest.html#ttest">9</a> und <a href="10-ftest.html#ftest">10</a>), außer dass im Fall der linearen Regression die Residuen als u.i.v. normal angenommen werden, während im Fall der Mittelwerte die tatsächlichen Datenpunkte <span class="math inline">\(y\)</span> als u.i.v. normal angenommen werden.</p>
<p>Analog zum üblichen 2-seitigen t-Test ist der p-Wert definiert als:
<span class="math display" id="eq:pv">\[\begin{equation}
2 \cdot \Pr\left(t&gt;|t_s|\right)=2 \cdot \left(1-F_t\left(|t_s|\right)\right)
\tag{12.27}
\end{equation}\]</span></p>
<p>Das Symbol <span class="math inline">\(F_t\left(|t_s|\right)\)</span> bezeichnet den Wert der kumulativen Verteilungsfunktion der t-Verteilung an der Stelle des absoluten Wertes der Teststatistik (<span class="math inline">\(|t_s|\)</span>, Abbildung <a href="12-regression.html#fig:tc">12.6</a>). Mit einem Signifikanzniveau von z.B. <span class="math inline">\(\alpha=0.01\)</span> gelangen wir zu einem kritischen Wert der Teststatistik <span class="math inline">\(t_c=t_{n-2;0.995}\)</span>, bei dessen Überschreitung wir die Nullhypothese ablehnen und die Parameterschätzer als signifikant bezeichnen (Abbildung <a href="12-regression.html#fig:tc">12.6</a>).</p>
<div class="figure" style="text-align: center"><span id="fig:tc"></span>
<img src="figs/tc.jpg" alt="Schema des t-Tests auf Signifikanz der Parameterschätzer. Die Teststatistik folgt einer t-Verteilung unter der Nullhypothese. Der tatsächliche Wert der Teststatistik $t_s$ ist blau markiert und wird für den 2-seitigen Test bei Null gespiegelt. Der kritische Wert der Teststatistik $t_c$, den wir von einem Signifikanzniveau von $\alpha=0.01$ erhalten, ist rot markiert; auch dieser wird für den 2-seitigen Test gespiegelt. Wir lehnen die Nullhypothese ab, wenn $|t_s|&gt;t_c$, d.h. für Werte von $t_s$ kleiner als $-t_c$ und größer als $t_c$, und nennen diese Parameterschätzer dann signifikant. Wir behalten die Nullhypothese bei wenn $|t_s|\leq t_c$, d.h. für Werte von $t_s$ zwischen $-t_c$ und $t_c$, und nennen diesen Parameterschätzer dann (vorläufig) nicht signifikant. In dem gezeigten Beispiel ist der Parameterschätzer nicht signifikant." width="80%" />
<p class="caption">
Abbildung 12.6: Schema des t-Tests auf Signifikanz der Parameterschätzer. Die Teststatistik folgt einer t-Verteilung unter der Nullhypothese. Der tatsächliche Wert der Teststatistik <span class="math inline">\(t_s\)</span> ist blau markiert und wird für den 2-seitigen Test bei Null gespiegelt. Der kritische Wert der Teststatistik <span class="math inline">\(t_c\)</span>, den wir von einem Signifikanzniveau von <span class="math inline">\(\alpha=0.01\)</span> erhalten, ist rot markiert; auch dieser wird für den 2-seitigen Test gespiegelt. Wir lehnen die Nullhypothese ab, wenn <span class="math inline">\(|t_s|&gt;t_c\)</span>, d.h. für Werte von <span class="math inline">\(t_s\)</span> kleiner als <span class="math inline">\(-t_c\)</span> und größer als <span class="math inline">\(t_c\)</span>, und nennen diese Parameterschätzer dann signifikant. Wir behalten die Nullhypothese bei wenn <span class="math inline">\(|t_s|\leq t_c\)</span>, d.h. für Werte von <span class="math inline">\(t_s\)</span> zwischen <span class="math inline">\(-t_c\)</span> und <span class="math inline">\(t_c\)</span>, und nennen diesen Parameterschätzer dann (vorläufig) nicht signifikant. In dem gezeigten Beispiel ist der Parameterschätzer nicht signifikant.
</p>
</div>
<p>Jetzt verstehen wir auch die restlichen Informationen des Outputs der <code>lm()</code> Funktion (s. oben):</p>
<div class="sourceCode" id="cb56"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb56-1"><a href="12-regression.html#cb56-1"></a><span class="kw">coef</span>(<span class="kw">summary</span>(reise_fit))</span></code></pre></div>
<pre><code>##             Estimate Std. Error t value  Pr(&gt;|t|)
## (Intercept)   4.9232    1.01874   4.833 7.533e-06
## stationen     0.6934    0.05976  11.604 4.641e-18</code></pre>
<p>Wie bereits oben erwähnt steht die Zeile “(Intercept)” für <span class="math inline">\(\beta_0\)</span> und die Zeile “stationen” für <span class="math inline">\(\beta_1\)</span>. In Spalte “Estimate” stehen die Werte der Parameterschätzer. In Spalte “Std. Error” stehen deren Standardfehler (Gleichungen <a href="12-regression.html#eq:seb0">(12.14)</a> und <a href="12-regression.html#eq:seb1">(12.15)</a>). In Spalte “t value” stehen die entsprechenden Werte der Teststatistik (Gleichungen <a href="12-regression.html#eq:tsb0">(12.25)</a> und <a href="12-regression.html#eq:tsb1">(12.26)</a>). In Spalte “Pr(&gt;|t|)” stehen die p-Werte der t-Tests auf Signifikanz der Parameter (Gleichung <a href="12-regression.html#eq:pv">(12.27)</a>). Wir sehen, dass in unserem Beispiel beide Parameter signifikant sind (die p-Werte sind wesentlich kleiner als das konventionelle <span class="math inline">\(\alpha=0.01\)</span>).</p>
</div>
<div id="güte-der-modellanpassung" class="section level2">
<h2><span class="header-section-number">12.7</span> Güte der Modellanpassung</h2>
<p>Die Signifikanz der Parameter der Regression ist eine Sache. Wie gut aber ist das Modell im Beschreiben der Daten? D.h. wieviel von der Varianz in den Daten wird vom Modell erklärt? Die Güte der Modellanpassung kann in erster Linie mit dem <strong>Bestimmtheitsmaß</strong> (<span class="math inline">\(r^2\)</span>) begutachtet werden, welches als Anteil der Varianz (in <span class="math inline">\(y\)</span>-Richtung) definiert ist, der durch das Modell erklärt wird:
<span class="math display" id="eq:r2">\[\begin{equation}
r^2=\frac{SSY-SSE}{SSY}=1-\frac{SSE}{SSY}
\tag{12.28}
\end{equation}\]</span>
Das Bestimmtheitsmaß ist der Korrelationskoeffizient nach Bravais-Pearson zum Quadrat (vgl. Kapitel <a href="05-korrelation.html#korrelation">5</a>).</p>
<p>Wie wir an Gleichung <a href="12-regression.html#eq:r2">(12.28)</a> sehen, wenn das Modell nicht mehr Variation als die Gesamtvariation um den Mittelwert erklärt, d.h. <span class="math inline">\(SSE=SSY\)</span> ist, dann ist <span class="math inline">\(r^2=0\)</span>. Umgekehrt, wenn das Modell perfekt zu den Daten passt, d.h. <span class="math inline">\(SSE=0\)</span> ist, dann ist <span class="math inline">\(r^2=1\)</span>. Werte dazwischen stellen unterschiedliche Grade der Anpassungsgüte dar. Das kann wiederum mit Abbildung <a href="12-regression.html#fig:ssysse">12.3</a> veranschaulicht werden, wobei der linke Teil <span class="math inline">\(SSY\)</span> und der rechte Teil <span class="math inline">\(SSE\)</span> verdeutlicht.</p>
<p>Wenn es darum geht, Modelle unterschiedlicher Komplexität (d.h. mit mehr oder weniger Parametern) mit <span class="math inline">\(r^2\)</span> zu vergleichen, dann ist es sinnvoll, das Bestimmtheitsmaß mit der Anzahl der Modellparameter zu korrigieren, da komplexere Modelle (mehr Parameter) automatisch zu besseren Anpassungen führen, einfach aufgrund der größeren Freiheitsgrade, die komplexere Modelle bei der Anpassung der Daten haben. Dies führt zum <strong>korregierten <span class="math inline">\(r^2\)</span></strong>:
<span class="math display" id="eq:adjr2">\[\begin{equation}
\bar r^2=1-\frac{\frac{SSE}{df_{SSE}}}{\frac{SSY}{df_{SSY}}}=1-\frac{SSE}{SSY} \cdot \frac{df_{SSY}}{df_{SSE}}
\tag{12.29}
\end{equation}\]</span></p>
<p>Rechnen wir <span class="math inline">\(r^2\)</span> und <span class="math inline">\(\bar r^2\)</span> für die Regression der Reisedaten aus:</p>
<div class="sourceCode" id="cb58"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb58-1"><a href="12-regression.html#cb58-1"></a><span class="kw">summary</span>(reise_fit)<span class="op">$</span>r.squared</span></code></pre></div>
<pre><code>## [1] 0.6548</code></pre>
<div class="sourceCode" id="cb60"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb60-1"><a href="12-regression.html#cb60-1"></a><span class="kw">summary</span>(reise_fit)<span class="op">$</span>adj.r.squared</span></code></pre></div>
<pre><code>## [1] 0.6499</code></pre>
<p>D.h. rund 65% der Varianz in den Entfernungs-Daten wird durch das lineare Modell mit Anzahl Stationen als Prädiktor erklärt.</p>
<p>Aber was heißt Güte der Modellanpassung? Sind alle Modellannahmen erfüllt? Folgende Annahmen ergeben sich aus der <strong>Maximum-Likelihood-Theorie</strong> (vgl. Schätzen von Verteilungsparametern, Kapitel <a href="08-schaetzen.html#schaetzen">8</a>):</p>
<ul>
<li>Die Residuen sind <strong>unabhängig</strong>, in diesem Fall gibt es keine serielle Korrelation in der Residuengrafik - dies kann mit dem Durbin-Watson-Test getestet werden</li>
<li>Die Residuen sind <strong>normalverteilt</strong> - dies kann visuell mit Hilfe des Quantil-Quantil-Diagramms (QQ-Plot) und dem Residuen-Histogramm beurteilt werden, und kann mit dem Kolmogorov-Smirnov-Test (Kapitel <a href="11-chi2test_kstest.html#chi2testkstest">11</a>) und dem Shapiro-Wilk-Test getestet werden</li>
<li>Die Varianz ist für alle Residuen konstant (die Residuen sind <strong>homoskedastisch</strong>), d.h. es erfolgt kein “Auffächern” der Residuen</li>
</ul>
<p>Sind diese Annahmen nicht erfüllt, können wir auf Datentransformation, gewichtete Regression oder Generalisierte Lineare Modelle zurückgreifen. Letzteres ist wird im Master <em>Global Change Geography</em> unterrichtet.</p>
<p>Eine erste nützliche diagnostische Darstellung ist die der Residuen in Serie, d.h. nach Index <span class="math inline">\(i\)</span>, um zu sehen, ob es ein Muster aufgrund des Datenerfassungsprozesses gibt. Für unser Beispiel:</p>
<div class="sourceCode" id="cb62"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb62-1"><a href="12-regression.html#cb62-1"></a><span class="co"># Residuen gegen Index plotten</span></span>
<span id="cb62-2"><a href="12-regression.html#cb62-2"></a><span class="kw">plot</span>(<span class="kw">residuals</span>(reise_fit), <span class="dt">pch =</span> <span class="dv">19</span>, <span class="dt">type =</span> <span class="st">&#39;p&#39;</span>, <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">25</span>,<span class="dv">25</span>))</span>
<span id="cb62-3"><a href="12-regression.html#cb62-3"></a><span class="kw">abline</span>(<span class="dt">h =</span> <span class="dv">0</span>, <span class="dt">lwd =</span> <span class="dv">3</span>, <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<p><img src="eids_files/figure-html/unnamed-chunk-31-1.png" width="672" />
Dieser Datensatz zeigt kein erkennbares Muster, was für eine Unabhängigkeit der Residuen spricht.</p>
<p>Wir sollten auch die Residuen nach dem modellierten Wert von <span class="math inline">\(y\)</span> plotten, um zu sehen, ob es ein Muster als Funktion der Größenordnung von <span class="math inline">\(y\)</span> gibt:</p>
<div class="sourceCode" id="cb63"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb63-1"><a href="12-regression.html#cb63-1"></a><span class="co"># Residuen gegen modellierte Werte von y plotten</span></span>
<span id="cb63-2"><a href="12-regression.html#cb63-2"></a><span class="kw">plot</span>(<span class="kw">fitted.values</span>(reise_fit),<span class="kw">residuals</span>(reise_fit), <span class="dt">pch =</span> <span class="dv">19</span>, <span class="dt">type =</span> <span class="st">&#39;p&#39;</span>, <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">25</span>,<span class="dv">25</span>))</span>
<span id="cb63-3"><a href="12-regression.html#cb63-3"></a><span class="kw">abline</span>(<span class="dt">h =</span> <span class="dv">0</span>, <span class="dt">lwd =</span> <span class="dv">3</span>, <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<p><img src="eids_files/figure-html/unnamed-chunk-32-1.png" width="672" />
Diese Grafik zeigt eine leichte Zunahme der Residuenvarianz von links nach rechts, d.h. in Richtung größerer Werte - eine Form der Heteroskedastizität.</p>
<p>Die Annahme, dass die Residuen normalverteilt sind, kann anhand des QQ-Plots beurteilt werden:</p>
<div class="sourceCode" id="cb64"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb64-1"><a href="12-regression.html#cb64-1"></a><span class="kw">qqnorm</span>(<span class="kw">residuals</span>(reise_fit))</span>
<span id="cb64-2"><a href="12-regression.html#cb64-2"></a><span class="kw">qqline</span>(<span class="kw">residuals</span>(reise_fit))</span></code></pre></div>
<p><img src="eids_files/figure-html/unnamed-chunk-33-1.png" width="672" /></p>
<p>Im QQ-Plot stellt jeder Datenpunkt ein bestimmtes Quantil der empirischen Verteilung dar. Dieses Quantil (nach Standardisierung) wird gegen den Wert dieses Quantils geplotted (vertikale Achse), der unter einer Standard-Normalverteilung (horizontale Achse) erwartet wird. Die resultierenden Formen sagen etwas über die Verteilung der Residuen aus. Im Fall einer Normalverteilung zum Beispiel, fallen alle Residuen auf eine gerade Linie (vgl. Kapitel <a href="08-schaetzen.html#schaetzen">8</a>). In unserem Beispiel deutet der QQ-Plot darauf hin, dass die Flanken der Residuenverteilung flacher als bei einer Normalverteilung abfallen, vermutlich auf aufgrund der o.g. Heteroskedastizität. Das können wir beim Histogramm der Residuen nicht so gut sehen da die Vergleichverteilung fehlt:</p>
<div class="sourceCode" id="cb65"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb65-1"><a href="12-regression.html#cb65-1"></a><span class="co"># Histogramm der Residuen plotten</span></span>
<span id="cb65-2"><a href="12-regression.html#cb65-2"></a><span class="kw">hist</span>(<span class="kw">residuals</span>(reise_fit), <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">25</span>,<span class="dv">25</span>))</span></code></pre></div>
<p><img src="eids_files/figure-html/unnamed-chunk-34-1.png" width="672" />
Die Heteroskedastizität würde man angehen, indem man mit einem Generalisierten Linearen Modell eine andere Residuenverteilung als die Normalverteilung annimmt. Das führt aber im Rahmen dieses Kurses zu weit. Es bleibt somit festzuhalten, dass das Regressionsproblem mit den hier behandelten Methoden noch nicht vollständig gelöst ist.</p>

</div>
</div>



<h3>Literatur</h3>
<div id="refs" class="references">
<div id="ref-wainer2009">
<p>Wainer, H. 2009. <em>Picturing the Uncertain World</em>. Princeton: Princeton University Press.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="10">
<li id="fn10"><p>Das ist korrekt wenn wir von linearer Regression im engen Sinn sprechen, obwohl Regressionsprobleme mit nominal oder ordinal skalierten unabhängigen Variablen mathematisch identisch sind. Auch Regressionsprobleme mit nominal oder ordinal skalierten abhängigen Variablen sind mathematisch ähnlich. Das wird im Masterstudiengang <strong>Global Change Geography</strong> gelehrt.<a href="12-regression.html#fnref10" class="footnote-back">↩︎</a></p></li>
<li id="fn11"><p>Summenregel: wenn <span class="math inline">\(y=u(t) \pm v(t)\)</span> dann <span class="math inline">\(\frac{dy}{dt}=\frac{du}{dt} \pm \frac{dv}{dt}\)</span><a href="12-regression.html#fnref11" class="footnote-back">↩︎</a></p></li>
<li id="fn12"><p>Kettenregel: wenn <span class="math inline">\(y=f[g(t)]\)</span> dann <span class="math inline">\(\frac{dy}{dt}=\frac{df[g]}{dg} \cdot \frac{dg}{dt}\)</span>, d.h. “äußere mal innere Ableitung”<a href="12-regression.html#fnref12" class="footnote-back">↩︎</a></p></li>
<li id="fn13"><p>Wenn wir jedoch die unabhängige Variable “stationen” zentrieren würden, d.h. von allen Datenpunkten den Mittelwert abziehen würden, dann ergäbe sich ein Achsenabschnitt, der die Entfernung für die mittlere Anzahl Stationen darstellen würde.<a href="12-regression.html#fnref13" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="11-chi2test_kstest.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="13-refs.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/lunr.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
