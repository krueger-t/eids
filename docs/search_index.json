[["index.html", "Einführung in die Statistik Vorwort", " Einführung in die Statistik Tobias Krueger 2021-10-09 Vorwort Dies ist das Skript für den Kurs Einführung in die Statistik am Geographischen Institut der Humboldt-Universität zu Berlin. Danksagung: Besonderer Dank gebührt Kassandra Jensch und Maeve Smyth für die Unterstützung bei der Erstellung dieses Skriptes. License: This script is licensed under the Creative Commons Zero v1.0 Universal license. You can learn the details here. In short, you can use my work. Just make sure you give me the appropriate credit the same way you would for any other scholarly resource. "],["01-einfuehrung.html", "Kapitel 1 Einführung 1.1 Statistik im empirischen Forschungsprozess 1.2 Warum Statistik? 1.3 Mathematische Notation und Grundlagen", " Kapitel 1 Einführung 1.1 Statistik im empirischen Forschungsprozess Lesen Sie dazu bitte Kapitel 2.4 von Zimmermann-Janschitz (2014), s. Moodle. Gemäß der dort gewählten Kategorisierung befasst sich die Statistik hauptsächlich mit Datenanalyse (Punkt 7), obwohl die angrenzenden Schritte ebenfalls wichtig sind. Auf Auswahl der Untersuchungseinheit (Punkt 4), Datenerhebung (Punkt 5) und Datenaufbereitung (Punkt 6) werden wir in Kapitel 2 näher eingehen. Punkt 8 (Interpretation und Rückschlüsse) wird durchgehend eine Rolle spielen. 1.2 Warum Statistik? Statistik ist Teil des physisch- und humangeographischen Methodenpakets. Da die Erkenntnisse der Geographie in vielen Teilen auf dem empirischen Forschungsprozess basieren ist statistische Analyse als Argumentationsunterstützung und als Beweissicherung unumgänglich. Außerdem dient sie der Bestätigung oder Widerlegung theoretischer Ansätze und der Generierung neuer Informationen aus verfügbaren Daten. In der Praxis dient Statistik häufig als Entscheidungsgrundlage. Lesen Sie bitte Kapitel 2.3 (Zimmermann-Janschitz 2014) für konkrete Anwendungsbeispiele der Statistik in der Geographie. Am Ende jenes Kapitels finden Sie außerdem eine Erklärung der Teilbereiche der Statistik. Mit der deskriptiven (beschreibenden) Statistik beschäftigen wir uns in den Wochen 3 - 5. Dazu ist das Lehrbuch von Zimmermann-Janschitz (2014) wichtig. Mit der induktiven (schließenden) Statistik beschäfigen wir uns in den Wochen 8 - 13. Die Brücke zwischen diesen beiden Teilbereichen - wie es Zimmermann-Janschitz (2014) darstellt - ist die Wahrscheinlichkeitstheorie, mit deren Grundlagen wir uns in den Wochen 6 - 7 auseinandersetzen. 1.3 Mathematische Notation und Grundlagen In diesem Unterkapitel werden wichtige Begriffe eingeführt und wichtige mathematische Grundlagen aus der Schule wiederholt. Lesen Sie bitte dazu Kapitel 1.2 von Zimmermann-Janschitz (2014). Dort werden anhand des Beispieles der Kostenaufstellung für eine Statistikexkursion die Begriffe Variable, Index und Summe eingeführt. Variable wird synonym mit Merkmal verwendet. In den Zeilen der Tabelle 1.1 in Zimmermann-Janschitz (2014) stehen dann die einzelnen Merkmalswerte oder einfach nur Werte für die Untersuchungselemente (statistische Einheiten). Jede statistische Einheit ist gekennzeichnet durch einen eigenen Index. An dieser Stelle sei ergänzt, dass ein Index auch unterschiedliche Variablen bezeichnen kann, z.B. \\(x_1, x_2, \\ldots\\). Die Summe verschiedener Merkmalswerte wird wie folgt abgekürzt: \\[\\begin{equation} \\sum_{i=1}^{n}x_i=x_1+x_2+\\ldots+x_{n-1}+x_n \\tag{1.1} \\end{equation}\\] Das Summenzeichen \\(\\Sigma\\) symbolisiert die Anweisung, die Merkmalswerte \\(x_i\\) zu addieren, wobei der Index \\(i\\) von 1 bis zur Anzahl der Werte \\(n\\) läuft. Eine ähnliche verkürzte Schreibweise gibt es für das Produkt: \\[\\begin{equation} \\prod_{i=1}^{n}x_i=x_1 \\cdot x_2 \\cdot \\ldots \\cdot x_{n-1} \\cdot x_n \\tag{1.2} \\end{equation}\\] Hier gibt das Produktzeichen \\(\\Pi\\) die Anweisung, die Merkmalswerte \\(x_i\\) zu multiplizieren, wobei wiederum der Index \\(i\\) von 1 bis zur Anzahl der Werte \\(n\\) läuft. Manchmal wird das Multiplikationszeichen weggelassen und es findet sich nur ein kleiner Abstand zwischen den zu multiplizierenden Größen: \\[\\begin{equation} \\prod_{i=1}^{n}x_i=x_1 \\, x_2 \\, \\ldots \\, x_{n-1} \\, x_n \\tag{1.3} \\end{equation}\\] Diese Schreibweise, die man häufig aus Platzgründen findet, impliziert in jedem Fall eine Multiplikation. Zwei weitere Begriffe, die Zimmermann-Janschitz (2014) nicht einführt, sind für diese Lehrveranstaltung noch wichtig, Vektor und Matrix: In einem Reihenvektor sind Größen (z.B. Merkmalswerte) horizontal angeordnet: \\[\\mathbf{x} = \\begin{pmatrix} x_1 &amp; x_2 &amp; \\cdots &amp; x_n \\end{pmatrix}\\] In einem Spaltevektor sind die Größen vertikal angeordnet: \\[\\mathbf{x} = \\begin{pmatrix} x_1\\\\ x_2\\\\ \\vdots\\\\ x_n \\end{pmatrix}\\] In einer Matrix sind Größen wie in einer Tabelle angeordnet, z.B. Merkmalswerte unterschiedlicher Variablen (Spalten): \\[\\mathbf{X} = \\begin{pmatrix} x_{1,1} &amp; x_{1,2} &amp; \\cdots &amp; x_{1,p}\\\\ x_{2,1} &amp; x_{2,2} &amp; \\cdots &amp; x_{2,p}\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ x_{n,1} &amp; x_{n,2} &amp; \\cdots &amp; x_{n,p} \\end{pmatrix}\\] Hier hat jeder Merkmalswert zwei Indizes, einen für die statistische Einheit (hier bis Anzahl \\(n\\)) und einen für die Variable (hier bis Anzahl \\(p\\)). Vektoren und Matrizen werden in der Regel fett gedruckt, wobei Vektoren \\(\\mathbf{x}\\) mit Kleinbuchstaben und Matrizen \\(\\mathbf{X}\\) mit Großbuchstaben bezeichnet werden. Die Rechenregeln für Vektoren und Matrizen sind in der linearen Algebra zusammengefasst. Wir werden daraus nur Auszüge in den letzten Semesterwochen verwenden. 1.3.1 Exponential- und Logarithmusfunktion Zwei mathematische Funktionen sind für diese Lehrveranstaltung besonders wichtig, die Exponential- und die Logarithmusfunktion. Die folgende Darstellung ist inspiriert von Gelman and Nolan (2002). Stellen Sie sich eine Amöbe vor, die sich innerhalb einer Stunde teilt (Abbildung 1.1). Diese zwei Amöben teilen sich jede in einer weiteren Stunde usw. Wie lautet die Gleichung für die Anzahl Amöben, \\(y\\), als Funktion der Zeit, \\(t\\) (in Stunden)? Abbildung 1.1: Sich teilende Amöbe. Quelle: http://www.gutenberg.org/files/18451/18451-h/images/illus002.jpg. Die Gleichung lautet: \\[\\begin{equation} y=2^t \\tag{1.4} \\end{equation}\\] Dies ist eine Exponentialfunktion mit Basis \\(2\\) und Exponent \\(t\\). Abbildung 1.2 zeigt zwei Plots dieser Funktion. (Den verwendeten R-Code werden sie im Verlauf der PC-Übung verstehen.) t &lt;- seq(1, 6) y &lt;- 2^t plot(t, y, pch = 19, type = &#39;b&#39;) plot(t, log(y), pch = 19, type = &#39;b&#39;) Abbildung 1.2: Links: Plot von Gleichung (1.4). Rechts: Plot von Gleichung (1.4) auf logarithmischer Skala. Die Umkehrfunktion der Exponentialfunktion ist die Logarithmusfunktion: \\[\\begin{equation} log(y)=log(2^t)=t \\cdot log(2) \\tag{1.5} \\end{equation}\\] Da der Logarithmus von \\(y\\) eine lineare Funktion von \\(t\\) ist (Gleichung (1.5)), zeigt die rechte Seite von Abbildung 1.2 (\\(y\\) auf logarithmischer Skala) eine gerade Linie. Übliche Basen der Logarithmusfunktion sind: \\[\\begin{equation} log_2\\left(2^t\\right)=lb\\left(2^t\\right)=t \\tag{1.6} \\end{equation}\\] Dies ist der sogenannte binäre Logarithmus (lb). \\[\\begin{equation} log_{10}\\left(10^t\\right)=lg\\left(10^t\\right)=t \\tag{1.7} \\end{equation}\\] Dies ist der sogenannte dekadische Logarithmus (lg). \\[\\begin{equation} log_e\\left(e^t\\right)=ln\\left(e^t\\right)=t \\tag{1.8} \\end{equation}\\] Dies ist der sogenannte natürliche Logarithmus (ln), wobei \\(e \\approx 2.7183\\) die Eulersche Zahl ist. Achtung! Programmiersprachen wie R nutzen oft eine andere Notation, der wir auch in diesem Kurs folgen: \\[\\begin{equation} ln()=log() \\tag{1.9} \\end{equation}\\] \\[\\begin{equation} e^t=\\exp(t) \\tag{1.10} \\end{equation}\\] Die Rechenregeln der Exponentialfunktion sind: \\[\\begin{equation} a^m \\cdot a^n=a^{m+n} \\tag{1.11} \\end{equation}\\] \\[\\begin{equation} a^n \\cdot b^n=(a \\cdot b)^n \\tag{1.12} \\end{equation}\\] \\[\\begin{equation} \\frac{a^m}{a^n}=a^{m-n} \\tag{1.13} \\end{equation}\\] \\[\\begin{equation} \\frac{a^n}{b^n}=\\left(\\frac{a}{b}\\right)^n \\tag{1.14} \\end{equation}\\] \\[\\begin{equation} \\left(a^m\\right)^n=a^{m \\cdot n} \\tag{1.15} \\end{equation}\\] Die Rechenregeln der Logarithmusfunktion sind: \\[\\begin{equation} log(u \\cdot v)=log(u)+log(v) \\tag{1.16} \\end{equation}\\] \\[\\begin{equation} log\\left(\\frac{u}{v}\\right)=log(u)-log(v) \\tag{1.17} \\end{equation}\\] \\[\\begin{equation} log\\left(u^r\\right)=r \\cdot log(u) \\tag{1.18} \\end{equation}\\] 1.3.2 Quadratische Funktion und Wurzelfunktion Abschließend sei noch die quadratische Funktion erwähnt (Abbildung 1.3, links): \\[\\begin{equation} f(x)=x^2 \\tag{1.19} \\end{equation}\\] Und ihre Umkehrfunktion, die Wurzelfunktion (Abbildung 1.3, rechts): \\[\\begin{equation} \\sqrt{x^2}=x \\tag{1.20} \\end{equation}\\] x &lt;- seq(0, 5, 0.1) plot(x, x^2, pch = 19, type = &#39;b&#39;) plot(x, sqrt(x^2), pch = 19, type = &#39;b&#39;) Abbildung 1.3: Links: Quadratische Funktion von \\(x\\). Rechts: Wurzelfunktion von \\(x^2\\), s. Gleichung (1.20). Literatur "],["02-begriffe.html", "Kapitel 2 Grundbegriffe und Datenerhebung 2.1 Statistische Grundbegriffe 2.2 Datenerhebung 2.3 Skalenniveaus", " Kapitel 2 Grundbegriffe und Datenerhebung Lesen Sie hierzu bitte Kapitel 3.1.2 und 3.1.3 von Zimmermann-Janschitz (2014). Im Folgenden finden Sie Leitfragen und Ergänzungen zu diesen Kapiteln. 2.1 Statistische Grundbegriffe Die statistische Masse umfasst all jene Elemente (Anzahl \\(N\\)), die für eine statistische Untersuchung Relevanz besitzen. Für die Bestimmung der statistischen Masse sind inhaltliche, zeitliche und räumliche Abgrenzungskriterien erforderlich. Alternative Begriffe sind (die geläufigsten fett gedruckt): statistische Grundmenge, Grundgesamtheit, Population, Kollektiv. Beispiele finden Sie in Zimmermann-Janschitz (2014), Kapitel 3.1.3. Die statistische Einheit \\(e_i\\) mit \\(i=1, \\ldots, n\\) stellt das Untersuchungselement und somit die kleinste, nicht weiter unterteilbare Einheit in einer statistischen Untersuchung dar. Diese statistische Einheit trägt jene Information (auch als Merkmal \\(X\\) bezeichnet), die im Zentrum der statistischen Untersuchung steht. Alternative Begriffe sind, je nach Kontext: Untersuchungseinheit, Proband, Merkmalsträger. Siehe Zimmermann-Janschitz (2014), Kapitel 3.1.3 für Beispiele. Jene Eigenschaft eines Untersuchungselements, die für die statistische Untersuchung von Bedeutung ist, wird als Merkmal \\(X\\) des Elements bezeichnet. Eine statistische Einheit weist mindestens ein Merkmal auf, kann aber ebenso durch mehrere Merkmale gekennzeichnet sein. Alternative Begriffe sind: Variable, Indikator. Beispiele finden Sie wiederum in Zimmermann-Janschitz (2014), Kapitel 3.1.3. Die Merkmalsausprägungen \\(a_j\\) mit \\(j=1, \\ldots, m\\) eines Merkmals \\(X\\) umfassen jene Manifestationen, die ein Merkmal im Rahmen einer statistischen Untersuchung annehmen kann. Alternative Begriffe sind: Merkmalskategorien, Modalitäten. Z.B. kann das Merkmal Schneedeckenhöhe Werte von null (kein Schnee vorhanden) bis zu mehreren Metern einnehmen. Der Merkmalswert \\(x_i\\) mit \\(i=1, \\ldots, n\\) schließlich ist jener Wert, den ein Merkmal \\(X\\) in einer statistischen Untersuchung tatsächlich annimmt. Alternative Begriffe sind: Beobachtungswert, Datum (Plural: Daten). Beispiel: Tatsächliche Schneedeckenmessung an einem Punkt von 285,5cm. 2.2 Datenerhebung Leitfragen zu Zimmermann-Janschitz (2014), Kapitel 3.1.2: Was ist der Unterschied zwischen Primärdaten und Sekundärdaten? Was sind die Vor- und Nachteile? Was sind Metadaten? Wozu sind sie wichtig? Was ist der Unterschied zwischen Gesamterhebung und Teilerhebung? Zur Primärdatenerhebung hören Sie mehr von Henning Nuissl im Seminar Einführung in die Geographie. An dieser Stelle aber noch ein paar Worte zur Auswahl einer Stichprobe aus einer Grundgesamtheit. Aus statistischer Perspektive sind dabei prinzipiell drei Aspekte zu beachten: Repräsentativität: Eine Stichprobe sollte die Variabilität der Grundgesamtheit möglichst genau abbilden, z.B. bezüglich Demographien oder räumlicher Unterschiede. Zufälligkeit: Jede Merkmalsausprägung der Grundgesamtheit sollte die gleiche Chance haben ausgewählt zu werden. In der Praxis ist dies oft nur näherungsweise möglich. Stichprobenumfang: Eine Stichprobe sollte ausreichend groß sein. Mehr dazu in der schließenden Statistik (Kapitel 8 - 10). 2.3 Skalenniveaus Die Skalenniveaus von Daten bestimmen den Informationsgehalt der Daten und damit das Analyse- und Interpretationspotenzial. In der Reihenfolge Nominalskala - Ordinalskala - metrische Skalen werden jeweils zusätzliche mathematische Operationen erschlossen (s. Zimmermann-Janschitz 2014, Tabelle 3.8, S. 79). Qualitative, klassifikatorische Merkmale befinden sich auf der Nominalskala. Die Kategorien können verbale Bezeichnungen oder Zahlencodes sein (Achtung: Die Zahl ist in dem Fall ein Code und keine natürliche Zahl mit der gerechnet werden kann.) Die zulässige mathematische Operation ist der Vergleich, d.h. Merkmalswerte von statistischen Einheiten sind entweder gleich oder verschieden. Beispiel: Art des Vulkanausbruchs (Tabelle 2.1). Obwohl \\(1+2=3\\) ist, ist Lava plus Gestein nicht gleich Gas! Tabelle 2.1: Art des Vulkanausbruchs. Lava Gestein Gas Asche 1 2 3 4 Qualitative, komparative Merkmale (Rangmerkmale) finden Sie auf der Ordinalskala. Wieder können die Kategorien verbale Bezeichnungen sein oder mittels Zahlen codiert. Die Zulässigen mathematischen Operationen sind der Vergleich sowie Wertung/Reihung/Ordnung. Es sind keine Aussagen über Distanz oder Ähnlichkeit benachbarter Merkmalsausprägungen möglich. Beispiel: Komfort der Unterkunft (Tabelle 2.2). Der Komfort eines ***Hotels ist größer als der Komfort eines *Hotels, aber nicht 3x so groß! Tabelle 2.2: Komfort der Unterkunft. Jugendherberge *Hotel **Hotel ***Hotel 0 1 2 3 Quantitative Merkmale befinden sich auf metrischen Skalen. Sie werden mit reellen Zahlen bezeichnet. Die zulässigen mathematischen Operationen sind der Vergleich, Wertung/Reihung/Ordnung sowie Addition/Subtraktion und im Falle der Rationalskala auch Multiplikation/Division. Die Unterscheidung Intervallskala und Rationalskala (Verhältnisskala) ist für uns nicht so wichtig. Intervallskalen fehlt ein natürlicher Nullpunkt und daher ist die Berechnung von Relationen nicht möglich. Sie kann aber auf einen Referenznullpunkt umgerechnet werden, wodurch Multiplikation und Division möglich wedren. Wenn wir also in diesem Kurs von einer metrischen Skala sprechen dann sind die mathematischen Operationen Vergleich, Wertung/Reihung/Ordnung, Addition/Subtraktion und Multiplikation/Division alle zulässig. Literatur "],["03-haeufigkeit.html", "Kapitel 3 Häufigkeiten und Lageparameter 3.1 Ziel der deskriptiven Statistik 3.2 Häufigkeiten 3.3 Lageparameter", " Kapitel 3 Häufigkeiten und Lageparameter Mit diesem Kapitel des Skriptes steigen wir in die deskriptive Statistik ein. Lesen Sie hierzu bitte Kapitel 3.2.1 und 3.2.2 von Zimmermann-Janschitz (2014). Im Folgenden finden Sie wie gehabt Leitfragen und Ergänzungen zu diesen Kapiteln. 3.1 Ziel der deskriptiven Statistik Jene Eigenschaft eines Untersuchungselements, die für die statistische Untersuchung von Bedeutung ist, wird als Merkmal \\(X\\) des Elements bezeichnet. Der Merkmalswert \\(x_i\\) mit \\(i=1,\\ldots,n\\) ist jener Wert, den ein Merkmal \\(X\\) in einer statistischen Untersuchung tatsächlich annimmt. Ziel der deskriptiven Statistik ist es, die Verteilung der Merkmalswerte eines Merkmals über den möglichen Wertebereich (Ausprägungen) mit einzelnen Parametern näher zu charakterisieren. Z.B. die Anzahl der Ausbrüche eines Vulkans an verschiedenen Tagen. Die Parameter sind: Lageparameter: Maße der zentralen Tendenz einer Verteilung (s. dieses Kapitel) Streuungsparameter: Maße der Variabilität einer Verteilung (s. Kapitel 4) Schiefe und Wölbung einer Verteilung (s. Kapitel 4) Dazu brauchen wir erstmal Begriffe wie absolute Häufigkeit, relative Häufigkeit und Summenhäufigkeit, sowie Diagramme wie Histogramme und Boxplots, die Verteilungen graphisch darstellen. 3.2 Häufigkeiten Schauen wir uns dazu die Reisedaten an, die Sie als Studierende in diesem Jahr eingegeben haben, und zwar nur die Entfernungen (der R-Code wird in den PC-Übungen schnell klar werden): # Paket laden, das für das Einlesen von xlsx gebraucht wird library(&quot;readxl&quot;) # Daten einlesen reisedat &lt;- read_excel(&quot;data/Daten_Distanz_Stationen.xlsx&quot;) # in Zahlen und data.frame umwandeln reisedat &lt;- as.data.frame(apply(reisedat, 2, as.numeric)) # Ausgabe von reisedat$distanz reisedat$distanz ## [1] 25.01 19.40 16.70 9.20 15.80 13.30 11.00 19.42 ## [9] 14.50 22.70 20.00 5.60 14.00 16.90 24.40 10.00 ## [17] 14.00 6.80 2.50 15.60 8.90 21.00 8.49 8.60 ## [25] 17.70 29.00 29.00 21.00 11.30 10.30 3.60 15.00 ## [33] 40.00 40.00 50.00 12.60 16.00 8.20 7.30 9.58 ## [41] 17.20 18.80 14.70 10.00 11.00 2.90 14.40 8.60 ## [49] 9.85 13.40 20.00 6.00 13.40 19.00 5.20 5.50 ## [57] 15.80 9.12 18.00 8.20 14.40 5.10 26.00 9.70 ## [65] 30.80 6.90 8.20 18.00 7.60 9.80 12.50 28.20 ## [73] 8.80 17.30 Das ist eine ungeordnete Reihe von 74 Datenpunkten, den sogenannten Rohdaten. Wenn wir die Rohdaten jetzt ordnen und in Klassen einteilen können wir absolute Häufigkeiten bestimmen, gewissermaßen durch Abzählen: # reisedat$distanz aufsteigend ordnen und ausgeben sort(reisedat$distanz) ## [1] 2.50 2.90 3.60 5.10 5.20 5.50 5.60 6.00 ## [9] 6.80 6.90 7.30 7.60 8.20 8.20 8.20 8.49 ## [17] 8.60 8.60 8.80 8.90 9.12 9.20 9.58 9.70 ## [25] 9.80 9.85 10.00 10.00 10.30 11.00 11.00 11.30 ## [33] 12.50 12.60 13.30 13.40 13.40 14.00 14.00 14.40 ## [41] 14.40 14.50 14.70 15.00 15.60 15.80 15.80 16.00 ## [49] 16.70 16.90 17.20 17.30 17.70 18.00 18.00 18.80 ## [57] 19.00 19.40 19.42 20.00 20.00 21.00 21.00 22.70 ## [65] 24.40 25.01 26.00 28.20 29.00 29.00 30.80 40.00 ## [73] 40.00 50.00 # absolute Häufigkeiten bestimmen für Klassen von 0 bis 55km, mit Breite 5km hist(reisedat$distanz, breaks = seq(0, 55, 5), main = &quot;&quot;, xlab = &quot;Entfernung (km)&quot;, ylab = &quot;absolute Häufigkeit&quot;) Diese Darstellung ist ein Histogramm. Dazu später mehr. Das Ordnen geschieht bei der Errechnung des Histogramms automatisch, hier haben wir den dazugehörigen R-Code nur der Anschaulichkeit halber eingefügt. Die absolute Häufigkeit, bezeichnet mit \\(h_j\\) für \\(h\\left(a_j\\right)\\) und \\(j=1,\\ldots,m\\), gibt also die Anzahl der statistischen Einheiten in einer Stichprobe an, welche die Merkmalsausprägung \\(a_j\\) für ein Merkmal \\(X\\) annehmen. In unserem Beispiel haben wir die Merkmalsausprägungen durch Klassifizierung gewissermaßen künstlich erzeugt, da die Menge der Ausprägungen des Merkmals Entfernung ja nicht abzählbar ist. Im Beispiel der Anzahl Vulkanausbrüche in Zimmermann-Janschitz (2014) gibt es dagegen abzählbare Merkmalsausprägungen. Die Summe der absoluten Häufigkeiten ist der Stichprobenumfang \\(n\\): \\[\\sum_{j=1}^{m}h_j=n \\quad\\text{mit}\\quad m\\leq n\\] Die relative Häufigkeit, bezeichnet mit \\(f_j\\) für \\(f\\left(a_j\\right)\\) und \\(j=1,\\ldots,m\\), gibt dann den Anteil der statistischen Einheiten an einer Stichprobe an, welche die Merkmalsausprägung \\(a_j\\) für ein Merkmal \\(X\\) annehmen: \\[f_j=f\\left(a_j\\right)=\\frac{h\\left(a_j\\right)}{n}\\] Das Histogramm bleibt gleich, nur dass die vertikale Achse anders skaliert ist. Da das in der hist()-Funktion nicht vorgesehen ist, ist der R-Code etwas länger: # Histogramm berechnen ohne Output h &lt;- hist(reisedat$distanz, breaks = seq(0, 55, 5), plot = FALSE) # absolute in relative Häufigkeiten umrechnen h$counts &lt;- h$counts / sum(h$counts) # Histogrammdaten plotten plot(h, freq = TRUE, col = &quot;gray&quot;, main = &quot;&quot;, xlab = &quot;Entfernung (km)&quot;, ylab = &quot;relative Häufigkeit&quot;) Interpretation: Der ersten Balken z.B. geht bis knapp unter 0.05, d.h. knapp 5% der Entfernungsdaten haben Werte zwischen 0 und 5km. Wenn wir uns das Histogramm der absoluten Häufigkeiten weiter oben anschauen, dann sehen wir, dass das 3 von 74 Datenpunkten sind. Die Summe der relativen Häufigkeiten ist 1, was 100% entspricht: \\[\\sum_{j=1}^{m}f_j=1 \\quad\\text{mit}\\quad m\\leq n\\] Kommen wir nun zu den Summenhäufigkeiten, auch genannt kumulative Häufigkeit oder kumulierte Häufigkeit. Die absolute Summenhäufigkeit einer Merkmalsausprägung \\(a_j\\) ist die Anzahl der Merkmalswerte, die kleiner oder gleich \\(a_j\\) sind. Die relative Summenhäufigkeit von \\(a_j\\) ist dementsprechend der Anteil der Merkmalswerte, die kleiner oder gleich \\(a_j\\) sind. Am besten visualisieren wir das kurz in R: # Histogramm berechnen ohne Output h &lt;- hist(reisedat$distanz, breaks = seq(0, 55, 5), plot = FALSE) # Häufigkeiten kumuliert aufsummieren h$counts &lt;- cumsum(h$counts) # Histogrammdaten plotten plot(h, freq = TRUE, col = &quot;gray&quot;, main = &quot;&quot;, xlab = &quot;Entfernung (km)&quot;, ylab = &quot;absolute Summenhäufigkeit&quot;) # absolute Summenhäufigkeiten in relative umrechnen h$counts &lt;- h$counts / max(h$counts) # Histogrammdaten plotten plot(h, freq = TRUE, col = &quot;gray&quot;, main = &quot;&quot;, xlab = &quot;Entfernung (km)&quot;, ylab = &quot;relative Summenhäufigkeit&quot;) Die Häufigkeiten der Klassen sind hier kumuliert aufsummiert, d.h. der letzte Balken ganz rechts hat die absolute Höhe 74, die Gesamtzahl der Datenpunkte \\(n\\), bzw. die relative Höhe 1 (100%). Sehen die dazu auch das Beispiel in Zimmermann-Janschitz (2014), Tabelle 3.10 auf Seite 87. Abschließend noch ein paar Worte zur Klassifizierung. Äquidistante Klassen, d.h. Klassen gleicher Breite, sind grundsätzlich zu bevorzugen. In R können Sie eine gewünschte Anzahl Klassen angeben, die dann äquidistant über den Wertebereich verteilt werden. Das ist sinnvoll für einen ersten Eindruck. Die Grundeinstellung von 10 Klassen ist dabei meist ausreichend. Im Laufe der Analyse wird es manchmal sinnvoller sein, bestimmte Klassen vorzugeben, auch (oder gerade) wenn manche Klasse in der betrachteten Stichprobe nicht vorkommen. Auch das ist in R möglich, indem Sie die Klassengrenzen (breaks) angeben. Probieren wir das in R aus: # Klassenbreite 5km hist(reisedat$distanz, breaks = seq(0, 60, 5), main = &quot;5km Klassenbreite&quot;, xlab = &quot;Entfernung (km)&quot;, ylab = &quot;absolute Häufigkeit&quot;) # Klassenbreite 10km hist(reisedat$distanz, breaks = seq(0, 60, 10), main = &quot;10km Klassenbreite&quot;, xlab = &quot;Entfernung (km)&quot;, ylab = &quot;absolute Häufigkeit&quot;) # Klassenbreite 20km hist(reisedat$distanz, breaks = seq(0, 60, 20), main = &quot;20km Klassenbreite&quot;, xlab = &quot;Entfernung (km)&quot;, ylab = &quot;absolute Häufigkeit&quot;) Je größer die Klassenbreite, desto mehr Nuancen der Verteilung der Werte gehen verloren. Je kleiner die Klassenbreite, desto mehr Lücken entstehen im Histogramm und die generelle Form der Verteilung tritt in den Hintergrund. Eine geeignete mittlere Klassenbreite wird man nur durch Ausprobieren hinbekommen. Siehe aber Zimmermann-Janschitz (2014), S. 91 - 102 für Richtlinien zur Klassenbildung. 3.3 Lageparameter Lageparameter sind Maße der zentralen Tendenz einer Häufigkeitsverteilung. Siehe Zimmermann-Janschitz (2014), Kapitel 3.2.2. Wichtig sind uns in dieser Veranstaltung der Modus, das arithmetische Mittel und der Median, weniger das harmonische Mittel und das geometrische Mittel, die Sie aber bei Zimmermann-Janschitz (2014) nachlesen können. Der Modus \\(\\bar x_{mod}\\) entspricht jener Merkmalsausprägung \\(a_j\\), die in der Häufigkeitsverteilung (lokal) am häufigsten vorkommt. In unseren Entfernungsdaten wäre das der Wert \\(7.5\\): h &lt;- hist(reisedat$distanz, breaks = seq(0, 55, 5), plot = FALSE) plot(h, freq = TRUE, col = &quot;gray&quot;, main = &quot;&quot;, xlab = &quot;Entfernung (km)&quot;, ylab = &quot;absolute Häufigkeit&quot;) # gib Klassenmitte aus (an der Stelle, wo die Häufigkeiten gleich dem Maximum sind) h$mids[h$counts == max(h$counts)] ## [1] 7.5 Bei künstlichen Klassen wie in unserem Beispiel wird typischerweise die Mitte der häufigsten Klasse angegeben. Sie sehen also, der Modus ist in diesem Fall abhängig von der gewählten Klassifizierung und nicht eindeutig. In der obigen Darstellung sehen wir außerdem weitere lokale Maxima bei \\(17.5\\), \\(27.5\\), \\(37.5\\) und \\(47.5\\), es handelt sich also in dieser Klassifizierung um eine multimodale Verteilung - eine Verteilung mit mehreren Modi. Ein Sonderfall der multimodalen Verteilung stellt die sogenannte bimodale Verteilung dar, bei der zwei lokale Maxima vorliegen. Das arithmetische Mittel \\(\\bar x\\) der Merkmalswerte \\(x_1, x_2, \\ldots, x_n\\) ist die Summe aller Merkmalswerte \\(x_i\\) relativ zur Stichprobengröße \\(n\\): \\[\\bar x=\\frac{\\sum_{i=1}^{n}x_i}{n}\\] Liegen absolute oder relative Häufigkeiten für die Merkmalsausprägungen \\(a_j\\) vor, kann das arithmetische Mittel \\(\\bar x\\) gewichtet berechnet werden: \\[\\bar x=\\frac{\\sum_{j=1}^{m}a_j\\cdot h_j}{n}=\\sum_{j=1}^{m}a_j\\cdot f_j\\quad\\text{mit}\\quad m\\leq n\\] Berechnen wir das arithmetische Mittel für unsere Entfernungsdaten mit der Funktion mean(): # berechne arithmetisches Mittel für Variable &quot;Distanz&quot; in km mean(reisedat$distanz) ## [1] 14.98 Andere Mittelwerte sind das geometrische Mittel und das harmonische Mittel, siehe Zimmermann-Janschitz (2014), S. 126 - 134. Der Median \\(\\bar x_{med}\\) schließlich entspricht jenem Merkmalswert \\(x_j\\) in einer Häufigkeitsverteilung, der eine geordnete Reihe von Merkmalswerten \\(x_1, x_2, \\ldots, x_n\\) in zwei gleich große Wertebereiche teilt. Für eine ungerade Anzahl von Merkmalswerten entspricht der Median dem mittleren Wert: \\[\\bar x_{med}=x_{\\frac{n+1}{2}}\\] Für eine gerade Anzahl von Merkmalswerten wird der Median aus dem arithmetischen Mittel der beiden mittleren Werte errechnet: \\[\\bar x_{med}=\\frac{x_{\\frac{n}{2}}+x_{\\frac{n}{2}+1}}{2}\\] Berechnen wir den Median für unsere Entfernungsdaten mit der Funktion median(): median(reisedat$distanz) ## [1] 13.7 Der Median wird auch 0.5-Quantil genannt. Allgemein entspricht ein p-Quantil \\(\\bar x_p\\) mit \\(0\\leq p\\leq 1\\) jenem Merkmalswert \\(x_j\\) in einer Häufigkeitsverteilung, der eine geordnete Reihe von Merkmalswerten \\(x_1, x_2, \\ldots, x_n\\) in zwei Wertebereiche teilt, so dass ein Anteil \\(p\\) der Werte kleiner oder gleich \\(x_j\\) ist. Ist das Produkt \\(n\\cdot p\\) nicht ganzzahlig, wird für \\(j\\) die dem Produkt nächstgrößere Zahl verwendet: \\[\\bar x_p=x_j\\] Ist das Produkt \\(n\\cdot p\\) ganzzahlig, dann ist \\(j=n\\cdot p\\): \\[\\bar x_p=\\frac{x_j+x_{j+1}}{2}\\] Auf Quantile werden wir im Zuge theoretischer Verteilungen noch näher zu sprechen kommen (s. Kapitel 7). Literatur "],["04-streuung.html", "Kapitel 4 Streuungsparameter, Schiefe und Wölbung 4.1 Streuungsparameter 4.2 Schiefe und Wölbung von Häufigkeitsverteilungen 4.3 Standardisierung (z-Transformation)", " Kapitel 4 Streuungsparameter, Schiefe und Wölbung 4.1 Streuungsparameter Lesen Sie dazu bitte Kapitel 3.2.3 von Zimmermann-Janschitz (2014). Streuungsparameter sind Maße der Variabilität einer Häufigkeitsverteilung. Uns interessieren hier v.a. Spannweite, Quartilsabstand, Varianz und Standardabweichung und Variationskoeffizient, weniger durchschnittliche absolute Abweichung, da wir letztere kaum in der Praxis sehen. Spannweite und Quartilsabstand lassen sich am besten mit einem sogenannten Box-Whisker-Plot, kurz einfach Boxplot, verdeutlichen (Abbildung 4.1). Ein Boxplot fasst die Verteilung der Werte eines Merkmals (in einer Stichprobe) zusammen. Die Spannweite ist der Abstand zwischen Minimum und Maximum der Merkmalswerte. Der Quartilsabstand ist der Abstand zwischen 0.25-Quantil und 0.75-Quantil; in diesem Bereich liegen 50% der Merkmalswerte (0.75 - 0.25). 0.25-Quantil, 0.5-Quantil (Median) und 0.75-Quantil heißen auch 1., 2. und 3. Quartil, weil sie den Wertebereich in vier gleichgroße Teile teilen: zwischen Minimum und 0.25-Quantil liegen 25% der Merkmalswerte, zwischen 0.25-Quantil und Median 25%, zwischen Median und 0.75-Quantil 25% und zwischen 0.75-Quantil und Maximum ebenfalls 25% aller Merkmalswerte. Ebenso gibt es auch Quintile usw., diese sind aber in der Praxis kaum von Bedeutung. Ein Boxplot kann horizontal (wie hier) und vertikal dargestellt werden. Abbildung 4.1: Boxplot mit Quartilsabstand und Spannweite. Der Boxplot ist eine vereinfachte Darstellung eines Histogramms. Schauen Sie sich dazu bitte Kapitel 4.3.6 von Zimmermann-Janschitz (2014) an, v.a. Abbildung 4.10. Erkennen Sie welcher Boxplot in 4.10b zu welchem Histogramm in 4.10c gehört? Die Entsprechung können Sie auch in unseren Reisedaten sehen (hier sowohl Distanz als auch Stationen): # Histogramm &quot;Distanz&quot; in km hist(reisedat$distanz, breaks = seq(0, 55, 5), main = &quot;&quot;, xlab = &quot;Entfernung (km)&quot;, ylab = &quot;absolute Häufigkeit&quot;) # Histogramm &quot;Stationen&quot; hist(reisedat$stationen, breaks = seq(0, 55, 5), main = &quot;&quot;, xlab = &quot;Anzahl Stationen&quot;, ylab = &quot;absolute Häufigkeit&quot;) # Boxplot &quot;Distanz&quot; in km boxplot(reisedat$distanz, range = 0, horizontal = TRUE, ylim = c(0, 55), xlab = &quot;Entfernung (km)&quot;) # Boxplot &quot;Stationen&quot; boxplot(reisedat$stationen, range = 0, horizontal = TRUE, ylim = c(0, 55), xlab = &quot;Anzahl Stationen&quot;) Sowohl Distanz als auch Stationen sind schief verteilt, die zentralen 50% der Verteilung - die Box im Boxplot - befinden sich links der Mitte. Für das Verständnis von Verteilungen in Kapitel 7 ist es wichtig, dass sie den Zusammenhang zwischen Histogramm und Boxplot verstehen! Nun zu den weiteren Streuungsparametern. Die Varianz \\(s^2\\) ist die mittlere (durchschnittliche) quadrierte Abweichung der Merkmalswerte \\(x_i\\quad\\left(i=1, 2, \\ldots, x_n\\right)\\) vom arithmetischen Mittel \\(\\bar x\\): \\[s^2=\\frac{\\sum_{i=1}^{n}\\left(x_i-\\bar x\\right)^2}{n-1}\\] Genau genommen ist das die korrigierte Varianz, wo durch \\(n-1\\) geteilt wird und nicht durch \\(n\\) wie man bei einer Mittelung erwarten könnte. Das Teilen durch \\(n-1\\) garantiert eine optimale Schätzung der Varianz der Grundgesamtheit anhand der Stichprobe - mehr dazu in der schließenden Statistik. Der Nenner \\(n-1\\) wird Anzahl Freiheitsgrade genannt und bezeichnet die Anzahl der Werte in einer Stichprobe, die für die Berechnung des Parameters (hier Varianz) frei zur Verfügung stehen. Im Fall der Varianz ist ein Wert der Stichprobe bereits belegt  durch das arithmetische Mittel. Daher reduziert sich die Zahl der Elemente der Stichprobe, die in die Berechnung eingehen um eins. Die Standardabweichung \\(s\\) ist die Quadratwurzel der mittleren quadrierten Abweichung der Merkmalswerte \\(x_i\\quad\\left(i=1, 2, \\ldots, x_n\\right)\\) vom arithmetischen Mittel \\(\\bar x\\), d.h. die Quadratwurzel der Varianz: \\[s=\\sqrt{s^2}=\\sqrt{\\frac{\\sum_{i=1}^{n}\\left(x_i-\\bar x\\right)^2}{n-1}}\\] Die Standardabweichung besitzt die gleiche Einheit wie die Merkmalswerte und ist deshalb einfacher zu interpretieren als die Varianz. Sie drückt die Streuung der Merkmalswerte um den Mittelwert bzw. deren Abweichung vom Mittelwert in einer anschaulichen Größe aus. Je größer die Werte der Standardabweichung sind, desto mehr streuen die Daten. Der Variationskoeffizient \\(v\\) einer Häufigkeitsverteilung mit den Merkmalswerten \\(x_i\\quad\\left(i=1, 2, \\ldots, x_n\\right)\\) schließlich ist die Standardabweichung \\(s\\) im Verhältnis zum Mittelwert \\(\\bar x\\): \\[v=\\frac{s}{\\bar x}\\] Der Variationskoeffizient setzt die Streuung der Merkmalswerte in unmittelbare Relation zum arithmetischen Mittel. Dadurch werden unterschiedliche Verteilungen vergleichbar. Schauen wir uns die Streungsparameter für die Reisedaten mittels R an: # &quot;Distanz&quot; in km # 1) arithmetisches Mittel dbar &lt;- mean(reisedat$distanz) # 2) Varianz s2d &lt;- var(reisedat$distanz) # 3) Standardabweichung sd &lt;- sqrt(s2d) # oder sd &lt;- sd(reisedat$distanz) # 4) Variationskoeffizient vd &lt;- sd / dbar # Ergebnisse: print(c(dbar, s2d, sd, vd)) ## [1] 14.9834 77.9118 8.8268 0.5891 # &quot;Stationen&quot; # 1) arithmetisches Mittel sbar &lt;- mean(reisedat$stationen) # 2) Varianz s2s &lt;- var(reisedat$stationen) # 3) Standardabweichung ss &lt;- sd(reisedat$stationen) # 4) Variationskoeffizient vs &lt;- ss / sbar # Ergebnisse: print(c(sbar, s2s, ss, vs)) ## [1] 13.8243 96.9139 9.8445 0.7121 Die Variable Stationen hat im Vergleich zu Distanz eine etwas größere Varianz bei etwas geringerem Mittelwert. Daher ist der Variationskoeffizient größer. 4.2 Schiefe und Wölbung von Häufigkeitsverteilungen Lesen Sie dazu bitte Kapitel 3.2.5 von Zimmermann-Janschitz (2014). Die Schiefe \\(a_3\\) einer Häufigkeitsverteilung von Merkmalswerten \\(x_1, x_2, \\ldots, x_n\\) mit dem arithmetischen Mittel \\(\\bar x\\) und der Standardabweichung \\(s\\) bezeichnet die Abweichung der Verteilung der Merkmalswerte von der symmetrischen Form: \\[a_3=\\frac{\\sum_{i=1}^{n}\\left(x_i-\\bar x\\right)^3}{n\\cdot s^3}\\] Für eine symmetrische Verteilung gilt: \\[a_3=0\\quad \\bar x_{mod}=\\bar x_{med}=\\bar x\\] D.h. Modus, Median und Arithmetisches Mittel sind identisch. Für eine sogenannte rechtsschiefe (linkssteile) Verteilung gilt: \\[a_3&gt;0\\quad \\bar x_{mod}&lt;\\bar x_{med}&lt;\\bar x\\] Für eine linkschiefe (rechtssteile) Verteilung gilt: \\[a_3&lt;0\\quad \\bar x_{mod}&gt;\\bar x_{med}&gt;\\bar x\\] Wie wir an Histogramm und Boxplot der Entfernungsdaten bereits gesehen haben, sind die Verteilungen der Merkmale Distanz und Stationen rechtsschief: # für diese Berechnung brauchen wir das Paket &quot;moments&quot; library(moments) # &quot;Distanz&quot; in km # 1) Schiefe skew_d &lt;- skewness(reisedat$distanz) # 2) Median med_d &lt;- median(reisedat$distanz) # 3) arithmetisches Mittel mean_d &lt;- mean(reisedat$distanz) # Ergebnisse: print(c(skew_d, med_d, mean_d)) ## [1] 1.516 13.700 14.983 Daraus folgt: Modus(=7.5) &lt; Median &lt; arithm. Mittelwert Das Merkmal &quot;Distanz&quot; ist rechtsschief verteilt. # &quot;Stationen&quot; # 1) Schiefe skew_s &lt;- skewness(reisedat$stationen) # 2) Median med_s &lt;- median(reisedat$stationen) # 3) arithmetisches Mittel mean_s &lt;- mean(reisedat$stationen) # Ergebnisse: print(c(skew_s, med_s, mean_s)) ## [1] 1.423 12.000 13.824 Daraus folgt: Modus(=7.5) &lt; Median &lt; arithm. Mittelwert Das Merkmal &quot;Stationen&quot; ist rechtsschief verteilt. Die Wölbung \\(a_4\\) einer Häufigkeitsverteilung von Merkmalswerten \\(x_1, x_2, \\ldots, x_n\\) mit dem arithmetischen Mittel \\(\\bar x\\) und der Standardabweichung \\(s\\) bestimmt die Steilheit einer Verteilung: \\[a_4=\\frac{\\sum_{i=1}^{n}\\left(x_i-\\bar x\\right)^4}{n\\cdot s^4}-3\\] Die Subtraktion von \\(-3\\) dient der Standardisierung auf die sogenannte Normalverteilung, eine symmetrische, glockenförmige Verteilung (s. Zimmermann-Janschitz 2014, Kapitel 3.2.5). Mehr zur Normalverteilung in Kapitel 7. Für eine Normalverteilung gilt: \\[a_4=0\\] Für eine spitzere Verteilung als die Normalverteilung gilt: \\[a_4&gt;0\\] Für eine flachere Verteilung als die Normalverteilung gilt: \\[a_4&lt;0\\] Die Verteilungen der Merkmale unserer Reisedaten sind beide spitzer als die Normalverteilung, wobei beide Variablen wegen ihrer Rechtsschiefe ohnehin nicht mit der Normalverteilung vergleichbar sind: # Wölbung der &quot;Distanz&quot; kurtosis(reisedat$distanz) - 3 ## [1] 3.038 # Wölbung der &quot;Stationen&quot; kurtosis(reisedat$stationen) - 3 ## [1] 2.266 4.3 Standardisierung (z-Transformation) Abschliessend sei noch die Standardisierung von Datenreihen erwähnt, auch genannt z-Transformation. Diese zentriert Datenreihen um Null und transformiert die Streuung der Daten so, dass verschiedene Datenreihen auf den selben Maßstab gebracht und somit vergleichbar werden. Mathematisch passiert das, indem wir von jedem Datenpunkt \\(x_i\\) in einer Stichprobe den Mittelwert der Stichprobe \\(\\bar x\\) abziehen und durch die Standardabweichug \\(s_x\\) teilen. Die so transformierten neuen Datenpunkte nennen wir üblicherweise \\(z_i\\): \\[\\begin{equation} z_i=\\frac{x_i-\\bar x}{s_x} \\tag{4.1} \\end{equation}\\] Der neue Mittelwert ist \\(\\bar z=0\\), die neue Standardabweichung ist \\(s_z=1\\). Sollten die \\(x\\) Daten einer sogenannten Normalverteilung folgen (s. Kapitel 7) dann folgen die \\(z\\) Daten einer Standardnormalverteilung. Literatur "],["05-korrelation.html", "Kapitel 5 Korrelationsanalyse 5.1 Nominalskalierte Merkmale: Kontingenztabelle und Chi-Quadrat Statistik 5.2 Ordinalskalierte Merkmale: Rangkorrelationskoeffizient nach Spearman 5.3 Metrische Merkmale: Scatterplot (Streudiagramm) und Korrelationskoeffizient nach Bravais-Pearson", " Kapitel 5 Korrelationsanalyse Lesen Sie hierzu bitte Kapitel 3.4.2 von Zimmermann-Janschitz (2014). Die Fragen, die uns besonders interessieren sind: Gibt es einen Zusammenhang zwischen zwei Merkmalen? Wenn ja, wie kann dieser Zusammenhang charakterisiert werden? Wie stark ist der Zusammenhang zwischen den Merkmalen? Wie kann man den Zusammenhang visualisieren? Die Verfahren sind unterschiedlich für nominalskalierte, ordinalskalierte und metrische Merkmale. Wir werden diese jetzt nacheinander einführen. Die Themen können Sie in Mittag (2016), Kapitel 8 und 9 vertiefen. 5.1 Nominalskalierte Merkmale: Kontingenztabelle und Chi-Quadrat Statistik Wir wollen diese Methode am Beispiel des Volksentscheids Tegel von 2017 verdeutlichen.1 Der Beschlussentwurf lautete: Der Flughafen Berlin-Tegel Otto-Lilienthal ergänzt und entlastet den geplanten Flughafen Berlin Brandenburg Willy Brandt (BER). Der Berliner Senat wird aufgefordert, sofort die Schließungsabsichten aufzugeben und alle Maßnahmen einzuleiten, die erforderlich sind, um den unbefristeten Fortbetrieb des Flughafens Tegel als Verkehrsflughafen zu sichern! Wir stellen uns die Frage: Gibt es einen Zusammenhang zwischen Abstimmungsverhalten und Bezirk? Man könnte meinen, dass Anwohner*innen in der Einflugschneise von Tegel eher für nein stimmten (d.h. für die Schließung), Anwohner*innen in der Einflugschneise des neuen BER dagegen eher für ja (d.h. gegen die Schließung). Überlegungen wie diese motivieren unsere Fragestellung. Die Kontingenztabelle (oder Kreuztabelle) zu diesem Beispiel finden Sie in Abbildung 5.1. Abbildung 5.1: Kontingenztabelle zum Abstimmungsverhalten im Volksentscheid Tegel 2017 nach Bezirk. Quelle: https://www.wahlen-berlin.de/wahlen/BU2017/afspraes/ve/index.html. In dieser Tabelle stehen absolute Häufigkeiten (vgl. Kapitel 3) für alle Kombinationen von Merkmalsausprägungen der Variablen \\(X\\) und \\(Y\\). \\(X\\) bezeichnet Bezirk mit den Ausprägungen \\(a_1, a_2, \\ldots, a_n\\). \\(Y\\) bezeichnet Abstimmungsverhalten mit den Ausprägungen \\(b_1\\) (ja) und \\(b_2\\) (nein). Die Spalten- bzw. Zeilensummen sind die sogenannten Randverteilungen von \\(Y\\) und \\(X\\). Ganz unten rechts steht der Stichprobenumfang (die Anzahl der Personen, die abgestimmt haben). Der Stichprobenumfang kann sowohl als Summe der Randverteilungswerte von \\(X\\), als auch als Summe der Randverteilungswerte von \\(Y\\) errechnet werden. Die allgemeine Notation der absoluten Häufigkeiten einer Kontingenztabelle finden Sie in Abbildung 5.2. In dieser Notation stehen \\(h_{\\cdot j}\\) und \\(h_{i \\cdot}\\) für die Werte der beiden Randverteilungen. Abbildung 5.2: Notation der absoluten Häufigkeiten in einer Kontingenztabelle. Nach: Mittag (2016). Wir können die Kontingenztabelle für dieses Beispiel auch mit relativen Häufigkeiten darstellen (Abbildung 5.3). Abbildung 5.3: Kontingenztabelle zum Abstimmungsverhalten im Volksentscheid Tegel mit relativen Häufigkeiten. Als nächstes müssen wir den Begriff der bedingten relativen Häufigkeit einführen. Die bedingte relative Häufigkeit ist die Häufigkeit einer Ausprägung des einen Merkmals relativ zur Häufigkeit einer bestimmten Ausprägung des zweiten Merkmals, d.h. bedingt dadurch, dass wir uns auf eine Ausprägung des zweiten Merkmals festlegen. Relative Häufigkeiten existieren somit in zwei Richtungen in einer Kontingenztabelle (Abbildung 5.4). Abbildung 5.4: Illustration relativer Häufigkeiten in einer Kontingenztabelle. Links: Absolute Häufigkeiten der Ausprägungen von \\(X\\) unter der Bedingung \\(Y=b_j\\). Rechts: Absolute Häufigkeiten der Ausprägungen von \\(Y\\) unter der Bedingung \\(X=a_i\\). Nach: Mittag (2016). Die Formeln dazu sind: \\[f_Y\\left(b_j|a_i\\right)=\\frac{h_{ij}}{h_{i\\cdot}}\\quad \\text{mit}\\quad j=1, 2, \\ldots, m\\] \\(f_Y\\left(b_j|a_i\\right)\\) steht für die relative Häufigkeit einer Merkmalsausprägung \\(b_j\\) von \\(Y\\), bedingt durch eine bestimme Ausprägung \\(a_i\\) von \\(X\\). Die Bedingtheit wird mit dem Zeichen \\(|\\) ausgedrückt. Und weiterhin: \\[f_X\\left(a_i|b_j\\right)=\\frac{h_{ij}}{h_{\\cdot j}}\\quad \\text{mit}\\quad i=1, 2, \\ldots, k\\] \\(f_X\\left(a_i|b_j\\right)\\) steht für die relative Häufigkeit einer Merkmalsausprägung \\(a_i\\) von \\(X\\), bedingt durch eine bestimme Ausprägung \\(b_j\\) von \\(Y\\). Beispiel: Die relative Häufigkeit von ja unter der Bedingung Charlottenburg-Wilmersdorf ist (vgl. 5.1): \\[f_Y\\left(b_1|a_4\\right)=\\frac{h_{41}}{h_{4\\cdot}}=\\frac{109799}{158471}=0.69\\] D.h. 69% der Stimmen in Charlottenburg-Wilmersdorf waren ja Stimmen. Die relative Häufigkeit von Charlottenburg-Wilmersdorf unter der Bedingung ja dagegen ist: \\[f_X\\left(a_4|b_1\\right)=\\frac{h_{41}}{h_{\\cdot 1}}=\\frac{109799}{994916}=0.11\\] D.h. 11% der ja Stimmen kamen aus Charlottenburg-Wilmersdorf. Was hat das alles mit dem Zusammenhang zwischen Abstimmungsverhalten und Bezirk zu tun? Die bedingten relativen Häufigkeiten helfen uns, etwas über die Abhängigkeit bzw. Unabhängigkeit der beiden Merkmale zu sagen. Das läuft über das Konzept der empirischen Unabhängigkeit: Intuitiv werden wir Unabhängigkeit von \\(X\\) und \\(Y\\) als gegeben ansehen, wenn die Ausprägung eines Merkmals keinen Einfluss auf die Ausprägung des anderen Merkmals hat. Dies bedeutet, dass eine bedingte Häufigkeitsverteilung für ein Merkmal nicht davon abhängt, welche Merkmalsausprägung für das andere Merkmal als Bedingung vorausgesetzt wird. D.h. der Anteil von Charlottenburg-Wilmersdorf an den ja Stimmen sollte etwa so groß sein wie der Anteil von Charlottenburg-Wilmersdorf an den nein Stimmen. Bzw. der Anteil der ja Stimmen in Charlottenburg-Wilmersdorf sollte etwa so groß sein wie der Anteil der ja Stimmen in jedem anderen Bezirk. In Formeln ausgedrückt heißt das: \\[f_X\\left(a_i|b_1\\right)=f_X\\left(a_i|b_2\\right)=\\cdots=f_X\\left(a_i|b_m\\right)\\] Bzw.: \\[\\frac{h_{i1}}{h_{\\cdot 1}}=\\frac{h_{i2}}{h_{\\cdot 2}}=\\cdots=\\frac{h_{im}}{h_{\\cdot m}}=\\frac{h_{i\\cdot}}{n}\\] Allgemein formuliert: \\[\\frac{h_{ij}}{h_{\\cdot j}}=\\frac{h_{i\\cdot}}{n}\\] Das Umstellen dieser Gleichung nach \\(h_{ij}\\) ergibt: \\[h_{ij}=\\frac{h_{i\\cdot}\\cdot h_{\\cdot j}}{n}:=\\tilde h_{ij}\\] Diese absolute Häufigkeit, die wir mit \\(\\tilde h_{ij}\\) bezeichnen, ist die bei empirischer Unabhängigkeit erwartete absolute Häufigkeit. Das Zeichen \\(:=\\) bedeutet wird definiert als. Wir haben uns also mit Hilfe der relativen Häufigkeiten in der Kontingenztabelle und unserer Konzeption der empirischen Unabhängigkeit neue, bei Unabhängigkeit erwartete absolute Häufigkeiten an jeder Stelle der Kontingenztabelle generiert. Diese können wir jetzt mit den tatsächlichen absoluten Häufigkeiten in der Kontingenztabelle vergleichen (Abbildung 5.5). Sind die Abweichungen klein können wir von Unabhängigkeit der Merkmale ausgehen. Sind die Abweichungen groß können wir von Abhängigkeit ausgehen. Abbildung 5.5: Vergleich der beobachteten absoluten Häufigkeiten \\(h_{ij}\\) und der bei empirischer Unabhängigkeit erwarteten absoluten Häufigkeiten \\(\\tilde h_{ij}\\). Als Vergleichsmaß wird die sogenannte Chi-Quadrat Statistik \\(\\mathcal{X}^2\\) (oder quadratische Kontingenz) verwendet, und das ist dann auch unser Zusammenhangsmaß für nominalskalierte Merkmale. \\(\\mathcal{X}^2\\) ergibt sich aus der Differenz der tatsächlichen Häufigkeiten \\(h_{ij}\\) und den erwarteten Häufigkeiten bei empirischer Unabhängigkeit \\(\\tilde h_{ij}\\): \\[\\mathcal{X}^2=\\sum_{i=1}^{k}\\sum_{j=1}^{m}\\frac{\\left(h_{ij}-\\tilde h_{ij}\\right)^2}{\\tilde h_{ij}}\\] Bei Unabhängigkeit der Merkmale ist \\(\\mathcal{X}^2=0\\). Bei vollständiger Abhängigkeit ist \\(\\mathcal{X}^2=\\mathcal{X}_{max}^2\\), wobei \\(\\mathcal{X}_{max}^2=n\\cdot(M-1)\\) mit \\(M=\\min(k;m)\\). D.h. der maximal mögliche Wert der \\(\\mathcal{X}^2\\) Statistik ergibt sich durch die Dimensionen der Kontingenztabelle, die Zeilenanzahl \\(k\\) und die Spaltenanzahl \\(m\\). Dadurch können wir leider die Größenordnung der \\(\\mathcal{X}^2\\) Statistik schlecht intuitiv einordnen, was unser Beispiel verdeutlicht: Für den Volksentscheid Tegel ergibt sich ein Wert von \\(\\mathcal{X}^2=49895.1\\), wobei \\(\\mathcal{X}_{max}^2=1732940\\). Da \\(\\mathcal{X}^2\\) näher an \\(0\\) ist als an \\(1 732 940\\), scheint die Abhängigkeit der Merkmale Bezirk und Abstimmungsverhalten gering zu sein. Ob sie dennoch statistisch signifikant ist können wir erst mit einem sogenannten Chi-Quadrat-Test beurteilen, den wir als Teil der schließenden Statistik in Kapitel 9.6 kennenlernen. 5.2 Ordinalskalierte Merkmale: Rangkorrelationskoeffizient nach Spearman Hier ist das Beispiel des Zusammenhangs von Grünflächenanteil und Bildungsgrad in Stadbezirken in Zimmermann-Janschitz (2014), Kapitel 3.4.2, S. 274 - 278 sehr anschaulich. Das Merkmal Grünflächenanteil ist zwar metrisch skaliert, muss aber ordinal skaliert werden, um es mit dem Merkmal Bildungsgrad vergleichbar zu machen, das bereits ordinal vorliegt. S. Tabelle 3.36 (Zimmermann-Janschitz 2014, S. 277). Der Rangkorrelationskoeffizient nach Spearman \\(r_s\\) für \\(n\\) Wertepaare \\(\\left(x_i,y_i\\right)\\) mit \\(i=1, 2, \\ldots, n\\) wird dann berechnet durch: \\[r_s=1-\\frac{6\\cdot\\sum_{i=1}^{n}d_i^2}{n\\cdot\\left(n^2-1\\right)}\\] Wobei \\(d_i\\) die Differenz der Ränge der beiden Merkmale \\(X\\) und \\(Y\\) ist. Bei perfekter negativer Rangkorrelation ist \\(r_s=-1\\). Bei perfekter positiver Rangkorrelation ist \\(r_s=1\\). Wenn keine Rangkorrelation vorliegt ist \\(r_s=0\\). Bei dem Grünflächenanteil/Bildungsgrad Beispiel von Zimmermann-Janschitz (2014) ist \\(r_s=0.84\\), die beiden Merkmale sind also stark positiv korreliert. D.h. größere Anteile von Grünflächen sind tendenziell mit höheren Bildungsgraden assoziiert und kleinere Anteile von Grünflächen sind tendenziell mit geringeren Bildungsgraden assoziiert. 5.3 Metrische Merkmale: Scatterplot (Streudiagramm) und Korrelationskoeffizient nach Bravais-Pearson An dieser Stelle können wir zu unseren Reisedaten zurückkehren. Beide Merkmale (Distanz und Stationen) sind metrisch skaliert2 und somit können wir den Korrelationskoeffizienten nach Bravais-Pearson verwenden, der mehr Informationen über den Zusammenhang, nämlich Linearität, liefert. Einen ersten Eindruck verschafft der Scatterplot (auch Streudiagramm), den wir einfach mit der plot() Funktion in R generieren: plot(reisedat$stationen, reisedat$distanz, xlab = &quot;Anzahl Stationen&quot;, ylab = &quot;Entfernung (km)&quot;) An dieser Stelle fällt das Wertepaar \\((0, 40)\\) oben links auf. Hierbei handelt es sich wahrscheinlich um eine Person, die gar nicht in Berlin wohnt. Anstatt \\(0\\) wollen wir hier aber einen fehlenden Wert erzeugen, der in R mit NA markiert wird, da diese Frage auf die Person gar nicht zutraf. Darstellungen wie diese sind somit enorm hilfreich für die Überprüfung der Daten! Ersetzen wir also \\(0\\) Stationen mit NA und plotten erneut: # 0 mit NA ersetzen reisedat$stationen[reisedat$stationen == 0] &lt;- NA # plotten plot(reisedat$stationen, reisedat$distanz, xlab = &quot;Anzahl Stationen&quot;, ylab = &quot;Entfernung (km)&quot;) Aufgabe: Überlegen Sie kurz, wie stark die Korrelation hier sein wird! (Auflösung weiter unten.) Überlegen Sie außerdem, woher die Streuung in Entfernung kommen könnte. Die lineare Korrelation zweier metrischer Merkmale wird üblicherweise mit dem Produkt-Moment-Korrelationskoeffizient nach Bravais-Pearson \\(r_{x,y}\\) für \\(n\\) Wertepaare \\(\\left(x_i,y_i\\right)\\) mit \\(i=1, 2, \\ldots, n\\) berechnet. Er ergibt sich aus den Standardabweichungen \\(s_x\\) und \\(s_y\\) und der standardisierten Kovarianz \\(s_{x,y}\\) durch: \\[r_{x,y}=\\frac{s_{x,y}}{s_x\\cdot s_y}=\\frac{\\frac{1}{n-1}\\cdot\\sum_{i=1}^{n}\\left(x_i-\\bar x\\right)\\cdot\\left(y_i-\\bar y\\right)}{\\sqrt{\\frac{1}{n-1}\\cdot\\sum_{i=1}^{n}\\left(x_i-\\bar x\\right)^2}\\cdot\\sqrt{\\frac{1}{n-1}\\cdot\\sum_{i=1}^{n}\\left(y_i-\\bar y\\right)^2}}\\] In R mit der cor() Funktion: cor(reisedat$stationen, reisedat$distanz, use = &quot;complete.obs&quot;, method = &quot;pearson&quot;) ## [1] 0.8092 Das Argument use = \"complete.obs\" teilt R mit, nur vollständige Datenpaare zu verwenden, d.h. ohne NA, ansonsten wäre der Output NA. Anstatt method = \"pearson\" sind ebenfalls method = \"spearman\" und method = \"kendall\" möglich; beides sind Rangkorrelationskoeffizienten. In Abbildung 5.6 sehen Sie Werte des Korrelationskoeffizienten für verschiedene Zusammenhänge. Bei augenscheinlich nicht-linearen Zusammenhängen wählt man üblicherweise einen Rangkorrelationskoeffizienten, der dann aussagekräftiger ist. Abbildung 5.6: Werte des Korrelationskoeffizienten für verschiedene Zusammenhänge. Quelle: https://upload.wikimedia.org/wikipedia/commons/thumb/0/02/Correlation_examples.png/440px-Correlation_examples.png. Literatur "],["06-wahrscheinlichkeit.html", "Kapitel 6 Grundlagen der Wahrscheinlichkeitsrechnung 6.1 Zufallsvorgänge 6.2 Ereignisse 6.3 Zufallsvariablen 6.4 Wahrscheinlichkeit 6.5 Axiome der Wahrscheinlichkeitstheorie nach Kolmogorow 6.6 Rechenregeln", " Kapitel 6 Grundlagen der Wahrscheinlichkeitsrechnung Die Wahrscheinlichkeitstheorie bildet die Brücke zwischen der deskriptiven und der induktiven Statistik, um bei dem Bild von Zimmermann-Janschitz (2014) zu bleiben. Sie liefert Modelle in der Form von Wahrscheinlichkeitsverteilungen (s. Kapitel 7), die es uns erlauben von Stichproben auf Grundgesamtheiten zu schließen. Dieses Kapitel motiviert zunächst Zufallsvorgänge und definiert ein paar grundlegende Begriffe. Sodann werden Ereignisse als Ergebnis eines Zufallsvorgangs über die Mengenleere eingeführt.3. Dazu gehört die Darstellung des Venn-Diagramms. Sodann werden Zufallsvariablen und Zufallsexperimente definiert, bevor wir zum Begriff der Wahrscheinlichkeit kommen. Dabei schauen wir uns besonders Konzepte wie bedingte Wahrscheinlichkeit und Unabhängigkeit von Ereignissen, sowie die Rechenregeln der Wahrscheinlichkeitsrechnung an. Im Quiz dieser Woche werden wir diese Regeln anwenden indem wir etwas mit Wahrscheinlichkeiten rechnen. 6.1 Zufallsvorgänge Wir alle kennen Zufallsvorgänge im Alltagsleben; z.B. Glücksspiele wie Roulette, Würfelspiele und die Ziehung der Lottozahlen. Aber auch Prozesse wie die Entwicklung von Börsenkursen, die Abschätzung von Schadensverläufen oder Lebenserwartungen (wichtig für Versicherungen) und Marktrisiken (wichtig für Unternehmen) werden als Zufallsvorgänge konzeptionalisiert und behandelt. Das sind Prozesse, die auf der Maßstabsebene, auf der wir sie betrachten gewisse Regularitäten aufweisen, die Glücksspielen ähneln und deshalb als Zufallsvorgänge betrachtet werden können. Das mag aber nur deshalb so sein weil wir die zugrunde liegenden Kausalketten nicht kennen oder nicht beschreiben können bzw. das zu aufwendig wäre. Das gleiche trifft übrigens auch auf Glücksspiele zu, deren Verlauf wir theoretisch auch physikalisch beschreiben könnten wenn wir nur alle Randbedingungen bestimmen könnten. Aus diesem Blickwinkel sind Zufallsvorgänge also immer nur grobskalige Modelle für Prozesse, die auf kleineren Skalen ablaufen und die wir nicht vollständig quantifizieren (können). Ob es am Ende einer Kausalkette unteilbare, echte Zufallsvorgänge gibt wird beispielsweise in der Physik debatiert. Die Wahrscheinlichkeitsrechnung stellt jedenfalls Modelle bereit, die es erlauben, den Verlauf zufallsabhängiger Prozesse abzuschätzen und von Stichproben auf Grundgesamtheiten zu schließen. Die bisher thematisierte beschreibende Statistik charakterisiert gegebene Datensätze ohne einen Rückschluss auf Eigenschaften umfassenderer Grundgesamtheiten zu vermitteln. Formal gesehen konzeptionalisieren wir Zufallsvorgänge wie folgt. Nehmen wir z.B. einen Würfelwurf. Der Wurf mit einem Würfel hat die sogenannte Ergebnismenge \\(\\Omega=\\{1,2,3,4,5,6\\}\\) aller möglicher sogenannter Elementarereignissen \\(\\omega=1,2,\\ldots,6\\) (Abbildung 6.1). Als Ereignis bezeichnen wir eine Teilmenge der Ergebnismenge; z.B. das Ereignis gerade Zahl \\(A=\\{2,4,6\\}\\). Abbildung 6.1: Ergebnismenge \\(\\Omega=\\{1,2,3,4,5,6\\}\\) eines Würfelwurfes. Nehmen wir nun das Beispiel eines Wurfes mit zwei Würfeln. Hier ist die Ergebnismenge \\(\\Omega=\\{(1;1),(1;2),\\ldots,(6;6)\\}\\) (Abbildung 6.2). Ein Ereignis ist z.B. Gesamtaugenzahl 7 \\(A=\\{(1;6),(2;5),(3;4),(4;3),(5;2),(6;1)\\}\\). Abbildung 6.2: Ergebnismenge \\(\\Omega=\\{(1;1),(1;2),\\ldots,(6;6)\\}\\) eines Wurfes mit zwei Würfeln. Ein Zufallsvorgang ist formal definiert als ein Prozess, der zu einem von mehreren, sich gegenseitig ausschließenden Ergebnissen \\(\\omega\\) führt. Welches Ergebnis eintritt, ist vorab nicht bekannt. Mögliche Ergebnisse \\(\\omega\\) heissen Elementarereignisse und die Ergebnismenge \\(\\Omega\\) ist definert als \\(\\Omega=\\{x : x \\text{ ist Elementarereignis}\\}\\). \\(\\Omega\\) kann endlich oder auch unendlich viele Elemente enthalten. Ein Beisiel für eine unendliche Ergebnismenge ist wie oft man Lotto spielen muss bis man Sechs Richtige und Zusatzzahl hat; \\(\\Omega=\\{1,2,\\ldots,\\infty\\}=\\mathbb{N}\\), wobei \\(\\mathbb{N}\\) die Menge der natürlichen Zahlen ist. Eine Teilmenge \\(A\\) von \\(\\Omega\\) heißt Ereignis. 6.2 Ereignisse Das sogenannte sichere Ereignis ist \\(A=\\Omega\\); z.B. bei einem Würfelwurf \\(A=\\Omega=\\{1,2,3,4,5,6\\}\\). Das sogenannte unmögliche Ereignis ist \\(A=\\bar\\Omega=\\emptyset\\), wobei \\(\\bar\\Omega\\) das Komplementärereignis (nicht \\(\\Omega\\)) und \\(\\emptyset\\) die leere Menge ist. Das Komplementärereignis lässt sich mit einem sogenannten Venn-Diagramm veranschaulichen (Abbildung 6.3). Das Venn-Diagramm kommt aus der Mengenleere und symbolsiert verschiedene Arten von Mengen (hier Ereignisse) als Flächen, deren Größe idealerweise proportional zur Größe der Menge ist, was aber in unseren Darstellungen nicht der Fall ist. Abbildung 6.3: Venn-Diagramm des Komplementärereignisses \\(\\bar A\\). Wenn das Rechteck die Ergebnismenge \\(\\Omega\\) ist dann sind alle Elementarereignisse, die nicht im Ereignis \\(A\\) (weiß) sind im Komplementärereignis \\(\\bar A\\) (grün).Nach: Mittag (2016). Die Schnittmenge zweier Ereignisse \\(A\\) und \\(B\\) wird mit \\(A\\cap B\\) symbolisiert; die Ereignisse \\(A\\) und \\(B\\) treten beide ein (Abbildung 6.4). Abbildung 6.4: Venn-Diagramm der Schnittmenge zweier Ereignisse \\(A\\cap B\\). Die grüne Fläche beschreibt die Schnittmenge, d.h. das Ereignis, dass \\(A\\) und \\(B\\) beide eintreten.Nach: Mittag (2016). Die Vereinigungsmenge zweier Ereignisse \\(A\\) und \\(B\\) wird mit \\(A\\cup B\\) symbolisiert; das Ereignis \\(A\\) oder das Ereignis \\(B\\) tritt ein (Abbildung 6.5). Abbildung 6.5: Venn-Diagramm der Vereinigungsmenge zweier Ereignisse \\(A\\cup B\\). Die grüne Fläche beschreibt die Vereinigungsmenge, d.h. das Ereignis, dass \\(A\\) oder \\(B\\) eintritt.Nach: Mittag (2016). Die Differenzmenge zweier Ereignisse \\(A\\) und \\(B\\) schließlich ist die Schnittmenge von \\(A\\) und \\(\\bar B\\) (Komplementärereignis von \\(B\\)) und wird mit \\(A\\setminus B=A\\cap\\bar B\\) symbolisiert (Abbildung 6.6). Abbildung 6.6: Venn-Diagramm der Differenzmenge zweier Ereignisse \\(A\\setminus B=A\\cap\\bar B\\). Die grüne Fläche beschreibt die Schnittmenge von \\(A\\) und \\(\\bar B\\).Nach: Mittag (2016). 6.3 Zufallsvariablen Interpretiert man die Werte eines Merkmals als Ergebnis eines Zufallsvorgangs, dann nennt man das Merkmal Zufallsvariable und die möglichen Ergebnisse des Zufallsprozesses Ausprägungen oder Realisierungen der betreffenden Zufallsvariable (vgl. deskriptive Statistik). Wir unterscheiden diskrete und stetige Zufallsvariablen. Bei einer diskreten Zufallsvariable ist die Anzahl der Ausprägungen abzählbar; z.B. Anzahl der Richtigen beim Lotto. Bei einer stetigen Zufallsvariable (auch kontinuierlich genannt) ist die Anzahl der Ausprägungen theoretisch nicht abzählbar; z.B. Entfernung zwischen Wohnort und Arbeitsplatz einer zufällig aus einer größeren Menschengruppe ausgewählten Person. Praktisch wird es aber immer eine Messgenauigkeit geben - wir können Entfernung nicht mit beliebig vielen Nachkommastellen messen. D.h. auch hier ist die Stetigkeit von Zufallsvariablen ein mathematisches Modell für das, was wir in der Welt sehen. In der Wissenschaft sprechen wir oft von Zufallsexperimenten, wobei wir kontrollierte und nicht-kontrollierte Zufallsexperimente unterscheiden. Ein kontrolliertes Zufallsexperiment ist unter annähernd gleichbleibenden Bedingungen wiederholbar; z.B. Ziehung der Lottozahlen. Ein nicht-kontrolliertes Zufallsexperiment ist z.B. die Durchschnittstemperatur im Monat Juli an einem bestimmten Ort. Diese hängt von Faktoren ab, die wir nicht experimentell kontrollieren können. 6.4 Wahrscheinlichkeit So wie wir Zufallsvorgänge und Ereignisse aus der Mengenleere kommend konzeptionalisiert haben4 ist die Wahrscheinlichkeit für das Eintreten eines Ereignisses \\(A\\) - \\(\\Pr(A)\\) - als Grenzwert der relativen Häufigkeit für das Eintreten von \\(A\\) definiert:5 \\[\\begin{equation} f_n(A)\\xrightarrow{n\\to\\infty}\\Pr(A) \\tag{6.1} \\end{equation}\\] Veranschaulichen wir das mit dem Beispiel Münzwurf (Abbildung 6.7): Mit zunehmender Anzahl Münzwürfe \\(n\\) nähert sich die relative Häufigkeit \\(f_n\\) des Ereignisses Zahl (ebenso Kopf) dem erwarteten Wert 0.5 an. Wenn \\(n\\) (hypothetisch) gegen unendlich geht (\\(n\\to\\infty\\)) dann nennen wir diesen Grenzwert Wahrscheinlichkeit. # simuliere 1000 Münzwürfe # dies geschieht hier mit einem Bernoulliprozess, # wobei die Wahrscheinlichkeit für Erfolg (in unserem Fall &quot;Zahl&quot;) gleich 0.5 ist # der Bernoulliprozess ist der Spezialfall eines Binomialprozesses mit einem Versuch, # daher der R-Befehl # im Output steht 1 für &quot;Zahl&quot; und 0 für &quot;Kopf&quot; zahl &lt;- rbinom(1000,1,0.5) # berechne relative Häufigkeit für &quot;Zahl&quot; nach jedem Münzwurf # das ist die Summe der bisherigen Ereignisse &quot;Zahl&quot;, # geteilt durch die jeweilige Anzahl der Münzwürfe f = cumsum(zahl) / seq(1,1000,1) # plot plot(seq(1,1000,1), f, type = &#39;n&#39;, xlim = c(0,1000), ylim = c(0,1), xlab = &#39;Anzahl Münzwürfe n&#39;, ylab = &#39;Relative Häufigkeit f_n(Zahl)&#39;) abline(h = 0.5, lwd = 3, col = &quot;red&quot;) points(seq(1,1000,1), f, pch = 20) text(800, 0.6, &#39;Grenzwert: Pr(Zahl)=0.5&#39;) Abbildung 6.7: Simulation von 1000 Münzwürfen und die Entwicklung der relativen Häufigkeit von Zahl. Wir sehen, dass sich die relative Häufigkeit mit zunehmender Anzahl Münzwürfe dem erwarteten Wert 0.5 annähert. Den Grenzwert der relativen Häufigkeit wenn die Anzahl der Münzwürfe gegen unendlich geht nennen wir Wahrscheinlichkeit. 6.5 Axiome der Wahrscheinlichkeitstheorie nach Kolmogorow Die Wahrscheinlichkeitstheorie beruht wie jede Theorie auf gewissen sogenannten Axiomen, d.h. Annahmen, die so grundlegend sind, dass sie in der Anwendung nicht in Frage gestellt werden. Ohne Axiome gibt es keine Wahrscheinlichkeitstheorie. Die Herleitung der Wahrscheinlichkeitstheorie aus der Mengenlehre formuliert die Axiome nach Kolmogorow: \\[K1: \\Pr(A)\\geq0\\] Das ist die Nicht-Negativitätsbedingung, die besagt, dass die Wahrscheinlichkeit eines Ereignisses \\(A\\) immer größer oder gleich 0 ist. \\[K2: \\Pr(\\Omega)=1\\] Das ist die Normierung, die besagt, dass die Wahrscheinlichkeit der Ergebnismenge \\(\\Omega\\) immer 1 ist, das sichere Ereignis. \\[K3: \\Pr(A\\cup B)=\\Pr(A)+\\Pr(B)\\quad\\text{falls}\\quad A\\cap B=\\emptyset\\] Das ist die Additivität bei Ereignissen, die keine Schnittmenge haben, die besagt, dass, falls \\(A\\) und \\(B\\) keine Schnittmenge haben, die Wahrscheinlichkeit, dass \\(A\\) oder \\(B\\) eintritt die Summe der Wahrscheinlichkeiten von \\(A\\) und \\(B\\) ist. Axiom K3 machen wir uns am besten wieder anhand eines Venn-Diagramms klar (Abbildung 6.8): Nehmen wir zwei Ereignisse eines Würfelwurfes \\(A=\\{1,2\\}\\) (1 oder 2) und \\(B=\\{3,4\\}\\) (3 oder 4), die keine Schnittmenge haben, d.h. \\(A\\cap B=\\emptyset\\). Dann ist die Wahrscheinlichkeit des Ereignisses \\(A\\cup B=\\{1,2,3,4\\}\\) (1 oder 2 oder 3 oder 4) die Summe der Einzelwahrscheinlichkeiten von \\(A\\) und \\(B\\); \\(\\Pr(A\\cup B)=\\Pr(A)+\\Pr(B)=\\frac{1}{3}+\\frac{1}{3}=\\frac{2}{3}\\). Abbildung 6.8: Venn-Diagramm zweier Ereignisse eines Würfelwurfes \\(A=\\{1,2\\}\\) und \\(B=\\{3,4\\}\\), die keine Schnittmenge haben \\(\\left(A\\cap B=\\emptyset\\right)\\). Die Wahrscheinlichkeit des Ereignisses \\(A\\) oder \\(B\\) \\(\\left(A\\cup B=\\{1,2,3,4\\}\\right)\\) ist die Summe der Einzelwahrscheinlichkeiten von \\(A\\) und \\(B\\). 6.6 Rechenregeln Aus den o.g. Axiomen lassen sich die Rechenregeln der Wahrscheinlichkeitsrechnung ableiten. Es mag phantastisch klingen, aber man braucht tatsächlich keine weiteren Annahmen als die Axiome, um zu relativ komplizierten Regeln zu kommen. Als erstes steht die Produktregel: \\[\\begin{equation} \\Pr(A\\cap B)=\\Pr(A|B)\\cdot\\Pr(B)=\\Pr(B|A)\\cdot\\Pr(A) \\tag{6.2} \\end{equation}\\] In Worten: Die Wahrscheinlichkeit, dass \\(A\\) und \\(B\\) beide eintreten errechnet sich aus der bedingten Wahrscheinlichkeit von \\(A\\) gegeben \\(B\\) mal der Wahrscheinlichkeit von \\(B\\), oder umgekehrt. Neu hier ist der Begriff der bedingten Wahrscheinlichkeit, für die das Symbol | verwendet wird: \\(\\Pr(A|B)\\) ist die Wahrscheinlichkeit von \\(A\\) falls \\(B\\) ebenfalls eingetreten ist (\\(A\\) gegeben \\(B\\)). Das ist nicht zu verwechseln mit der Wahrscheinlichkeit \\(\\Pr(A\\cap B)\\), dass \\(A\\) und \\(B\\) beide eintreten (dafür ist die Produktregel da); bei der bedingten Wahrscheinlichkeit geht es nur um die Wahrscheinlichkeit eines Ereignisses (hier \\(A\\)), jedoch bedingt durch ein anderes Ereignis (hier \\(B\\)). Die Reihenfolge von \\(A\\) und \\(B\\) macht hier keinen Unterschied, d.h. \\(\\Pr(A|B)\\cdot\\Pr(B)=\\Pr(B|A)\\cdot\\Pr(A)\\). Machen wir uns die Produktregel mit einem sogenannten Baumdiagramm zweier Würfelwürfe klar (Abbildung 6.9): Die Wahrscheinlichkeit im ersten Wurf eine 1 zu würfeln ist \\(\\Pr(A=1)=\\frac{1}{6}\\). Die Wahrscheinlichkeit im zweiten Wurf ebenfalls eine 1 zu würfeln, d.h. wenn man bereits eine gewürfelt hat, ist die bedingte Wahrscheinlichkeit \\(\\Pr(B=1|A=1)=\\frac{1}{6}\\). Die Wahrscheinlichkeit zwei Einsen zu würfeln ist laut Produktregel \\(\\Pr(A=1\\cap B=1)=\\Pr(B=1|A=1)\\cdot\\Pr(A=1)=\\frac{1}{6}\\cdot\\frac{1}{6}=\\frac{1}{36}\\), was ebenfalls intuitiv Sinn macht. Auch wenn das Baumdiagramm eine Reihenfolge suggeriert, gilt die Rechnung nicht nur für sequenzielle sondern auch für gleichzeitige Würfelwürfe sein. Ebenfalls können \\(A\\) und \\(B\\) vertauscht werden, da laut Produktregel \\(\\Pr(A|B)\\cdot\\Pr(B)=\\Pr(B|A)\\cdot\\Pr(A)\\). Abbildung 6.9: Baumdiagramm zweier Würfelwürfe. Der erste Würfelwurf ist das Ereignis \\(A\\) mit sechs Ausprägungen \\(1,2,\\ldots,6\\). Für jede dieser Ausprägungen gibt es wieder sechs Ausprägungen \\(1,2,\\ldots,6\\) des Ereignisses \\(B\\), des zweiten Würfelwurfs. Jeder der Pfade, die sich so ergeben führt zu einer Ausprägung des Ereignisses \\(A\\cap B\\), \\(A\\) und \\(B\\). Die Gesamtzahl dieser Ausprägungen ist \\(6\\cdot 6=36\\). Die Wahrscheinlichkeit von \\(A\\) ist \\(\\Pr(A)\\), die Wahrscheinlichkeit von \\(B\\) ist aber \\(\\Pr(B|A)\\), die Wahrscheinlichkeit von \\(B\\) unter der Bedingung, dass \\(A\\) ebenfalls eingetreten ist. Das können sequenzielle als auch gleichzeitige Würfelwürfe sein, und auch die Reihenfolge ist egal, da laut Produktregel \\(\\Pr(A|B)\\cdot\\Pr(B)=\\Pr(B|A)\\cdot\\Pr(A)\\). Jetzt ist es etwas akademisch bei diesem Beispiel von bedingten Wahrscheinlichkeiten zu sprechen, da die bedingte Wahrscheinlichkeit \\(\\Pr(B|A)\\) im Fall der Würfelwürfe ja gleich \\(\\Pr(B)\\) ist, z.B. \\(\\Pr(B=1|A=1)=\\Pr(B=1)=\\frac{1}{6}\\). Das Ergebnis des Wurfes \\(A\\) hat keinen Auswirkungen auf die Wahrscheinlichkeit von \\(B\\); wir sagen die beiden Ereignisse sind unabhängig. Zwei Ereignisse \\(A\\) und \\(B\\) werden formal als unabhängig bezeichnet, wenn das Eintreten eines Ereignisses keinen Einfluss auf das andere Ereignis hat, z.B. \\(\\Pr(A\\cap B)=\\Pr(B|A)\\cdot\\Pr(A)=\\Pr(B)\\cdot\\Pr(A)\\). Grundsätzlich kann aber ein Ereignis \\(A\\) die Wahrscheinlichkeit eines zweiten Ereignisses \\(B\\) beeinflussen. Als zweites steht die Summenregel: \\[\\begin{equation} \\Pr(A)+\\Pr(\\bar A)=1 \\tag{6.3} \\end{equation}\\] In Worten: Die Summe der Wahrscheinlichkeiten, dass \\(A\\) eintritt oder nicht ist 1. Wichtiger ist die generalisierte Summenregel: \\[\\begin{equation} \\Pr(A\\cup B)=\\Pr(A)+\\Pr(B)-\\Pr(A\\cap B) \\tag{6.4} \\end{equation}\\] In Worten: Die Wahrscheinlichkeit, dass \\(A\\) oder \\(B\\) eintritt oder beide errechnet sich aus der Summe der Wahrscheinlichkeiten von \\(A\\) und \\(B\\) minus der Wahrscheinlichkeit, dass \\(A\\) und \\(B\\) beide eintreten. Das können wir uns wiederum mit einem Venn-Diagramm klar machen (Abbildung 6.10): Im Venn-Diagramm sind die Flächengrößen proportional zu den Wahrscheinlichkeiten der entsprechenden Ereignisse. Wenn uns die Wahrscheinlichkeit interessiert, dass \\(A\\) oder \\(B\\) eintritt (oder beide)6 dann ist das die Summe der beiden Flächen im Venn-Diagramm, d.h. die Summe der Einzelwahrscheinlichkeiten \\(\\Pr(A)\\) und \\(\\Pr(B)\\). Wenn jetzt aber die beiden Flächen überlappen dann würde man mit der vorgenannten Summe die Schnittmenge doppelt zählen. Deshalb ziehen wir die Schnittmenge, die Wahrscheinlichkeit \\(\\Pr(A\\cap B)\\), in der generalisierten Summenregel einmal ab, um die reine Vereinigungsmenge, die Wahrscheinlichkeit \\(\\Pr(A\\cup B)\\), zu erhalten. Abbildung 6.10: Venn-Diagramm der generalisierten Summenregel, wobei die Flächengrößen proportional zu den Wahrscheinlichkeiten der entsprechenden Ereignisse sind. Die Wahrscheinlichkeit, dass \\(A\\) oder \\(B\\) eintritt (oder beide) ist die Summe der beiden Flächen im Venn-Diagramm, d.h. die Summe der Einzelwahrscheinlichkeiten \\(\\Pr(A)\\) und \\(\\Pr(B)\\), minus der überlappenden Schnittmenge, die Wahrscheinlichkeit \\(\\Pr(A\\cap B)\\), da wir diese sonst doppelt zählen würden. Als dritte und letzte Regel steht die Additivität: Für \\(n\\) sich gegenseitig ausschließende Ereignisse \\(\\{A_1,\\ldots,A_n\\}\\), d.h. nur ein Ereignis kann auftreten, gilt: \\[\\begin{equation} \\Pr(A_1\\cup \\cdots \\cup A_m)=\\sum_{i=1}^{m}\\Pr\\left(A_i\\right)\\quad \\text{für}\\quad 1\\leq m\\leq n \\tag{6.5} \\end{equation}\\] Für \\(n\\) vollständige Ereignisse \\(\\{A_1,\\ldots,A_n\\}\\), d.h. ein Ereignis muss auftreten, gilt: \\[\\begin{equation} \\sum_{i=1}^{n}\\Pr\\left(A_i\\right)=1 \\tag{6.6} \\end{equation}\\] Ein Beispiel liefert wieder das Baumdiagramm zweier Würfelwürfe (Abbildung 6.11): Die Wahrscheinlichkeit, dass wir im ersten Wurf eine 1 oder eine 2 würfeln ist die Summe der Einzelwahrscheinlichkeiten: \\(\\Pr((A=1)\\cup (A=2))=\\frac{1}{6}+\\frac{1}{6}=\\frac{1}{3}\\). Die Wahrscheinlichkeit, bei zwei Würfen eine der 36 Ausprägungen zu würfeln ist logischerweise 1: \\(\\sum_{j=1}^{6}\\sum_{i=1}^{6}\\Pr(B_j\\cap A_i)=\\frac{1}{36}+\\cdots+\\frac{1}{36}=1\\). Abbildung 6.11: Baumdiagramm zweier Würfelwürfe. Die Wahrscheinlichkeit, dass wir im ersten Wurf eine 1 oder eine 2 würfeln ist die Summe der Einzelwahrscheinlichkeiten. Die Wahrscheinlichkeit, bei zwei Würfen eine der 36 Ausprägungen zu würfeln ist logischerweise 1. Literatur "],["07-verteilungen.html", "Kapitel 7 Verteilungen 7.1 Von der empirischen zur theoretischen Verteilung 7.2 Parameter theoretischer Verteilungen 7.3 Kenngrößen theoretischer Verteilungen 7.4 Wichtige Verteilungen und ihre Anwendungen", " Kapitel 7 Verteilungen In Kapitel 6 haben wir gesehen, dass alle \\(36\\) Elementarereignisse zweier Würfelwürfe \\(\\omega=(1;1),(1;2),\\ldots,(6;6)\\) (vgl. Abbildung 6.2) die gleiche Wahrscheinlichkeit haben, nämlich \\(\\Pr(\\omega_i)=\\frac{1}{6}\\cdot\\frac{1}{6}=\\frac{1}{36}\\). In Abbildung 7.1 links, sehen wir eine Verteilung dieser Wahrscheinlichkeiten; in diesem Fall ist es eine sogenannte diskrete Gleichverteilung. Im Quiz haben wir gesehen, dass wenn uns das Ereignis Gesamtaugenzahl interessiert, hier mit \\(E\\) symbolisiert, es Ergebnisse gibt, die wahrscheinlicher sind als andere (das deckt sich mit unserer Erfahrung mit Würfelspielen). Die Verteilung ist symmetrisch um ein Maximum bei \\(7\\) (Abbildung 7.1 rechts), eine sogenannte diskrete Dreiecksverteilung. Formal schreiben wir: \\[\\begin{equation} \\Pr(E)=\\sum_{\\omega_i\\in E}\\Pr\\left(\\omega_i\\right) \\tag{7.1} \\end{equation}\\] In Worten: Die Wahrscheinlichkeit des Ereignisses \\(E\\) ist gleich der Summe der Wahrscheinlichkeiten aller Elementarereignisse \\(\\omega_i\\), die zu \\(E\\) gehören. Abbildung 7.1: Links: Die \\(36\\) Elementarereignisse zweier Würfelwürfe sind gleichverteilt mit \\(\\Pr(\\omega_i)=\\frac{1}{36}\\); die Wahrscheinlichkeitsverteilung ist eine Gleichverteilung. Rechts: Die Wahrscheinlichkeitsverteilung des Ereignisses \\(E\\), der Gesamtaugenzahl zweier Würfelwürfe, ist eine Dreiecksverteilung; das Ergebnis \\(7\\) ist am wahrscheinlichsten (vgl. Quiz zu Kapitel 6). In diesem Kapitel soll es um solche Wahrscheinlichkeitsverteilungen gehen. Erinnern wir uns, dass eine Zufallsvariable ein Merkmal ist, dessen Werte als Ergebnisse eines Zufallsvorgangs interpretiert werden (vgl. Kapitel 6). Die Wahrscheinlichkeitsverteilung der Zufallsvariablen ist nun ein Modell, das das Verhalten der Zufallsvariablen vollständig beschreibt. Alternative Bezeichnungen sind Verteilung und theoretische Verteilung. So wie wir diskrete und stetige Zufallsvariablen unterscheiden (vgl. Kapitel 6), unterscheiden wir auch diskrete und stetige Verteilungen. Für diskrete Verteilungen werden wir den Begriff Wahrscheinlichkeitsfunktion kennenlernen und für stetige Verteilungen den Begriff Dichtefunktion. Für kumulierte Wahrscheinlichkeiten lernen wir den Begriff Verteilungsfunktion kennen, der für diskrete und stetige Verteilungen gleichermaßen gilt. Am Ende des Kapitels stellen wir wichtige Verteilungen und ihre Anwendungen kurz vor. 7.1 Von der empirischen zur theoretischen Verteilung Wir haben bereits Darstellungen von empirischen Verteilungen in der Form von Histogrammen (und Boxplots) kennengelernt (vgl Kapitel 3). Nehmen wir das Beispiel Entfernung aus Ihren Reisedaten (Abbildung 7.2). Entfernung ist eine stetige Zufallsvariable, die hier aber klassiert dargestellt ist. Ein Beispiel für eine diskrete Zufallsvariable ist das Merkmal Anzahl Vulkanausbrüche aus Zimmermann-Janschitz (2014). Abbildung 7.2: Links: Empirische Verteilung des Merkmals Entfernung. Der 5. Balken z.B. hat eine Höhe von etwa \\(0.05\\); d.h. rund 5% der Merkmalswerte sind in Klasse 5 (20 - 25km). Rechts: Empirische Verteilungsfunktion des Merkmals Entfernung. Der 5. Balken hat eine Höhe von etwa \\(0.88\\); d.h. rund 88% der Merkmalswerte sind kleiner oder gleich 25km. Die theoretische Verteilung ist eine mathematische Funktion, die an die Daten der Stichprobe angepasst wurde (Abbildung 7.3). Genau genommen ist die theoretische Verteilung unser Modell für die Grundgesamtheit, aus der die Stichprobe entnommen wurde. Aber die Parameter dieses Modells werden anhand der Stichprobendaten geschätzt (wie genau lernen wir in Kapitel 8). Für die Entfernungsdaten passt die Lognormalverteilung ganz gut - mehr dazu weiter unten. Abbildung 7.3: Links: Theoretische Verteilung des Merkmals Entfernung über der empirischen Verteilung. Hier wurde eine Lognormalverteilung angepasst. Rechts: Theoretische Verteilungsfunktion des Merkmals Entfernung über der empirischen Verteilungsfunktion. Im Fall von diskreten Zufallsvariablen sprechen wir von der Wahrscheinlichkeitsfunktion: \\[\\begin{equation} f_X(x)=\\Pr(X=x) \\tag{7.2} \\end{equation}\\] In Worten: Die Wahrscheinlichkeitsfunktion einer diskreten Zufallsvariablen \\(X\\) in Abhängigkeit der Ausprägung \\(x\\) ist die Eintrittswahrscheinlichkeit von \\(x\\). Vgl. Abbildung 6.2 und Abbildung 7.1, links. Im Fall von stetigen Zufallsvariablen sprechen wir von der Dichtefunktion: \\[\\begin{equation} \\int_{a}^{b}f_X(x)\\;dx=\\Pr(a\\leq x\\leq b) \\tag{7.3} \\end{equation}\\] In Worten: Das Integral von \\(x=a\\) bis \\(x=b\\) der Dichtefunktion einer stetigen Zufallsvariablen \\(X\\) in Abhängigkeit der Ausprägung \\(x\\) ist die Wahrscheinlichkeit, dass \\(X\\) zwischen \\(a\\) und \\(b\\) liegt. Vgl. Abbildung 7.3, links. Die kumulierten Wahrscheinlichkeiten beschreibt in beiden Fällen die Verteilungsfunktion: \\[\\begin{equation} F_X(x)=\\Pr(X\\leq x) \\tag{7.4} \\end{equation}\\] In Worten: Die Verteilungsfunktion einer beliebigen Zufallsvariablen \\(X\\) in Abhängigkeit der Ausprägung \\(x\\) ist die Wahrscheinlichkeit, dass \\(X\\) kleiner oder gleich \\(x\\) ist. Vgl. Abbildung 7.3, rechts. Die Analogien zwischen empirischen und theoretischen Verteilungen können Sie auch nochmal in Mittag (2016), Tabelle 10.3 auf S. 159 nachlesen. 7.2 Parameter theoretischer Verteilungen Die Form einer Verteilung wird grundsätzlich von der mathematischen Formel der entsprechenden Wahrscheinlichkeits- bzw. Dichtefunktion bestimmt. Die für uns wichtigsten Verteilungen und ihre Formen werden wir weiter unten kennenlernen. Auf die mathematischen Formeln werden wir in diesem Kurs nicht eingehen; sie können beispielsweise in Mittag (2016) nachgeschlagen werden. Wichtig zu wissen ist aber, dass die Wahrscheinlichkeits- bzw. Dichtefunktion sogenannte Parameter hat, die die Lage und die Breite der Verteilung, und bei manchen Verteilungen auch die Form selber, bestimmen. Schauen wir uns das für den einfachsten Fall der sogenannten Normalverteilung an (Abbildung 7.4)7. # generiere x-Werte von -5 bis 5 in 0.1-Schritten x &lt;- seq(-5, 5, 0.1) # plotte Dichtefunktion der Normalverteilung mit µ = 0 und sigma = 1 für x plot(x, dnorm(x, mean = 0, sd = 1), type = &#39;l&#39;, xlab = &#39;X = x&#39;, ylab = &#39;Dichtefunktion&#39;) Abbildung 7.4: Form der Dichtefunktion der Normalverteilung einer beliebigen Zufallsvariablen \\(X\\). Hier ist die Standardnormalverteilung \\(N(0,1)\\) dargestellt, mit Mittelwert \\(\\mu=0\\) und Standardabweichung \\(\\sigma=1\\). Wenn wir ausdrücken wollen, dass eine Zufallsvariable \\(X\\) normalverteilt ist schreiben wir: \\[\\begin{equation} X\\sim N(\\mu,\\sigma) \\tag{7.5} \\end{equation}\\] Das Tildesymbol \\(\\sim\\) bedeutet ist verteilt gemäß, das \\(N\\) steht für die Normalverteilung und in Klammern stehen die Parameter der Verteilung, in diesem Fall \\(\\mu\\) und \\(\\sigma\\). Andere Verteilungen haben eine höhere oder auch geringere Anzahl an Parametern und diese haben oft auch andere Bezeichnungen. Im Fall der Normalverteilung ist \\(\\mu\\) der Mittelwert (Erwartungswert) und \\(\\sigma\\) ist die Standardabweichung der Zufallsvariablen \\(X\\)8. Es handelt sich hier um theoretische Parameter der Grundgesamtheit, nicht zu verwechseln mit den empirischen Parametern einer Stichprobe. Wenn die Stichprobe aber tatsächlich aus einer normalverteilten Grundgesamtheit stammt, dann sind empirischer Mittelwert und Standardabweichung die Schätzer der entsprechenden theoretischen Parameter! Das Schätzen von Verteilungsparametern wird uns in Kapitel 8 beschäftigen. Den Effekt verschiedener Werte für \\(\\mu\\) und \\(\\sigma\\) auf die Lage und Breite der Verteilung sehen wir wenn wir \\(\\mu\\) und \\(\\sigma\\) variieren (Abbildung 7.5). # generiere x-Werte von -5 bis 5 in 0.1-Schritten x &lt;- seq(-5, 5, 0.1) # plotte Dichtefunktion der Normalverteilung mit µ = -1, 0, 1 und sigma = 1 für x # (linke Abbildung) plot(x, dnorm(x, mean = 0, sd = 1), type=&#39;l&#39;, ylim = c(0, 0.45), xlab = &#39;X = x&#39;, ylab = &#39;Dichtefunktion&#39;, panel.first = grid()) curve(dnorm(x, mean = -1, sd = 1), add = TRUE, col = &#39;red&#39;) curve(dnorm(x, mean = 1, sd = 1), add = TRUE, col = &#39;blue&#39;) legend(&#39;topright&#39;, legend = c(&#39;N(-1, 1)&#39;, &#39;N(0, 1)&#39;, &#39;N(1, 1)&#39;), col = c(&#39;red&#39;, &#39;black&#39;, &#39;blue&#39;), lty = 1) # plotte Dichtefunktion der Normalverteilung mit µ = 0 und sigma = 1, 1.5, 2 für x # (rechte Abbildung) plot(x, dnorm(x, mean = 0, sd = 1), type = &#39;l&#39;, ylim = c(0, 0.45), xlab = &#39;X = x&#39;, ylab = &#39;Dichtefunktion&#39;, panel.first = grid()) curve(dnorm(x, mean = 0, sd = 2), add = TRUE, col = &#39;red&#39;) curve(dnorm(x, mean = 0, sd = 1.5), add = TRUE, col = &#39;blue&#39;) legend(&#39;topright&#39;, legend = c(&#39;N(0, 2)&#39;, &#39;N(0, 1.5)&#39;, &#39;N(0, 1)&#39;), col = c(&#39;red&#39;, &#39;blue&#39;, &#39;black&#39;), lty = 1) Abbildung 7.5: Links: Dichtefunktion der Normalverteilung einer beliebigen Zufallsvariablen \\(X\\) für verschiedene Werte \\(\\mu\\). Wir sehen, dass der Parameter \\(\\mu\\) die Verteilung nach rechts bzw. links verschiebt. Rechts: Dichtefunktion der Normalverteilung einer beliebigen Zufallsvariablen \\(X\\) für verschiedene Werte \\(\\sigma\\). Wir sehen, dass der Parameter \\(\\sigma\\) die Verteilung breiter bzw. schmaler macht. Der Parameter \\(\\mu\\) verschiebt also die Verteilung nach rechts bzw. links; Breite und Form bleiben gleich. Der Parameter \\(\\sigma\\) macht die Verteilung breiter bzw. schmaler; Lage und Form bleiben gleich. Merke: Die Fläche unter der Kurve bleibt ebenfalls gleich; sie ist immer \\(1\\) aufgrund des 2. Axioms nach Kolmogorow \\(K2: \\Pr(\\Omega)=1\\) (Normierung); vgl. Kapitel 6. Deshalb wird die Verteilung wenn sie breiter wird gleichzeitig niedriger und wenn sie schmaler wird gleichzeitig höher. Bisher haben wir uns nur die Dichtefunktion der Normalverteilung angeschaut. Es fehlt die Verteilungsfunktion als Darstellung der kumulierten Wahrscheinlichkeit. Diese ist wichtig, da wir damit einfach Quantile berechnen können, die in der induktiven Statistik häufig zum Einsatz kommen. Die Verteilungsfunktion der Normalverteilung ist in Abbildung 7.6 für verschiedene Werte \\(\\mu\\) und \\(\\sigma\\) dargestellt (vgl. Abbildung 7.5). # generiere x-Werte von -5 bis 5 in 0.1-Schritten x &lt;- seq(-5, 5, 0.1) # plotte Verteilungsfunktion der Normalverteilung mit µ = -1, 0, 1 und sigma = 1 für x # (linke Abbildung) plot(x, pnorm(x, mean = 0, sd = 1), type = &#39;l&#39;, xlab = &#39;X = x&#39;, ylab = &#39;Verteilungsfunktion&#39;, panel.first = grid()) curve(pnorm(x, mean = -1, sd = 1), add = TRUE, col = &#39;red&#39;) curve(pnorm(x, mean = 1, sd = 1), add = TRUE, col = &#39;blue&#39;) legend(&#39;topleft&#39;, legend = c(&#39;N(-1, 1)&#39;, &#39;N(0, 1)&#39;, &#39;N(1, 1)&#39;), col = c(&#39;red&#39;, &#39;black&#39;, &#39;blue&#39;), lty = 1) # plotte Verteilungsfunktion der Normalverteilung mit µ = 0 und sigma = 1, 1.5, 2 für x # (rechte Abbildung) plot(x, pnorm(x, mean = 0, sd = 1), type = &#39;l&#39;, xlab = &#39;X = x&#39;, ylab = &#39;Verteilungsfunktion&#39;, panel.first = grid()) curve(pnorm(x, mean = 0, sd = 2), add = TRUE, col = &#39;red&#39;) curve(pnorm(x, mean = 0, sd = 1.5), add = TRUE, col = &#39;blue&#39;) legend(&#39;topleft&#39;, legend = c(&#39;N(0, 2)&#39;, &#39;N(0, 1.5)&#39;, &#39;N(0, 1)&#39;), col = c(&#39;red&#39;, &#39;blue&#39;, &#39;black&#39;), lty = 1) Abbildung 7.6: Links: Verteilungsfunktion der Normalverteilung einer beliebigen Zufallsvariablen \\(X\\) für verschiedene Werte \\(\\mu\\). Rechts: Verteilungsfunktion der Normalverteilung einer beliebigen Zufallsvariablen \\(X\\) für verschiedene Werte \\(\\sigma\\). Vgl. Abbildung 7.5. Wenn wir Abbildung 7.5 und Abbildung 7.6 vergleichen, dann sehen wir, dass die Verteilungsfunktion dort am steilsten ist, wo die Dichtefunktion ihr Maximum hat; dort ändert sich sozusagen in der kumulierten Wahrscheinichkeit am meisten. Wenn die Dichtefunktion abflacht, dann nimmt auch die Steigung der Verteilungsfunktion wieder ab. Es ist wichtig, dass Sie den Zusammenhang zwischen Dichte- und Verteilungsfunktion verstehen, der analog zum Zusammenhang zwischen Häufigkeit und Summenhäufigkeit ist (vgl. Kapitel 3). 7.3 Kenngrößen theoretischer Verteilungen Auch für die Kenngrößen theoretischer Verteilungen ist es wichtig, dass wir die Parallelen zu den empirischen Verteilungen sehen. Der Erwartungswert theoretischer Verteilungen ist: \\[\\begin{equation} E(X)=\\mu=\\sum_{j=1}^{m}x_j\\cdot \\Pr\\left(X=x_j\\right)\\quad\\text{für diskrete Verteilungen} \\tag{7.6} \\end{equation}\\] \\[\\begin{equation} E(X)=\\mu=\\int_{-\\infty}^{\\infty}x\\cdot f_X(x)\\;dx\\quad\\text{für stetige Verteilungen} \\tag{7.7} \\end{equation}\\] Merke: Für den theoretischen Erwartungswert wird häufig das Symbol \\(\\mu\\) verwendet. Damit ist aber nicht unbedingt der Parameter gleichen Namens der Normalverteilung gemeint, obwohl natürlich beide Größen identisch sind, wenn die Zufallsvariable normalverteilt ist. Aber auch der Erwartungswert anderer theoretischer Verteilungen wird häufig mit \\(\\mu\\) bezeichnet. Das ist leider so verwirrend. Die Formulierung des Erwartungswertes ist analog zur gewichteten Formulierung des arithmetischen Mittels empirischer Verteilungen (vgl. Kapitel 3): \\[\\bar x=\\sum_{j=1}^{m}a_j\\cdot f_j\\] Vgl. die ungewichtete Formulierung: \\[\\bar x=\\frac{\\sum_{i=1}^{n}x_i}{n}\\] Die Varianz theoretischer Verteilungen ist der Erwartungswert der quadrierten Streuung um den Mittelwert \\(\\sigma^2=E\\left(\\left(x_j-\\mu\\right)^2\\right)\\), d.h.: \\[\\begin{equation} V(X)=\\sigma^2=\\sum_{j=1}^{m}\\left(x_j-\\mu\\right)^2\\cdot \\Pr\\left(X=x_j\\right)\\quad\\text{für diskrete Verteilungen} \\tag{7.8} \\end{equation}\\] \\[\\begin{equation} V(X)=\\sigma^2=\\int_{-\\infty}^{\\infty}\\left(x-\\mu\\right)^2\\cdot f_X(x)\\;dx\\quad\\text{für stetige Verteilungen} \\tag{7.9} \\end{equation}\\] Merke: Wiederum wird hier häufig das Symbol des entsprechenden Parameters der Normalverteilung, \\(\\sigma^2\\), verwendet. Beide Größen sind identisch, wenn die Zufallsvariable normalverteilt ist. Aber auch die Varianze anderer theoretischer Verteilungen wird häufig mit \\(\\sigma^2\\) bezeichnet. Die Formulierung der Varianz ist analog zur Formulierung der Varianz empirischer Verteilungen (vgl. Kapitel 4): \\[s^2=\\frac{\\sum_{i=1}^{n}\\left(x_i-\\bar x\\right)^2}{n-1}\\] Die Standardabweichung theoretischer Verteilungen ist dementsprechend: \\[\\begin{equation} \\sigma=\\sqrt{\\sigma^2} \\tag{7.10} \\end{equation}\\] Das ist für diskrete und stetige Zufallsvariablen gleich, und analog zur Formulierung der Standardabweichung empirischer Verteilungen: \\[s=\\sqrt{s^2}\\] Quantile theoretischer Verteilungen sind: \\[\\begin{equation} F_X(x_p)=\\Pr\\left(X\\leq x_p\\right)=p \\tag{7.11} \\end{equation}\\] In Worten: Das \\(p\\)-Quantil einer beliebigen Zufallsvariablen \\(X\\) ist die Stelle \\(x_p\\), an der die Verteilungsfunktion \\(F_X\\) der Zufallsvariablen gleich \\(p\\) ist. Die Verteilungsfunktion gibt die Wahrscheinlichkeit an, dass die Zufallsvariable \\(X\\) einen Wert kleiner oder gleich \\(x_p\\) annimmt. Das Prinzip ist das gleiche für empirische Verteilungen, wobei die folgenden Rechenregeln gelten (vgl. Kapitel 4): \\[\\bar x_p=\\frac{x_{n\\cdot p}+x_{n\\cdot p+1}}{2}\\quad\\text{falls}\\quad n\\cdot p\\quad\\text{ganzzahlig}\\] \\[\\bar x_p=x_i\\quad\\text{falls}\\quad n\\cdot p\\quad\\text{nicht ganzzahlig, wobei}\\quad i\\quad\\text{die nächstgrößere Zahl ist}\\] 7.4 Wichtige Verteilungen und ihre Anwendungen Hier soll auf ein paar Vetreilungen eingegangen werden, die für den weiteren Verlauf Ihres Studiums wichtig sind. 7.4.1 Normalverteilung Die Normalverteilung (Abbildung 7.5) ist eine symmetrische, stetige Verteilung. Der Wertebereich geht von \\(-\\infty\\) bis \\(\\infty\\), d.h. die Verteilung ist für negative als auch positive Ausprägungen definiert. Die Normalverteilung ist die am häufigsten angenommene Verteilung. Das mag zum einen daran liegen, dass die Normalverteilung mathematisch relativ einfach zu handhaben ist. Zum anderen scheinen aber tatsächlich viele reale Zufallsvorgänge einer Normalverteilung zu folgen. Ein typisches Beispiel ist die Verteilung von zufälligen Messfehlern bei der Verwendung von Messinstrumenten im Feld und im Labor. Sehen Sie sich nochmal Abbildung 7.4 an. Wenn kein systematischer Fehler vorliegt, dann macht es Sinn, die Verteilung von Messfehlern um Null zu zentrieren - im Mittel misst das Instrument genau. Um diesen Mittelwert \\(\\mu=0\\) streuen die Daten mehr oder weniger stark - in Abhängigkeit der Standardabweichung \\(\\sigma\\) - wobei kleine Abweichungen wahrscheinlicher sind als große. Man bestimmt die Verteilungsparameter anhand von Wiederholungsmessungen an der selben Probe oder am selben Standort. Diese Konzeption des Messfehlers werden Sie beispielsweise im bodenkundlichen Praktikum verwenden. Mathematisch lässt sich zeigen, dass die Normalverteilung aus der Summe vieler Einzelprozesse entsteht, selbst wenn die Einzelprozesse ganz anderen Verteilungen folgen; der aggregierte Zufallsprozess folgt einer Normalverteilung. Das ist der zentrale Grenzwertsatz. Theoretisch muss es die Summe unendlich vieler Einzelprozesse sein; wir sehen eine Normalverteilung aber bereits bei wenigen Summanden auftauchen, was wir anhand wiederholter Würfelwürfe zeigen können (Abbildung 7.7). # Paket laden, das uns die diskrete Gleichverteilung gibt library(&#39;extraDistr&#39;) # 1000 Realisationen eines Würfelwurfes simulieren w1 &lt;- rdunif(1000, 1, 6) # 1000 Realisationen der Summe zweier Würfelwürfe simulieren w2 &lt;- w1 + rdunif(1000, 1, 6) # usw. bis zu sechs Würfelwürfen w3 &lt;- w2 + rdunif(1000, 1, 6) w4 &lt;- w3 + rdunif(1000, 1, 6) w5 &lt;- w4 + rdunif(1000, 1, 6) w6 &lt;- w5 + rdunif(1000, 1, 6) # empirische Wahrscheinlichkeitsfunktion plotten hist(w1, breaks = seq(0.5, 6.5, 1), freq = FALSE, main = &#39;1 Würfel&#39;, xlab = &#39;Gesamtaugenzahl&#39;, ylab = &#39;Wahrscheinlichkeit&#39;) hist(w2, breaks = seq(1.5, 12.5, 1), freq = FALSE, main = &#39;2 Würfel&#39;, xlab = &#39;Gesamtaugenzahl&#39;, ylab = &#39;Wahrscheinlichkeit&#39;) hist(w3, breaks = seq(2.5, 18.5, 1), freq = FALSE, main = &#39;3 Würfel&#39;, xlab = &#39;Gesamtaugenzahl&#39;, ylab = &#39;Wahrscheinlichkeit&#39;) hist(w4, breaks = seq(3.5, 24.5, 1), freq = FALSE, main = &#39;4 Würfel&#39;, xlab = &#39;Gesamtaugenzahl&#39;, ylab = &#39;Wahrscheinlichkeit&#39;) hist(w5, breaks = seq(4.5, 30.5, 1), freq = FALSE, main = &#39;5 Würfel&#39;, xlab = &#39;Gesamtaugenzahl&#39;, ylab = &#39;Wahrscheinlichkeit&#39;) hist(w6, breaks = seq(5.5, 36.5, 1), freq = FALSE, main = &#39;6 Würfel&#39;, xlab = &#39;Gesamtaugenzahl&#39;, ylab = &#39;Wahrscheinlichkeit&#39;) Abbildung 7.7: Illustration des zentralen Grenzwertsatzes. Mit zunehmender Anzahl addierter Zufallsprozesse (hier Würfelwürfe, die jeweils einer Gleichverteilung folgen), nähert sich die Wahrscheinlichkeitsverteilung der Summe einer Normalverteilung an. 7.4.2 Lognormalverteilung Die Lognormalverteilung ist eine typische Wahl wenn eine stetige Zufallsvariable nur positive Ausprägungen hat, da die Verteilung von \\(0\\) bis \\(\\infty\\) definiert ist. Sie ist rechtsschief, d.h. linkssteil (vgl. Kapitel 4). Gemäß dem zentralen Grenzwertsatz (s. oben) kann sie als aggregierte Verteilung vieler multiplikativer Zufallsvorgänge interpretiert werden. Ein Beispiel für lognormalverteilte Zufallsvariablen ist Einkommen; viele haben wenig und wenige haben viel. Wir schreiben: \\[\\begin{equation} X\\sim logN(\\mu,\\sigma) \\tag{7.12} \\end{equation}\\] Die Parameter der Lognormalverteilung sind, leider wie die der Normalverteilung, ebenfalls mit \\(\\mu\\) und \\(\\sigma\\) bezeichnet. Diese sind aber nicht mehr gleich mit Erwartungswert und Standardabweichung der Zufallsvariablen; hier sind es lediglich Symbole für die Parameter. Die Symbole haben sich etabliert, weil es eine einfache Entsprechung zwischen Lognormal- und Normalverteilung gibt: Wenn \\(X\\) lognormalverteilt ist, \\(X\\sim logN(\\mu,\\sigma)\\), dann ist das log-transformierte \\(X\\) normalverteilt, \\(\\log(X)\\sim N(\\mu,\\sigma)\\), mit den gleichen Parametern \\(\\mu\\) und \\(\\sigma\\). D.h. die Schätzer für die beiden Parameter sind das arithmetische Mittel bzw. die Standardabweichung der log-transformierten Daten einer Stichprobe (s. Kapitel 8). Schauen wir uns die Form der Lognormalverteilung für verschiedene Parameterkonstellationen an (Abbildung 7.8). Beide Parameter kontrollieren hier sowohl den Erwartungswert als auch die Varianz und damit auch die Form der Verteilung; im Beispiel sehen wir das nur für \\(\\sigma\\) während \\(\\mu\\) konstant gehalten ist. An den Formeln für Erwartungswert und Varianz könnten wir sehen, dass dort jeweils beide Parameter eingehen. Das können Sie beispielsweise in Mittag (2016) nachlesen. # generiere x-Werte von 0 bis 3 in 0.05-Schritten x &lt;- seq(0, 3, 0.05) # plotte Dichtefunktion der Lognormalverteilung mit µ = 0 und sigma = 0.25, 0.5, 1 für x # (linke Abbildung) plot(x, dlnorm(x, mean = 0, sd = 0.25), type = &#39;l&#39;, ylim = c(0, 2), xlab = &#39;X = x&#39;, ylab = &#39;Dichtefunktion&#39;, panel.first = grid()) curve(dlnorm(x, mean = 0, sd = 0.5), add = TRUE, col = &#39;red&#39;) curve(dlnorm(x, mean = 0, sd = 1), add = TRUE, col = &#39;blue&#39;) legend(&#39;topright&#39;, legend = c(&#39;logN(0, 0.25)&#39;, &#39;logN(0, 0.5)&#39;, &#39;logN(0, 1)&#39;), col = c(&#39;black&#39;, &#39;red&#39;, &#39;blue&#39;), lty = 1) # Verteilungsfunktion # (rechte Abbildung) plot(x, plnorm(x, mean = 0, sd = 0.25), type = &#39;l&#39;, xlab = &#39;X = x&#39;, ylab = &#39;Verteilungsfunktion&#39;, panel.first = grid()) curve(plnorm(x, mean = 0, sd = 0.5), add = TRUE, col = &#39;red&#39;) curve(plnorm(x, mean = 0, sd = 1), add = TRUE, col = &#39;blue&#39;) legend(&#39;bottomright&#39;, legend = c(&#39;logN(0, 0.25)&#39;, &#39;logN(0, 0.5)&#39;, &#39;logN(0, 1)&#39;), col = c(&#39;black&#39;, &#39;red&#39;, &#39;blue&#39;), lty = 1) Abbildung 7.8: Links: Dichtefunktion der Lognormalverteilung einer beliebigen Zufallsvariablen \\(X\\) für verschiedene Werte \\(\\sigma\\), wobei \\(\\mu=0\\) gesetzt ist. Wir sehen, dass der Parameter \\(\\sigma\\) sowohl die Lage als auch die Breite, sowie die Form der Verteilung kontrolliert. Gleiches gilt für \\(\\mu\\), ist aber hier nicht dargestellt. Sie können den R-Code anpassen, um Variationen von \\(\\mu\\) zu simulieren. Rechts: Verteilungsfunktionfunktion der entsprechenden Variationen der Lognormalverteilung. Da unsere Entfernungsdaten rechtsschief verteilt sind - mit relativ kleinen Werten (wenige Kilometer) und relativ großen Werten (Zehner von Kilometern), bietet sich die Lognormalverteilung als theoretische Verteilung an und lässt sich auch gut an die Daten anpassen (vgl. Abbildung 7.2). Interessanterweise waren die Entfernungsdaten aus dem vergangenen Semester eher normalverteilt, ohne die Rechtsschiefe, die wir in diesem Semester sehen. Vielleicht liegt das daran, dass aufgrund der Corona-bedingten digitalen Lehre einige von Ihnen außerhalb von Berlin wohnen, da sie nicht nach Adlershof pendeln müssen. Nutzen wir jedenfalls die Entfernungsdaten, um uns den Zusammenhang zwischen Lognormal- und Normalverteilung zu veranschaulichen. Wir hatten gesagt, wenn \\(X\\) lognormalverteilt ist (Abbildung 7.9 links) dann ist \\(\\log(X)\\) normalverteilt mit den gleichen Parametern; das sehen wir in Abbildung 7.9 rechts. # Lognormalverteilung von X # Histogramm berechnen (ohne Output) h &lt;- hist(reisedat$distanz, breaks = seq(0, 55, 5), plot = FALSE) # absolute in relative Häufigkeiten umrechnen h$counts &lt;- h$counts / sum(h$counts) # plot plot(h, freq = FALSE, col = &#39;gray&#39;, main = &#39;&#39;, xlab = &#39;Entfernung (km)&#39;, ylab = &#39;Dichtefunktion&#39;) curve(dlnorm(x, meanlog = mean(log(reisedat$distanz)), sdlog = sd(log(reisedat$distanz))), add = TRUE, col = &#39;red&#39;) # Normalverteilung von log(X) # Histogramm berechnen (ohne Output) hlog &lt;- hist(log(reisedat$distanz), breaks = seq(0, 5, 0.5), plot = FALSE) # absolute in relative Häufigkeiten umrechnen hlog$counts &lt;- hlog$counts / sum(hlog$counts) # plot plot(hlog, freq = FALSE, col = &#39;gray&#39;, main = &#39;&#39;, xlab = &#39;log(Entfernung) (log(km))&#39;, ylab = &#39;Dichtefunktion&#39;) curve(dnorm(x, mean = mean(log(reisedat$distanz)), sd = sd(log(reisedat$distanz))), add = TRUE, col = &#39;red&#39;) Abbildung 7.9: Links: Empirische Verteilung der Entfernungsdaten mit theoretischer Lognormalverteilung. Rechts: Empirische Verteilung der log-transformierten Entfernungsdaten mit theoretischer Normalverteilung. Abschließend sei erwähnt, dass wir für Zufallsvariablen, die nur positive Ausprägungen haben, wie z.B. Entfernung, sinnvollerweise eine theoretische Verteilung annehmen sollten, die auch nur für positive Werte definiert ist, wie z.B. die Lognormalverteilung. Leider sind die meisten dieser Verteilungen schiefe Verteilungen. Wenn wir eine symmetrische Verteilung wollen, dann bietet sich weiterhin die Normalverteilung an, obwohl sie theoretisch negative Werte erzeugt, die physikalisch unrealistisch sind. In der Praxis wird die Normalverteilung oft relativ weit genug von Null weg sein, so dass negative Werte eine vernachlässigbar kleine Wahrscheinlichkeit haben. Wenn uns negative Werte trotzdem Probleme bereiten, dann können wir auf die sogenannte abgeschnittene Normalverteilung zurückgreifen, die bei Null abgeschnitten und dann reskaliert ist, so dass die Fläche unter der Dichtefunktion weiterhin \\(1\\) ist. 7.4.3 Gleichverteilung Die diskrete Gleichverteilung haben wir bereits kennengelernt (hier nochmal in Abbildung 7.10 links). Beispiele sind die Elementarereignisse von Glücksspielen: Würfelwürfe, Münzwürfe, Roulette, Lottozahlen, usw. Analog dazu gibt es auch eine stetige Gleichverteilung (Abbildung 7.10 rechts): \\[\\begin{equation} X\\sim U(a,b) \\tag{7.13} \\end{equation}\\] In Worten: Die Zufallsvariable \\(X\\) ist gleichverteilt über dem Intervall \\([a,b]\\) (\\(a\\) und \\(b\\) sind die Parameter der Gleichverteilung). Abbildung 7.10: Links: Diskrete Gleichverteilung der Elementarereignisse zweier Würfelwürfe. Rechts: Stetige Gleichverteilung zwischen \\(2\\) und \\(4\\). 7.4.4 Binomialverteilung Die Binomialverteilung werden viele noch aus der Schule kennen, als Verteilung für Ziehen mit Zurücklegen, z.B. von Murmeln verschiedener Farbe aus einer Urne. So wird oft die Wahrscheinlichkeitsrechnung eingeführt. Wir schreiben: \\[\\begin{equation} X\\sim B(p,n) \\tag{7.14} \\end{equation}\\] In Worten: Die Zufallsvariable \\(X\\) ist binomialverteilt mit Erfolgswahrscheinlichkeit \\(p\\) und Anzahl der Versuche \\(n\\). Der Parameter \\(p\\) ist die Erfolgswahrscheinlichkeit, z.B. der Anteil blauer Murmeln in einer Urne mit blauen und roten Murmeln, aus der wir mit Zurücklegen ziehen. Dann ist die Wahrscheinlichkeit bei einem Versuch eine blaue Murmel zu ziehen gleich \\(p\\), und bei mehreren Versuchen \\(n\\) erwarten wir insgesamt \\(p\\cdot n\\) blaue Kugeln zu ziehen9. Die Binomialverteilung ist eine diskrete Verteilung. Abbildung 7.11 verdeutlicht ihre Form für verschiedene Parameterkonstellationen. Abbildung 7.11: Links: Wahrscheinlichkeitsfunktion der Binomialverteilung für verschiedene Werte für \\(p\\) und \\(n\\). Rechts: Verteilungsfunktion der entsprechenden Binomialverteilungsvarianten. In der Geographie verwenden wir die Binomialverteilung häufig für Zähldaten und Proportionen, z.B. in der Biogeographie (Anzahl Individuen einer bestimmten Tierart in einem Gebiet oder der Anteil überlebender Organismen als Funktion der Konzentration einer toxischen Substanz). Die Binomialverteilung dient aber auch als Wahrscheinlichkeitsmodell für Präsenz bzw. Absenz beispielsweise einer Tierart in einem Gebiet. 7.4.5 Poisson-Verteilung Die Poisson-Verteilung ist eine weitere diskrete Verteilung für Zähldaten, z.B. die Anzahl Vogelarten in einem Gebiet als Funktion topographischer Parameter. Wir schreiben: \\[\\begin{equation} X\\sim Pois(\\lambda) \\tag{7.15} \\end{equation}\\] In Worten: Die Zufallsvariable \\(X\\) ist Poisson-verteilt mit Parameter \\(\\lambda\\). Die Poisson-Verteilung hat nur einen Parameter, \\(\\lambda\\), der sowohl der Erwartungswert ist, als auch die Breite und damit auch die Form der Verteilung steuert (Abbildung 7.12). Abbildung 7.12: Links: Wahrscheinlichkeitsfunktion der Poisson-Verteilung für verschiedene Werte für \\(\\lambda\\). Rechts: Verteilungsfunktion der entsprechenden Poisson-Verteilungsvarianten. 7.4.6 Gumbel-Verteilung Die Gumbel-Verteilung ist eine stetige, rechtsschiefe (d.h. linkssteile) Verteilung, die für den negativen als auch positiven Wertebereich definiert ist. Sie kommt beispielsweise in der Klimageographie als Extremwertverteilung von Hochwässern und Trockenheiten zur Anwendung. Wir schreiben: \\[\\begin{equation} X\\sim Gum(\\mu,\\beta) \\tag{7.16} \\end{equation}\\] In Worten: Die Zufallsvariable \\(X\\) ist Gumbel-verteilt mit Lageparameter \\(\\mu\\) und Skalierungsparameter \\(\\beta\\). Wiederum sind \\(\\mu\\) und \\(\\beta\\) lediglich Symbole für die Parameter der Verteilung und \\(\\mu\\) hat keine Entsprechung im theoretischen Mittelwert oder in dem \\(\\mu\\) der Normalverteilung. Abbildung 7.13 illustriert die Form der Gumbel-Verteilung. Abbildung 7.13: Links: Dichtefunktion der Gumbel-Verteilung für verschiedene Werte für \\(\\mu\\) und \\(\\beta\\). Rechts: Verteilungsfunktion der entsprechenden Gumbel-Verteilungsvarianten. 7.4.7 Weibull-Verteilung Die Weibull-Verteilung ist eine weitere stetige Extremwertverteilung, die beispielsweise in der Klimageographie die Verteilung von Windgeschwindigkeiten modelliert. Sie ist nur für positive Werte definiert. Wir schreiben: \\[\\begin{equation} X\\sim Wei(\\lambda,k) \\tag{7.17} \\end{equation}\\] In Worten: Die Zufallsvariable \\(X\\) ist Weibull-verteilt mit Skalierungsparameter \\(\\lambda\\) und Formparameter \\(k\\). Die Weibull-Verteilung hat die Besonderheit, in Abhängigkeit der Parameter rechts- oder linksschief oder annähernd symmetrisch zu sein (Abbildung 7.14). Sie ist daher sehr flexibel einsetzbar. Abbildung 7.14: Links: Dichtefunktion der Weibull-Verteilung für verschiedene Werte für \\(\\lambda\\) und \\(k\\). Rechts: Verteilungsfunktion der entsprechenden Weibull-Verteilungsvarianten. Literatur "],["08-schaetzen.html", "Kapitel 8 Schätzen von Verteilungsparametern 8.1 Anforderungen an Schätzer 8.2 Normalverteilung: Schätzer für \\(\\mu\\) 8.3 Normalverteilung: Schätzer für \\(\\sigma\\) 8.4 Quantil-Quantil-Diagramm (QQ-Plot)", " Kapitel 8 Schätzen von Verteilungsparametern Mit diesem Kapitel beginnt die schließende oder induktive Statistik, mit der wir anhand von Stichproben etwas über größere Grundgesamtheiten aussagen möchten. Der Anspruch geht also über die beschreibende oder deskriptive Statistik hinaus, wobei die Wahrscheinlichkeitstheorie und Verteilungen (Kapitel 6 und 7) gewissermaßen die Brücke zwischen den beiden Teilbereichen der Statistik bildet, um bei dem Bild von Zimmermann-Janschitz (2014) zu bleiben. Die deskriptive Statistik wird uns die nächsten sechs Wochen beschäftigen. Nach diesem Kapitel zu Schätzen von Verteilungsparametern gibt es ein Kapitel zu statistischen Tests (Kapitel 9), das wir über drei Wochen lesen werden, und abschließend Kapitel 10 zu linearer Regression, das wir über zwei Wochen lesen werden. In Kapitel 10 werden viele Lehrveranstaltungsinhalte zusammen kommen - also bleiben Sie dran. In diesem Kapitel diskutieren wir: die Anforderungen an Schätzer den Schätzer des Mittelwertes \\(\\mu\\) einer Normalverteilung und dessen Standardfehler und Konfidenzintervall den Schätzer der Standardabweichung \\(\\sigma\\) einer Normalverteilung das Quantil-Quantil-Diagramm (QQ-Plot) als visueller Verteilungstest Zunächst aber ein paar Worte zum Grundprinzip der schließenden Statistik (vgl. Mittag (2016), Abb. 14.1, S. 212). Wie wir wissen ist eine Stichprobe (im besten Fall) eine zufällige Auswahl von Merkmalsträgern aus einer Grundgesamtheit von Merkmalsträgern. In der schließenden Statistik geht es jetzt um die Verdichtung der Informationen in der Stichprobe in Form von Stichprobenfunktionen, mit denen wir bestimmte Parameter der Grundgesamtheit schätzen. Im Fall von Verteilungsparametern handelt es sich bei den Stichprobenfunktionen v.a. um den Mittelwert und die Standardabweichung. 8.1 Anforderungen an Schätzer Die Parameter der Stichprobe (Mittelwert, Standardabweichung etc.) dienen als Schätzer der Parameter der Grundgesamtheit. Die Anforderungen an einen solchen Schätzer sind: Erwartungstreue: Der Schätzer soll den wahren (unbekannten) Wert der Grundgesamtheit möglichst genau wiedergeben Konsistenz: Die Genauigkeit der Schätzung soll mit wachsender Stichprobengröße zunehmen Effizienz: Unterschiede der Schätzwerte sollen bei Ziehung verschiedener Stichproben möglichst gering sein In wie weit diese Anforderungen erfüllt sind ergibt sich aus dem statistischen Modell der Grundgesamtheit, z.B. dass Merkmalsausprägungen normalverteilt ist. Schauen Sie sich dazu nochmal Kapitel 7 an. Die Abbildungen 7.2 und 7.3 zeigen wie wir von der empirischen zur theoretischen Verteilung kommen. In Abbildungen 7.5 und 7.6 sehen wir die Normalverteilung als ein solches theoretisches Modell der Grundgesamtheit für verschiedene Konstellationen der Parameter \\(\\mu\\) und \\(\\sigma\\). Da die Stichprobendaten als Realisationen von Zufallsvariablen interpretiert werden (vgl. Kapitel 7), ist auch der aus ihnen errechnete Schätzwert eine Realisation einer Zufallsvariablen, die Schätzer genannt wird. Dazu die folgende Notation: für einen nicht näher spezifizierten Parameter verwenden wir das Symbol \\(\\theta\\) (theta) den Schätzer dieses Parameters bezeichnen wir mit \\(\\hat\\theta\\) (theta-Dach) ein Parametervektor wird fett gedruckt: \\(\\boldsymbol\\theta=\\begin{pmatrix}\\theta_1 &amp; \\theta_2 &amp; \\cdots &amp; \\theta_p\\end{pmatrix}\\) z.B. Im Fall der Normalverteilung: \\(X\\sim N(\\boldsymbol\\theta=\\begin{pmatrix}\\mu &amp; \\sigma\\end{pmatrix}\\)) Erwartungstreue bedeutet mathematisch: \\[\\begin{equation} E(\\hat\\theta)=\\theta \\tag{8.1} \\end{equation}\\] In Worten: Der Erwartungswert des Schätzers ist der wahre Wert des Parameters. D.h. der Schätzer trifft im Mittel den zu schätzenden Wert genau. Wenn ein Schätzer nicht erwartungstreu ist, heißt die Differenz Verzerrung oder Bias: \\[\\begin{equation} B(\\hat\\theta)=E(\\hat\\theta)-\\theta \\tag{8.2} \\end{equation}\\] Ein verzerrter Schätzer, dessen Verzerrung gegen Null geht wenn der Stichprobenumfang \\(n\\) gegen Unendlich geht heißt asymptotisch erwartungstreu: \\[\\begin{equation} \\lim_{n\\to\\infty}E(\\hat\\theta)=\\theta \\tag{8.3} \\end{equation}\\] 8.2 Normalverteilung: Schätzer für \\(\\mu\\) Der Schätzer für \\(\\mu\\) ist: \\[\\begin{equation} \\hat\\mu=\\bar x \\tag{8.4} \\end{equation}\\] In Worten: Der Schätzer des Mittelwertes einer normalverteilten Grundgesamheit \\(\\hat\\mu\\) ist der arithmetische Mittelwert einer Stichprobe aus dieser Grundgesamtheit. Der Schätzer wird in der Regel nicht exakt mit dem wahren Wert übereinstimmen, nur im Mittel - er ist erwartungstreu (s.o.). Da \\(\\hat\\mu\\) als Zufallsvariable interpretiert wird, kann diese Unsicherheit als sogenannter Standardfehler des Mittelwertschätzers \\(s_{\\hat\\mu}\\) ausgedrückt werden: \\[\\begin{equation} s_{\\hat\\mu}=\\frac{s}{\\sqrt{n}} \\tag{8.5} \\end{equation}\\] Wobei \\(s\\) die Standardabweichung der Stichprobe ist (vgl. Kapitel 4) und \\(n\\) der Stichprobenumfang. Wir sehen an der Formel, dass je kleiner die Standardabweichung der Stichprobe ist und je größer der Stichprobenumfang ist desto besser die Schätzung wird - das ist die o.g. Konsistenzeigenschaft, die wir von einem Schätzer erwarten. Der Standardfehler wird mit wahrscheinlichkeitstheoretischen Annahmen verknüpft und geht so in die Berechnung des sogenannten Konfidenzintervalls ein, das die Unsicherheit des Schätzers mit Wahrscheinlichkeiten ausdrückt. Das Konfidenzintervall des Mittelwertschätzers wollen wir im Folgenden einmal herleiten. Das bildet die Grundlage für viele ähnliche Herleitungen, die später bei den Tests und bei der Regression kommen, die wir dann aber nicht mehr so ausführlich behandeln werden. Unter Annahme einer normalverteilten Grundgesamtheit folgt der standardisierte Schätzer des Mittelwertes bei wiederholtem Stichprobenziehen einer sogenannten t-Verteilung: \\[\\begin{equation} \\frac{\\hat\\mu-\\mu}{s_{\\hat\\mu}}\\sim t_{n-1} \\tag{8.6} \\end{equation}\\] Die Herleitung können Sie z.B. in Mittag (2016), S. 218ff nachlesen. Hier soll uns nur die Bedeutung dieser Formel interessieren. \\(\\hat\\mu-\\mu\\) ist die Differenz des Schätzers vom wahren Mittelwert (den wir nicht kennen). Diese Differenz wird mit dem Standardfehler des Schätzers \\(s_{\\hat\\mu}\\) normalisiert. Das Symbol \\(\\sim\\) (Tilde) heißt ist verteilt gemäß (vgl. Kapitel 7). Und \\(t_{n-1}\\) steht für die t-Verteilung, die einen Parameter hat (genannt Freiheitsgrade), der hier den Wert \\(n-1\\) annimmt. Man spricht von einer t-Verteilung mit \\(n-1\\) Freiheitsgraden. Je größer diese Zahl desto schmaler die t-Verteilung. Abbildung 8.1 zeigt die t-Verteilung für verschiedene Freiheitsgrade. Wenn \\(n\\) gegen Unendlich geht geht die t-Verteilung in die Standardnormalverteilung \\(N(0,1)\\) über (vgl. Abbildung 7.4). Man sieht in Abbildung 8.1, dass sich schon zwischen 100 und 1000 Freiheitsgraden nicht mehr viel an der Breite der t-Verteilung ändert. Wie die Standardnormalverteilung geht die t-Verteilung von \\(-\\infty\\) bis \\(+\\infty\\) und ist symmetrisch um einen Mittelwert 0. D.h. mit dieser Verteilung erwarten wir bei wiederholter Stichprobenziehung im Mittel den wahren Mittelwert der Grundgesamtheit genau zu schätzen, \\(E\\left(\\hat\\mu-\\mu\\right)=0\\). Aber von Stichprobe zu Stichprobe erwarten wir eine Variation des Schätzwertes gemäß der t-Verteilung, Formel (8.6). Abbildung 8.1: Links: Dichtefunktion der t-Verteilung einer beliebigen Zufallsvariablen \\(Z\\) für verschiedene Freiheitsgrade (der einzige Parameter der t-Verteilung heißt Freiheitsgrade). Rechts: Verteilungsfunktion der entsprechenden t-Verteilungsvarianten. Das zentrale 95% Konfidenzintervall einer beliebigen Zufallsvariablen \\(Z\\sim t_{n-1}\\) ist: \\[\\begin{equation} \\Pr\\left(-t_{n-1;0.975}\\leq Z\\leq t_{n-1;0.975}\\right)=0.95 \\tag{8.7} \\end{equation}\\] \\(t_{n-1;0.975}\\) steht für das 0.975-Quantil der t-Verteilung mit \\(n-1\\) Freiheitsgraden (Abbildung 8.2). Da die t-Verteilung symmetrisch ist gilt \\(-t_{n-1;0.975}=t_{n-1;0.025}\\). Abbildung 8.2: Links: Dichtefunktion der t-Verteilung einer beliebigen Zufallsvariablen \\(Z\\), hier mit 10 Freiheitsgraden. Die Grenzen des zentralen 95% Konfidenzintervalls sind rot markiert. Es umschließt die zentralen 95% der Verteilung, so dass 2.5% links und 2.5% rechts davon liegen. Rechts: Verteilungsfunktion der entsprechenden t-Verteilung. Die Grenzen des Konfidenzintervalls kann man hier direkt ablesen, als 0.025- und 0.975-Quantil. Da die t-Verteilung symmetrisch ist gilt \\(-t_{n-1;0.975}=t_{n-1;0.025}\\). Setzen wir nun Formel (8.6) für \\(Z\\) in Formel (8.7) ein erhalten wir: \\[\\begin{equation} \\Pr\\left(-t_{n-1;0.975}\\leq\\frac{\\hat\\mu-\\mu}{s_{\\hat\\mu}}\\leq t_{n-1;0.975}\\right)=0.95 \\tag{8.8} \\end{equation}\\] Multplizieren der inneren Ungleichung mit \\(s_{\\hat\\mu}\\) ergibt: \\[\\begin{equation} \\Pr\\left(-t_{n-1;0.975}\\cdot s_{\\hat\\mu}\\leq\\hat\\mu-\\mu\\leq t_{n-1;0.975}\\cdot s_{\\hat\\mu}\\right)=0.95 \\tag{8.9} \\end{equation}\\] Subtraktion von \\(\\hat\\mu\\) ergibt: \\[\\begin{equation} \\Pr\\left(-\\hat\\mu-t_{n-1;0.975}\\cdot s_{\\hat\\mu}\\leq-\\mu\\leq -\\hat\\mu+t_{n-1;0.975}\\cdot s_{\\hat\\mu}\\right)=0.95 \\tag{8.9} \\end{equation}\\] Und schlussendlich Multiplikation mit \\(-1\\): \\[\\begin{equation} \\Pr\\left(\\hat\\mu-t_{n-1;0.975}\\cdot s_{\\hat\\mu}\\leq\\mu\\leq \\hat\\mu+t_{n-1;0.975}\\cdot s_{\\hat\\mu}\\right)=0.95 \\tag{8.10} \\end{equation}\\] Merke: Um den letzten Schritt zu verstehen muss man wissen, dass sich bei der Multiplikation einer Ungleichung mit einer negativen Zahl die Vergleichszeichen (hier \\(\\leq\\)) umdrehen. Danach wurde die Ungleichung wieder von klein nach groß geordnet. Setzen wir nun die Formeln (8.4) und (8.5) in Formel (8.10) ein erhalten wir: \\[\\begin{equation} \\Pr\\left(\\bar x-t_{n-1;0.975}\\cdot\\frac{s}{\\sqrt{n}}\\leq\\mu\\leq\\bar x+t_{n-1;0.975}\\cdot\\frac{s}{\\sqrt{n}}\\right)=0.95 \\tag{8.11} \\end{equation}\\] Das ist nun das zentrale 95% Konfidenzintervall, das besagt: Die Wahrscheinlichkeit, dass der wahre Wert \\(\\mu\\) größer oder gleich \\(\\bar x-t_{n-1;0.975}\\cdot\\frac{s}{\\sqrt{n}}\\) und kleiner oder gleich \\(\\bar x+t_{n-1;0.975}\\cdot\\frac{s}{\\sqrt{n}}\\) ist, ist 0.95. Man kann das Konfidenzintervall auch mit den Intervallgrenzen so schreiben: \\[\\begin{equation} KI=\\left[\\bar x-t_{n-1;0.975}\\cdot\\frac{s}{\\sqrt{n}};\\bar x+t_{n-1;0.975}\\cdot\\frac{s}{\\sqrt{n}}\\right] \\tag{8.12} \\end{equation}\\] Beachten Sie die Interpretation des Konfidenzintervalls, die sich aus dem klassischen Wahrscheinlichkeitsbegriff ergibt (vgl. Kapitel 6): Bei hypothetischer wiederholter Stichprobenziehung des selben Umfangs aus der selben Grundgesamtheit enthält in 95% der Fälle das Konfidenzintervall den wahren Wert, während die restlichen 5% der Intervalle den wahren Wert nicht enthalten (Abbildung 8.3). Für die einzige Stichprobe, die wir jeweils vorliegen haben ist das keine Wahrscheinlichkeit, dass der wahre Wert in den Grenzen des Konfidenzinervalls liegt! Das wird in der Praxis often verwechselt, also behalten Sie sich das gut. # plot 1: 5 Zufallszahlen # initialise xbar &lt;- rep(NA,50) ki &lt;- cbind(rep(NA,50),rep(NA,50)) # Schleife über 50 Wiederholungen for(i in 1:50){ # ziehe 5 Zufallszahlen aus Standardnormalverteilung N(0,1) n &lt;- 5 x &lt;- rnorm(n, mean=0, sd=1) # Mittelwert xbar[i] &lt;- mean(x) # zentrales 95% Konfidenzintervall ki[i,1:2] &lt;- xbar[i] + c(-1,1)*qt(0.975,df=n-1)*sd(x)/sqrt(n) } # plotte Mittelwerte und Konfidenzintervalle für Wiederholungen wh &lt;- seq(1,50,1) plot(wh, xbar, type=&#39;p&#39;, pch=15, ylim=c(-4,4), xlab=&quot;Wiederholung&quot;, ylab=&quot;Mittelwertschätzer&quot;, main=&quot;n=5&quot;) segments(x0=wh, y0=ki[,1], x1=wh, y1=ki[,2]) # markiere jene Wiederholungen rot, # deren Konfidenzintervall nicht den wahren Mittelwert 0 einschließt id &lt;- ki[,1] &gt;= 0 | ki[,2] &lt;= 0 points(wh[id], xbar[id], pch=15, col=&quot;red&quot;) segments(x0=wh[id], y0=ki[id,1], x1=wh[id], y1=ki[id,2], col=&quot;red&quot;) # plot 2: 10 Zufallszahlen # initialise xbar &lt;- rep(NA,50) ki &lt;- cbind(rep(NA,50),rep(NA,50)) # Schleife über 50 Wiederholungen for(i in 1:50){ # ziehe 10 Zufallszahlen aus Standardnormalverteilung N(0,1) n &lt;- 10 x &lt;- rnorm(n, mean=0, sd=1) # Mittelwert xbar[i] &lt;- mean(x) # zentrales 95% Konfidenzintervall ki[i,1:2] &lt;- xbar[i] + c(-1,1)*qt(0.975,df=n-1)*sd(x)/sqrt(n) } # plotte Mittelwerte und Konfidenzintervalle für Wiederholungen wh &lt;- seq(1,50,1) plot(wh, xbar, type=&#39;p&#39;, pch=15, ylim=c(-4,4), xlab=&quot;Wiederholung&quot;, ylab=&quot;Mittelwertschätzer&quot;, main=&quot;n=10&quot;) segments(x0=wh, y0=ki[,1], x1=wh, y1=ki[,2]) # markiere jene Wiederholungen rot, # deren Konfidenzintervall nicht den wahren Mittelwert 0 einschließt id &lt;- ki[,1] &gt;= 0 | ki[,2] &lt;= 0 points(wh[id], xbar[id], pch=15, col=&quot;red&quot;) segments(x0=wh[id], y0=ki[id,1], x1=wh[id], y1=ki[id,2], col=&quot;red&quot;) Abbildung 8.3: 50 Wiederholungen von Stichprobenziehungen aus der gleichen Grundgesamtheit, \\(N(0,1)\\), aus denen jeweils der Mittelwert geschätzt wurde, einmal mit \\(n=5\\) (links) und einmal mit \\(n=10\\) (rechts). Gezeigt ist jeweils der geschätzte Mittelwert mit Konfidenzintervall. Konfidenzintervalle, die den wahren Wert \\(\\mu=0\\) nicht einschließen sind rot markiert; das sind bei einer ausreichenden Anzahl Wiederholungen 5%, d.h. in diesem Fall 2-3 von 50. Wenn hier die Grafiken abweichen dann liegt das an der geringen Anzahl Wiederholungen. Außerdem sehen wir, dass bei größerem Stichprobenumfang der Standardfehler und damit die Breite der Konfidenzintervalle kleiner sind (\\(n=10\\) rechts im Vergleich zu \\(n=5\\) links); vgl. Gleichung (8.5). Nach: Mittag (2016). 8.3 Normalverteilung: Schätzer für \\(\\sigma\\) Der Schätzer für \\(\\sigma\\) ist: \\[\\begin{equation} \\hat\\sigma=s=\\sqrt{\\frac{\\sum_{i=1}^{n}\\left(x_i-\\bar x\\right)^2}{n-1}} \\tag{8.13} \\end{equation}\\] In Worten: Der Schätzer der Standardabweichung einer normalverteilten Grundgesamheit \\(\\hat\\sigma\\) ist die Standardabweichung einer Stichprobe aus dieser Grundgesamtheit. Die Herleitung beider Schätzer erfolgt über die sogenannte Maximum-Likelihood-Theorie, auf die wir hier nicht näher eingehen wollen. Sie können die Herleitung aber z.B. in Dormann (2013), S. 48ff nachlesen. Auf der Maximum-Likelihood-Theorie basiert die Annahme unabhängig identisch verteilter (u.i.v.) Daten. Nur wenn diese erfüllt ist, sind wahrscheinlichkeitstheoretische Aussagen wie Standardfehler und Konfidenzintervalle gültig! Oft, z.B. wenn Daten korrelieren, ist dies nur annäherungsweise der Fall. 8.4 Quantil-Quantil-Diagramm (QQ-Plot) Ein visueller Test, ob eine Stichprobe aus einer bestimmten Verteilung entstammt ist das sogenannte Quantil-Quantil-Diagramm (QQ-Plot). Abbildung 8.4 zeigt QQ-Plots für unsere Entfernungsdaten, einmal original und einmal log-transformiert. # QQ-Plot für Originaldaten qqnorm(reisedat$distanz) qqline(reisedat$distanz) # QQ-Plot für log-transformierte Daten qqnorm(log(reisedat$distanz)) qqline(log(reisedat$distanz)) Abbildung 8.4: Links: QQ-Plot der Entfernungsdaten. Wie wir in Kapitel 3 und Kapitel 7 gesehen haben sind die Originaldaten rechtsschief, was wir an dem nach oben gewölbten QQ-Plot sehen (vgl. Abbildung 8.5). Eine Lognormalverteilung passte hier ganz gut. Rechts: QQ-Plot der log-transformierten Entfernungsdaten. Hier passt die Normalverteilung ganz gut, was wir an dem etwa geraden QQ-Plot sehen. Im QQ-Plot stellt jeder Datenpunkt ein bestimmtes Quantil der empirischen Verteilung dar (vgl. Kapitel 3). Dieses Quantil (nach Standardisierung; vgl. Kapitel 4) wird gegen den Wert dieses Quantils geplotted (vertikale Achse), der unter einer Standard-Normalverteilung (horizontale Achse) erwartet wird. Die resultierenden Formen sagen etwas über die Verteilung aus (Abbildung 8.5). Im Fall einer Normalverteilung fallen alle Punkte auf eine gerade Linie. # 1) simuliere normalverteilte Daten x_norm &lt;- rnorm(100, 0, 1) # QQ-Plot qqnorm(x_norm, xlim = c(-4, 4), ylim = c(-4, 4), main=&#39;normalverteilte Daten&#39;) qqline(x_norm) # 2) simuliere rechtsschiefe Daten x_right &lt;- rlnorm(100, 0, 1) # QQ-Plot qqnorm(x_right, xlim = c(-4, 4), ylim = c(-2, 6), main=&#39;rechtsschiefe Daten&#39;) qqline(x_right) # 3) simuliere linksschiefe Daten x_left &lt;- -x_right # QQ-Plot qqnorm(x_left, xlim = c(-4, 4), ylim = c(-6, 2), main=&#39;linksschiefe Daten&#39;) qqline(x_left) # 4) simuliere Verteilung mit flachen Flanken x_thick &lt;- rcauchy(100, 0, 1) # QQ-Plot qqnorm(x_thick, xlim = c(-4, 4), ylim = c(-8, 8), main=&#39;Verteilung mit flachen Flanken&#39;) qqline(x_thick) Abbildung 8.5: Charakteristische Formen des QQ-Plots und was diese für die Verteilung bedeuten. Eine einfache Verteilung mit steileren Flanken als die der Normalverteilung konnte ich nicht finden. Dieser Fall würde ein S-Form aufweisen, wie die Variante mit flachen Flanken, nur an der Diagonale gespiegelt. Literatur "],["09-tests.html", "Kapitel 9 Statistische Tests 9.1 Grundprinzipien statistischer Tests 9.2 t-Test (Vergleich von Mittelwerten) 9.3 Interpretation des p-Wertes 9.4 F-Test (Vergleich von Varianzen) 9.5 Verteilungstest (Kolmogorow-Smirnow-Test) 9.6 Unabhängigkeitstest (Chi-Quadrat-Test)", " Kapitel 9 Statistische Tests Wie wir in Kapitel 8 gelernt haben geht es in der schließenden Statistik um die Verdichtung der Informationen in einer Stichprobe in Form von Stichprobenfunktionen, mit denen wir bestimmte Parameter der Grundgesamtheit schätzen (vgl. Mittag (2016), Abb. 14.1, S. 212). Während es sich im Fall von Verteilungsparametern bei den Stichprobenfunktionen v.a. um den Mittelwert und die Standardabweichung handelt, sind die Stichprobenfunktionen im Fall von statistischen Tests sogenannte Teststatistiken, die die Informationen in der Stichprobe verdichten. Wir werden das vorliegende Kapitel über die nächsten drei Wochen lesen. Dabei werden wir anhand der folgenden neun Beispiele vier verschiedene Tests kennenlernen: Beispiel 1: Laut dieser Umfrage hält jeder zweite Berufspendler eine durchschnittliche Fahrtzeit von bis zu 60 min pro Strecke für akzeptabel. Im Wintersemester 2019/20 haben wir die Anreisezeiten der Studierenden dieses Kurses abgefragt.10 Ein Histogramm dieser Stichprobe sehen Sie in Abbildung 9.1. Ist der mit dem Stichprobenmittel \\(\\bar x=50.8\\) geschätzte Mittelwert \\(\\mu\\) der Grundgesamtheit kleiner als der Vergleichswert \\(\\mu_0=60\\) aus der Studie? Beziehungsweise, ist der Unterschied statistisch signifikant, wenn wir die Streuung der Stichprobe berücksichtigen? Die Frage kleiner als wenn es um einen Mittelwert geht beantwortet der sogenannte linksseitige Einstichproben-t-Test (Kapitel 9.2.1). Abbildung 9.1: Histogramm des Merkmals Anreisezeit der Reisedaten aus dem Wintersemester 2019/20. Die vertikale Linie markiert den Mittelwert von 50.8 min. Beispiel 2: Ist der mit dem Stichprobenmittel \\(\\bar x=50.8\\) geschätzte Mittelwert \\(\\mu\\) der Grundgesamtheit ungleich dem Vergleichswert \\(\\mu_0=60\\) aus der Studie? Wenn wir die Frage so formulieren brauchen wir einen sogenannten zweiseitigen Einstichproben-t-Test (Kapitel 9.2.1). Beispiel 3: Laut derselben Umfrage nehmen 21% der Pendler eine Fahrtzeit zwischen 30 und 45 min in Kauf. Ist der mit dem Stichprobenmittel \\(\\bar x=50.8\\) geschätzte Mittelwert \\(\\mu\\) der Grundgesamtheit größer als der Vergleichswert \\(\\mu_0=45\\) aus der Studie? Beziehungsweise, ist der Unterschied statistisch signifikant, wenn wir die Streuung der Stichprobe berücksichtigen? Die Frage größer als beantwortet der rechtsseitige Einstichproben-t-Test (Kapitel 9.2.1).11 Beispiel 4: Die Stichprobe wurde zufällig geteilt (Abbildung 9.2). Sind die mit den neuen Stichprobenmitteln \\(\\bar x_1=50.4\\) und \\(\\bar x_2=51.2\\) geschätzten Mittelwerte \\(\\mu_1\\) und \\(\\mu_2\\) ungleich? Beziehungsweise, können wir statistisch nachvollziehen, dass die beiden Stichproben Grundgesamtheiten mit dem selben Mittelwert entstammen? Diese Frage beantwortet der sogenannte Zweistichproben-t-Test (zweiseitig) (Kapitel 9.2.2). Abbildung 9.2: Histogramme des Merkmals Anreisezeit der Reisedaten aus dem Wintersemester 2019/20. Die Stichprobe wurde zufällig geteilt. Die vertikalen Linien markieren die Mittelwerte \\(\\bar x_1=50.4\\) (links) und \\(\\bar x_2=51.2\\) (rechts). Die Varianzen sind \\(s_1^2=352\\) und \\(s_2^2=384\\). Beispiel 5 (Dormann 2013): Auf den Nord- und Südseiten einer Stichprobe von Bäumen wurde jeweils die Anzahl Moosarten bestimmt (Abbildung 9.3). Ist die Anzahl Moosarten auf der Nord- und Südseite derselben Bäume unterschiedlich? Dafür brauchen wir den gepaarten Zweistichproben-t-Test (zweiseitig) (Kapitel 9.2.4). Abbildung 9.3: Verteilung der Anzahl Moosarten auf der Südseite (links) und Nordseite (rechts) derselben Bäume. Daten aus: Dormann (2013). Beispiel 6: Zurück zu den beiden neuen Stichproben aus Beispiel 4 (Abbildung 9.2). Ist die Varianz \\(\\sigma_2^2\\) (gegeben \\(s_2^2=384\\)) größer als die Varianz \\(\\sigma_1^2\\) (gegeben \\(s_1^2=352\\))? Beziehungsweise, können wir statistisch nachvollziehen, dass die beiden Stichproben Grundgesamtheiten mit derselben Varianz entstammen? Diese Frage beantwortet der sogenannte F-Test (rechtsseitig) (Kapitel 9.4). Beispiel 7: Entstammt die Stichprobe der Reisezeit (Abbildung 9.1) einer normalverteilten Grundgesamtheit? Die Parameter dieser Normalverteilung werden anhand der Stichprobe geschätzt. Diese Frage beantwortet der sogenannte Einstichproben-Kolmogorow-Smirnow-Test (Kapitel 9.5). Beispiel 8: Entstammen die beiden Teilstichproben der Reisezeit (Abbildung 9.2) einer gemeinsamen Verteilung? Beziehungsweise, können wir das statistisch nachvollziehen? Dafür brauchen wir den Zweistichproben-Kolmogorow-Smirnow-Test (Kapitel 9.5). Beispiel 9: Zurück zum Volksentscheid Tegel aus Kapitel 5. Gibt es einen Zusammenhang zwischen Bezirk und Votum beim Volksentscheid Tegel? Beziehungsweise, ist der geringe Zusammenhang, den wir bereits festgestellt haben, statistisch signifikant? Diese Frage beantwortet der sogenannte Chi-Quadrat-Test (Kapitel 9.6). 9.1 Grundprinzipien statistischer Tests Die folgenden Prinzipien liegen allen statistischen Tests zugrunde, wobei wir vieles am Beispiel des t-Tests demonstrieren, der dann in Kapitel 9.2 vollständig behandelt wird. 9.1.1 Nullhypothese und Alternativhypothese Das Formulieren von Hypothesen ist die formale Vorgehensweise, Fragestellungen wie die oben genannten Beispiele statistisch zu übersetzen. Bsp. 3: Ist \\(\\mu\\) (gegeben \\(\\bar x=50.8\\)) größer als \\(\\mu_0=45\\)? Jeder statistische Test verlangt eine bestimmte Nullhypothese \\(H_0\\). Bsp. 3: \\(H_0: \\mu\\leq\\mu_0\\) Diese wird getestet. Die Alternativhypothese \\(H_1\\) ist aber die, die sich zunächst aus den Zahlenwerten ergibt. Bsp. 3: \\(H_1: \\mu&gt;\\mu_0\\) Hypothesen können nur abgelehnt (falsifiziert) werden. Das Annehmen von Hypothesen gilt nur bis auf weiteres. 9.1.2 Zweiseitig und einseitig Wir unterscheiden zweiseitige und einseitige Tests. Bei zweiseitigen Tests wird auf ungleich/gleich getestet. Bsp. 2: Ist \\(\\mu\\) (gegeben \\(\\bar x=50.8\\)) ungleich \\(\\mu_0=60\\)? Bei einseitigen Tests wird auf kleiner/nicht kleiner oder größer/nicht größer getestet. Bsp. 1: Ist \\(\\mu\\) (gegeben \\(\\bar x=50.8\\)) kleiner als \\(\\mu_0=60\\)? Ein einseitiger Test ist in der Regel aussagekräftiger. Die Ergebnisse beider Tests lassen sich aber einfach ineinander überführen - wie wir noch sehen werden. 9.1.3 Die Teststatistik Jeder Test hat eine bestimmte Teststatistik (Prüfwert) von der wir wissen, wie sie bei wiederholtem Stichprobenziehen verteilt ist (unter bestimmten Annahmen), falls die Nullhypothese wahr ist.12 Die Teststatistik eines Einstichproben-t-Tests beispielsweise ist: \\[\\begin{equation} t_s=\\frac{\\hat\\mu-\\mu_0}{s_{\\hat\\mu}}\\sim t_{n-1} \\tag{9.1} \\end{equation}\\] \\(\\hat\\mu\\) ist der Mittelwertschätzer; in Bsp. 1 \\(\\hat\\mu=\\bar x=50.8\\). \\(\\mu_0\\) ist der Wert, mit dem wir den Schätzer vergleichen; in Bsp. 1 \\(\\mu_0=60\\). \\(s_{\\hat\\mu}\\) ist der Standardfehler des Mittelwertschätzers. Ist die Grundgesamtheit normalverteilt mit \\(\\mu=\\mu_0\\), dann ist der so standardisierte Schätzer des Mittelwertes (die Teststatistik \\(t_s\\)) bei wiederholtem Stichprobenziehen t-verteilt mit \\(n-1\\) Freiheitsgraden (vgl. Kapitel 8).13 9.1.4 Was genau getestet wird Wenn die Teststatistik nahe dem Zentrum der Verteilung ist, die unter der Nullhypothese zu erwarten ist, d.h. in einem Bereich hoher Wahrscheinlichkeit, dann lehnen wir die Nullhypothese nicht ab. In Abbildung 9.4 ist das für die Teststatistik \\(t_s\\) in blau und die t-Verteilung dargestellt. Im Bsp. 2 würden wir in so einem Fall bis auf weiteres schließen, dass \\(\\mu\\) (gegeben \\(\\bar x=50.8\\)) gleich \\(\\mu_0=60\\) ist. Ist die Teststatistik dagegen in den Extremen der Verteilung, d.h. in einem Bereich geringer Wahrscheinlichkeit, dann lehnen wir die Nullhypothese ab. In Abbildung 9.4 ist das mit den roten Pfeilen verdeutlicht. Im Bsp. 2 würden wir in so einem Fall schließen, dass \\(\\mu\\) (gegeben \\(\\bar x=50.8\\)) ungleich \\(\\mu_0=60\\) ist. Das ist in dem Beispiel tatsächlich das Ergebnis - wie wir noch sehen werden. Abbildung 9.4: Grundprinzip des statistischen Testens, hier dargestellt für einen konstruierten t-Test: Verteilungsfunktion der t-Verteilung mit 97 Freiheitsgraden, mit Teststatistik \\(t_s\\) (blau) im Zentrum der Verteilung, d.h. im Bereich hoher Wahrscheinlichkeit unter der Nullhypothese. Wir lehnen die Nullhypothese nicht ab. Wäre die Teststatistik dagegen in den Extremen der Verteilung (mit roten Pfeilen verdeutlicht), wäre sie im Bereich geringer Wahrscheinlichkeit unter der Nullhypothese. In dem Fall lehnen wir die Nulhypothese ab. Beachte: Bei der zweiseitigen Version des Tests schauen wir auf beiden Seiten der Verteilung (beide Extreme), während wir bei dem linksseitigen Test nur auf die linke und bei dem rechtsseitigen Test nur auf die rechte Seite schauen. Auf der linken Seite von Abbildung 9.4 befinden wir uns mit der Teststatistik \\(t_s\\) wenn \\(\\hat\\mu&lt;\\mu_0\\), d.h. wir testen kleiner/nicht kleiner. Auf der rechten Seite befinden wir uns wenn \\(\\hat\\mu&gt;\\mu_0\\), d.h. wir testen größer/nicht größer. 9.2 t-Test (Vergleich von Mittelwerten) Jetzt haben wir schon viel über den t-Test gehört; er ist dazu da, Mittelwerte zu vergleichen. Wenn wir den Mittelwert einer Stichprobe gegen einen Vergleichswert testen dann ist das der Einstichproben-t-Test. Wenn wir die Mittelwerte zweier Stichproben vergleichen dann ist das der Zweistichproben-t-Test. Wenn die beiden Stichproben gepaart sind, d.h. wenn die Merkmalswerte jeweils für die selbe statistische Einheit erhoben wurden, dann spricht man vom gepaarten Zweistichproben-t-Test. Die Teststatistik ist in allen diesen Fällen ähnlich. Schauen wir uns nun die Varianten des t-Tests anhand der Beispiele an. 9.2.1 Einstichproben-t-Test 9.2.1.1 Beispiel 1 (linksseitiger Einstichproben-t-Test) Siehe Abbildung 9.1: Ist der mit dem Stichprobenmittel \\(\\bar x=50.8\\) geschätzte Mittelwert \\(\\mu\\) der Grundgesamtheit kleiner als der Vergleichswert \\(\\mu_0=60\\)? Beziehungsweise, ist der Unterschied statistisch signifikant, wenn wir die Streuung der Stichprobe berücksichtigen? Die Nullhypothese ist in diesem Fall, dass der Mittelwert größer oder gleich dem Vergleichswert ist: \\[H_0:\\mu\\geq\\mu_0\\] Die Alternativhypothese ist, dass der Mittelwert kleiner als der Vergleichswert ist: \\[H_1:\\mu&lt;\\mu_0\\] Die Alternativhypothese ergibt sich wie gesagt aus den Zahlenwerten der Stichprobe, deren Mittelwert tatsächlich kleiner is als \\(\\mu_0=60\\). Wir hoffen, die Alternativhypothese zu bestätigen indem wir die Nullhypothese ablehnen. Die vorliegende Formulierung der Hypothesen ist der linksseitige Test. Die Teststatistik (Formel (9.1)) rechnen wir anhand der Stichprobe wie folgt aus (vgl. Kapitel 8): \\[t_s=\\frac{\\hat\\mu-\\mu_0}{s_{\\hat\\mu}}\\sim t_{n-1}\\] \\[t_s=\\frac{\\bar x-\\mu_0}{s_{\\bar x}}\\sim t_{n-1}\\] \\[t_s=\\frac{\\bar x-\\mu_0}{s}\\cdot\\sqrt{n}\\sim t_{n-1}\\] Setzen wir die Zahlenwerte aus der Stichprobe ein (reisedat19$t enthält die Merkmalswerte für Anreisezeit aus dem Wintersemester 2019/20): # Mittelwert # na.rm=TRUE ignoriert NAs xbar &lt;- mean(reisedat19$t, na.rm=TRUE) xbar ## [1] 50.81 # Vergleichswert mu0 &lt;- 60 # Standardabweichung # na.rm=TRUE ignoriert NAs s &lt;- sd(reisedat19$t, na.rm=TRUE) s ## [1] 19.11 # Stichprobenumfang # !is.na(reisedat19$t) verweist auf die Werte, die nicht NA sind n &lt;- length(reisedat19$t[!is.na(reisedat19$t)]) n ## [1] 98 # Teststatistik ts &lt;- (xbar - mu0)/s*sqrt(n) ts ## [1] -4.764 Dieser Wert der Teststatistik liegt in den Extremen der t-Verteilung, die unter der Nullhypothese zu erwarten ist: plot(seq(-5,5,0.01), pt(seq(-5,5,0.01), n-1), ylim=c(0,1), type=&#39;l&#39;, xlab=&#39;Z=t_s&#39;, ylab=&#39;Verteilungsfunktion&#39;) lines(c(1, 1)*ts, c(0, pt(ts, n-1)), col=&#39;blue&#39;) text(ts,-0.2,&quot;t_s&quot;, col=&quot;blue&quot;, xpd=TRUE) Wie extrem der Wert der Teststatistik ist (wie unwahrscheinlich er unter der Nullhypothese ist) misst der sogenannte p-Wert. Der p-Wert ist die Wahrscheinlichkeit, unter Annahme der Nullhypothese, durch Zufall einen extremeren Wert als den der Teststatistik zu erhalten. In Formelsprache: \\[\\Pr\\left(Z&lt;t_s\\right)=F_t\\left(t_s\\right)\\] Die Wahrscheinlichkeit eines kleineren Wertes als den der vorliegenden Teststatistik ist gleich der Verteilungsfunktion der t-Verteilung an der Stelle der Teststatistik (vgl. Kapitel 7). Mit Zahlenwerten: pt(ts, n-1) ## [1] 3.331e-06 Der p-Wert ist sehr klein, d.h. es ist sehr unwahrscheinlich, dass dieser Wert der Teststatisk durch Zufall zustande kam falls die Nullhypothese wahr ist, d.h. wir sollten die Nullhypothese ablehnen. In der Praxis entscheiden wir das auf Basis eines sogenannten Signifikanzniveaus von 0.01: Ist der p-Wert kleiner oder gleich 0.01 lehnen wir die Nullhypothese ab. Ist der p-Wert größer als 0.01 behalten wir die Nullhypothese bis auf weiteres bei. Das Signifikanzniveau von 0.01 ist dabei reine Konvention! Tatsächlich gab es dazu kürzlich eine Debatte unter Statistikern. Und R beispielsweise gibt Signifikanz zu mehreren Niveaus an. Grundsätzlich ist immer der p-Wert anzugeben; dann kann jede Person ihr eigenes Signifikanzniveau ansetzen. Für unser Beispiel 1 schließen wir also: Der Unterschied zwischen dem mit dem Stichprobenmittel \\(\\bar x\\) geschätzten Mittelwert \\(\\mu\\) der Grundgesamtheit und dem Vergleichswert \\(\\mu_0=60\\) ist statistisch signifikant. 9.2.1.2 Beispiel 2 (zweiseitiger Einstichproben-t-Test) Wir können die Fragestellung auch schwächer formulieren, als zweiseitiges Testproblem: Ist der mit dem Stichprobenmittel \\(\\bar x=50.8\\) geschätzte Mittelwert \\(\\mu\\) der Grundgesamtheit ungleich dem Vergleichswert \\(\\mu_0=60\\)? Die Nullhypothese ist in diesem Fall, dass der Mittelwert gleich dem Vergleichswert ist:14 \\[H_0:\\mu=\\mu_0\\] Die Alternativhypothese ist, dass die beiden Wert nicht gleich sind: \\[H_1:\\mu\\ne\\mu_0\\] Die Teststatistik ist die gleiche wie im linksseitigen Fall, nur dass wir jetzt auf beide Extreme der t-Verteilung schauen, die unter der Nullhypothese zu erwarten ist: plot(seq(-5,5,0.01), pt(seq(-5,5,0.01), n-1), ylim=c(0,1), type=&#39;l&#39;, xlab=&#39;Z=t_s&#39;, ylab=&#39;Verteilungsfunktion&#39;) lines(c(1, 1)*ts, c(0, pt(ts, n-1)), col=&#39;blue&#39;) lines(c(1, 1)*(-ts), c(0, pt(-ts, n-1)), col=&#39;blue&#39;) text(ts,-0.2,&quot;t_s&quot;, col=&quot;blue&quot;, xpd=TRUE) Wir spiegeln also den Wert der Teststatistik an Null, und der p-Wert ist jetzt die Wahrscheinlichkeit eines Wertes der Teststatistik jenseits dieser beiden Grenzen: \\[\\Pr\\left(Z&lt;t_s\\right)+\\Pr\\left(Z&gt;-t_s\\right)=2\\cdot\\Pr\\left(Z&gt;\\left|t_s\\right|\\right)=2\\cdot \\left(1-F_t\\left(\\left|t_s\\right|\\right)\\right)\\] Die Wahrscheinlichkeit eines extremeren Wertes als den der vorliegenden Teststatistik (auf beiden Seiten) ist zweimal die Wahrscheinlichkeit eines größeren Wertes als den Absolutwert \\(\\left|t_s\\right|\\) der vorliegenden Teststatistik - wegen der Symmetrie der t-Verteilung um Null. Die Wahrscheinlichkeit eines größeren Wertes ist Eins minus die Verteilungsfunktion an der entsprechenden Stelle (vgl. Kapitel 7). Mit Zahlenwerten: 2*(1-pt(abs(ts),n-1)) ## [1] 6.663e-06 Wie wir sehen ist der p-Wert des zweiseitigen Tests genau zweimal der p-Wert des einseitigen Tests. D.h. wenn der zweiseitige Test signifikant ist, dann ist auch der einseitige Test signifikant. In der Praxis wird oft ein zweiseitiger Test durchgeführt und dann für die einseitige Variante, die sich aus den Zahlenwerten ergibt (hier Bsp. 1), der p-Wert halbiert. Für Beispiel 2 schließen wir jedenfalls: Es kann ausgeschlossen werden, dass der mit dem Stichprobenmittel \\(\\bar x\\) geschätzte Mittelwert \\(\\mu\\) der Grundgesamtheit gleich dem Vergleichswert \\(\\mu_0=60\\) ist. Der zweiseitige Test ist wie gesagt ein schwächerer Test als der einseitige, den wir bereits in Bsp. 1 durchgeführt haben. In der Praxis würde man die Tests wie gesagt nicht so hintereinander schalten, sondern umgekehrt. 9.2.1.3 Beispiel 3 (rechtsseitiger Einstichproben-t-Test) Es fehlt noch der rechtsseitige Test, für den wir eine Fragestellung wie folgt konstruiert haben: Ist der mit dem Stichprobenmittel \\(\\bar x=50.8\\) geschätzte Mittelwert \\(\\mu\\) der Grundgesamtheit größer als der Vergleichswert \\(\\mu_0=45\\)? Beziehungsweise, ist der Unterschied statistisch signifikant, wenn wir die Streuung der Stichprobe berücksichtigen? Die Nullhypothese ist in diesem Fall, dass der Mittelwert kleiner oder gleich dem Vergleichswert ist: \\[H_0:\\mu\\leq\\mu_0\\] Die Alternativhypothese ist, dass der Mittelwert größer als der Vergleichswert ist: \\[H_1:\\mu&gt;\\mu_0\\] Wieder ist die Alternativhypothese die, die sich aus den Zahlenwerten der Stichprobe ergibt, deren Mittelwert tatsächlich größer is als \\(\\mu_0=45\\). Die Formel der Teststatistik ist die gleiche wie im links- und zweiseitigen Fall, nur dass wir jetzt gemäß der Fragestellung \\(\\mu_0=45\\) einsetzen: ts &lt;- (xbar - 45)/s*sqrt(n) ts ## [1] 3.008 Der Wert der Teststatistik ist jetzt positiv, da \\(\\bar x\\) größer ist als \\(\\mu_0\\). Er liegt ebenfalls in den Extremen der t-Verteilung, die unter der Nullhypothese zu erwarten ist, nur eben auf der rechten Seite: plot(seq(-5,5,0.01), pt(seq(-5,5,0.01), n-1), ylim=c(0,1), type=&#39;l&#39;, xlab=&#39;Z=t_s&#39;, ylab=&#39;Verteilungsfunktion&#39;) lines(c(1, 1)*ts, c(0, pt(ts, n-1)), col=&#39;blue&#39;) text(ts,-0.2,&quot;t_s&quot;, col=&quot;blue&quot;, xpd=TRUE) Der p-Wert im rechtseitigen Fall ist: \\[\\Pr\\left(Z&gt;t_s\\right)=1-F_t\\left(t_s\\right)\\] Die Wahrscheinlichkeit eines größeren Wertes als den der vorliegenden Teststatistik ist Eins minus die Verteilungsfunktion an der Stelle der Teststatistik (vgl. Kapitel 7). Mit Zahlenwerten: 1-pt(ts, n-1) ## [1] 0.001673 Dieser p-Wert ist ebenfalls kleiner als das konventionelle Signifikanzniveau von 0.01, d.h. wir lehnen diese Nullhypothese ebenfalls ab und schließen für Beispiel 3: Der Unterschied zwischen dem mit dem Stichprobenmittel \\(\\bar x\\) geschätzten Mittelwert \\(\\mu\\) der Grundgesamtheit und dem Vergleichswert \\(\\mu_0=45\\) ist statistisch signifikant. 9.2.2 Zweistichproben-t-Test 9.2.2.1 Beispiel 4 (zweiseitiger Zweistichproben-t-Test) Siehe Abbildung 9.2: Sind die mit den Stichprobenmitteln \\(\\bar x_1=50.4\\) und \\(\\bar x_2=51.2\\) geschätzten Mittelwerte \\(\\mu_1\\) und \\(\\mu_2\\) ungleich? Beziehungsweise, können wir statistisch nachvollziehen, dass die beiden Stichproben Grundgesamtheiten mit dem selben Mittelwert entstammen? Die Nullhypothese ist in diesem Fall, dass die beiden Mittelwerte gleich sind: \\[H_0:\\mu_1=\\mu_2\\] Die Alternativhypothese ist, dass die beiden Werte nicht gleich sind: \\[H_1:\\mu_1\\ne\\mu_2\\] Wir vergleichen also jetzt zwei Mittelwerte aus zwei Stichproben und nicht mehr gegen einen Vergleichswert. Die Alternativhypothese ergibt sich wiederum aus den Zahlenwerten der Stichproben, deren Mittelwerte tatsächlich ungleich sind. Die Teststatistik ist leicht anders als im einseitigen Fall, da die Differenz der beiden Mittelwerte jetzt mit beiden Standardfehlern standardisiert wird: \\[\\begin{equation} t_s=\\frac{\\bar x_1-\\bar x_2}{\\sqrt{\\frac{s_1^2}{n_1}+\\frac{s_2^2}{n_2}}}\\sim t_{n_1+n_2-2} \\tag{9.2} \\end{equation}\\] Auch in die Anzahl Freiheitsgrade der t-Verteilung, die unter der Nullhypothese zu erwarten ist, gehen beide Stichprobenumfänge ein. Setzen wir die Zahlenwerte aus den Stichproben ein (t1 und t2 stehen für die Merkmalswerte für Anreisezeit der ersten bzw. zweiten Stichprobe): # Mittelwerte # na.rm=TRUE ignoriert NAs xbar1 &lt;- mean(t1, na.rm=TRUE) xbar1 ## [1] 50.35 xbar2 &lt;- mean(t2, na.rm=TRUE) xbar2 ## [1] 51.24 # Varianzen var1 &lt;- var(t1, na.rm=TRUE) var1 ## [1] 352.2 var2 &lt;- var(t2, na.rm=TRUE) var2 ## [1] 384.4 # Stichprobenumfänge # !is.na(t1) verweist auf die Werte, die nicht NA sind n1 &lt;- length(t1[!is.na(t1)]) n1 ## [1] 48 n2 &lt;- length(t2[!is.na(t2)]) n2 ## [1] 50 # Teststatistik ts &lt;- (xbar1 - xbar2)/sqrt(var1/n1+var2/n2) ts ## [1] -0.2285 Dieser Wert der Teststatistik liegt, anders als in den vorherigen Beispielen, im Zentrum der t-Verteilung, die unter der Nullhypothese zu erwarten ist: plot(seq(-5,5,0.01), pt(seq(-5,5,0.01), n1+n2-2), ylim=c(0,1), type=&#39;l&#39;, xlab=&#39;Z=t_s&#39;, ylab=&#39;Verteilungsfunktion&#39;) lines(c(1, 1)*ts, c(0, pt(ts, n1+n2-2)), col=&#39;blue&#39;) lines(c(1, 1)*(-ts), c(0, pt(-ts, n1+n2-2)), col=&#39;blue&#39;) text(ts,-0.2,&quot;t_s&quot;, col=&quot;blue&quot;, xpd=TRUE) Der p-Wert ist, analog zum zweiseitigen Einstichproben-t-Test: \\[\\Pr\\left(Z&lt;t_s\\right)+\\Pr\\left(Z&gt;-t_s\\right)=2\\cdot\\Pr\\left(Z&gt;\\left|t_s\\right|\\right)=2\\cdot \\left(1-F_t\\left(\\left|t_s\\right|\\right)\\right)\\] Mit Zahlenwerten: 2*(1-pt(abs(ts),n1+n2-2)) ## [1] 0.8197 Der einseitige (hier linksseitige) p-Wert wäre: \\[\\Pr\\left(Z&lt;t_s\\right)=F_t\\left(t_s\\right)\\] Mit Zahlenwerten: pt(ts,n1+n2-2) ## [1] 0.4099 Also wieder halb so groß wie der zweiseitige p-Wert. Der p-Wert ist viel größer als das konventionelle Signifikanzniveau von 0.01, d.h. es ist sehr wahrscheinlich, dass dieser Wert der Teststatistik durch Zufall zustande kam falls die Nullhypothese wahr ist, d.h. wir können die Nullhypothese nicht ablehnen. Für Beispiel 4 schließen wir also: Der kleine Unterschied der mit den Stichprobenmitteln \\(\\bar x_1\\) und \\(\\bar x_2\\) geschätzten Mittelwerte \\(\\mu_1\\) und \\(\\mu_2\\) ist nicht signifikant. D.h. wir schließen, dass die beiden Stichproben Grundgesamtheiten mit dem selben Mittelwert entstammen. Tatsächlich entstammen sie der selben Grundgesamtheit. Um das zu zeigen, müssen wir noch die Varianzen testen. Das macht der F-Test in Bsp. 6. 9.2.3 Varianten der Teststatistik Die Teststatistik in Formel (9.2) ist der allgemein gültige Fall, in dem die Varianzen und Umfänge der beiden Stichproben ungleich sein können. Für die Fälle, in denen Varianzen und/oder Stichprobenumfänge gleich sind, vereinfacht sich Formel (9.2) wie folgt. Bei ungleicher Varianz und gleichem Stichprobenumfang: \\[\\begin{equation} t_s=\\frac{\\bar x_1-\\bar x_2}{\\sqrt{\\frac{s_1^2+s_2^2}{n}}}\\sim t_{2\\cdot n-2} \\tag{9.3} \\end{equation}\\] Bei gleicher Varianz und gleichem Stichprobenumfang: \\[\\begin{equation} t_s=\\frac{\\bar x_1-\\bar x_2}{\\sqrt{\\frac{2\\cdot s^2}{n}}}\\sim t_{2\\cdot n-2} \\tag{9.4} \\end{equation}\\] Wobei der Schätzer der gemeinsamen theoretischen Varianz die sogenannte gewichtete Stichprobenvarianz \\(s^2=\\frac{\\left(n_1-1\\right)\\cdot s_1^2+\\left(n_2-1\\right)\\cdot s_2^2}{n_1+n_2-2}\\) ist.15 Bei gleicher Varianz und ungleichem Stichprobenumfang ist die Teststatistik: \\[\\begin{equation} t_s=\\frac{\\bar x_1-\\bar x_2}{\\sqrt{\\left(\\frac{1}{n_1}+\\frac{1}{n_2}\\right)\\cdot s^2}}\\sim t_{n_1+n_2-2} \\tag{9.5} \\end{equation}\\] Aufgrund der unterschiedlichen Teststatistiken in Abhängigkeit der Varianzannahme muss dem Zweistichproben-t-Test ein F-Test auf Ungleichheit/Gleichheit der Varianzen vorgeschaltet werden (siehe Bsp. 6). 9.2.4 Gepaarter Zweistichproben-t-Test Gepaarte Stichproben liegen vor, wenn für die selben statistischen Einheiten zwei Merkmale aufgenommen wurden, die vergleichbar sind, z.B.: Die Anzahl Moosarten auf der Süd- und Nordseite der selben Bäume Krankheitsmerkmale von Patienten vor und nach der Behandlung Behandlung und Kontrolle im selben Block (sogenanntes Blockdesign) Was Blockdesign genau bedeutet können Sie in Dormann (2013), Kapitel 14.2.1 nachlesen. Das wird in der Biogeographie noch eine Rolle spielen. 9.2.4.1 Beispiel 5 (zweiseitiger gepaarter Zweistichproben-t-Test) Siehe Abbildung 9.3: Ist die Anzahl Moosarten auf der Nord- und Südseite derselben Bäume unterschiedlich? In diesem Beispiel liegt eine gepaarte Stichprobe vor, da die Anzahl Moosarten jeweils auf der Nord- und Südseite derselben Bäume bestimmt wurde. Deshalb wird hier getestet, ob die Differenz der Anzahl Moosarten \\(d\\) gleich oder ungleich Null ist. Die Nullhypothese ist, dass die Differenz gleich Null ist: \\[H_0:d=0\\] Die Alternativhypothese ist, dass die Differenz ungleich Null ist: \\[H_1:d\\ne0\\] Wir bilden die Differenz einfach indem wir die Merkmalswerte paarweise subtrahieren (moosdat enthält die Originaldaten): d &lt;- moosdat$n - moosdat$s # Histogramm mit Mittelwert h &lt;- hist(d, breaks = seq(-16, 16, 4), plot = FALSE) h$counts &lt;- h$counts / sum(h$counts) plot(h, freq = TRUE, col = &quot;gray&quot;, ylim = c(0,0.6), main = &quot;&quot;, xlab = &quot;Differenz Anzahl Moosarten&quot;, ylab = &quot;relative Häufigkeit&quot;) lines(c(1,1)*mean(d, na.rm=TRUE), c(0, 0.6), col=&#39;black&#39;) Aus dem Zweistichproben-t-Test ist so ein Einstichproben-t-Test geworden, mit der Teststatistik: \\[\\begin{equation} t_s=\\frac{\\hat d-0}{s}\\cdot\\sqrt{n}\\sim t_{n-1} \\tag{9.6} \\end{equation}\\] Mit Zahlenwerten: # Mittelwert dbar &lt;- mean(d) dbar ## [1] 10 # Standardabweichung s &lt;- sd(d) s ## [1] 3.162 # Stichprobenumfang n &lt;- length(d) n ## [1] 5 # Teststatistik ts &lt;- (dbar - 0)/s*sqrt(n) ts ## [1] 7.071 Dieser Wert der Teststatistik liegt wieder in den Extremen der t-Verteilung, die unter der Nullhypothese zu erwarten ist: plot(seq(-10,10,0.01), pt(seq(-10,10,0.01), n-1), ylim=c(0,1), type=&#39;l&#39;, xlab=&#39;Z=t_s&#39;, ylab=&#39;Verteilungsfunktion&#39;) lines(c(1, 1)*ts, c(0, pt(ts, n-1)), col=&#39;blue&#39;) lines(c(1, 1)*(-ts), c(0, pt(-ts, n-1)), col=&#39;blue&#39;) text(ts,-0.2,&quot;t_s&quot;, col=&quot;blue&quot;, xpd=TRUE) Der p-Wert ist, wie in jedem zweiseitigem Fall: \\[\\Pr\\left(Z&lt;t_s\\right)+\\Pr\\left(Z&gt;-t_s\\right)=2\\cdot\\Pr\\left(Z&gt;\\left|t_s\\right|\\right)=2\\cdot \\left(1-F_t\\left(\\left|t_s\\right|\\right)\\right)\\] Mit Zahlenwerten: 2*(1-pt(abs(ts),n-1)) ## [1] 0.002111 Der einseitige (hier rechtsseitige) p-Wert wäre: \\[\\Pr\\left(Z&gt;t_s\\right)=1-F_t\\left(t_s\\right)\\] Mit Zahlenwerten: 1-pt(ts,n-1) ## [1] 0.001055 Da der p-Wert kleiner ist als das konventionelle Signifikanzniveau von 0.01 ist lehnen wir die Nullhypothese ab und schließen für Beispiel 5: Auf der Nordseite der Bäume wachsen signifikant mehr Moosarten als auf der Südseite. 9.3 Interpretation des p-Wertes An dieser Stelle ein paar Worte zur Interpretation des p-Wertes. Der p-Wert und das Signifikanzniveau (hier 0.01) hängen wie gesagt zusammen: Ist der p-Wert kleiner oder gleich 0.01 wird die Nullhypothese abgelehnt; ist der p-Wert größer als 0.01 wird die Nullhypothese bis auf weiteres beibehalten. Andere Signifikanzniveaus sind üblich (0.001, 0.05 etc.) und R gibt wie gesagt immer mehrere an. Aber was sagt ein p-Wert von 0.01 nun genau aus? Ein p-Wert von 0.01 sagt aus, dass wir bei hypothetisch wiederholter Stichprobenziehung des selben Umfangs aus der selben Grundgesamtheit in 1% der Fälle die Nullhypothese ablehnen würden obwohl sie wahr ist - ein sogenannter Fehler 1. Art. In den Worten des Wissenschaftsphilosophen Ian Hacking (2001): Entweder ist die Nullhypothese wahr und etwas ungewöhnliches ist per Zufall geschehen (Wahrscheinlichkeit 1%), oder die Nullhypothese ist falsch. Der p-Wert ist also keine Wahrscheinlichkeit, dass die Nullhypothese wahr ist! 9.4 F-Test (Vergleich von Varianzen) Während der t-Test für den Vergleich von Mittelwerten zuständig ist, dient der F-Test dem Vergleich von Varianzen. Schauen wir uns das an einem Beispiel an. 9.4.1 Beispiel 6 (rechtsseitiger F-test) Siehe Abbildung 9.2: Ist die Varianz \\(\\sigma_2^2\\) (gegeben \\(s_2^2=384\\)) größer als die Varianz \\(\\sigma_1^2\\) (gegeben \\(s_1^2=352\\))? Beziehungsweise, können wir statistisch nachvollziehen, dass die beiden Stichproben Grundgesamtheiten mit derselben Varianz entstammen? Die Nullhypothese ist in diesem Fall, dass die beiden Varianzen gleich sind: \\[H_0:\\sigma_2^2=\\sigma_1^2\\] Die Alternativhypothese ist, dass \\(\\sigma_2^2\\) größer ist als \\(\\sigma_1^2\\): \\[H_1:\\sigma_2^2&gt;\\sigma_1^2\\] Die Alternativhypothese ergibt sich wie beim t-Test aus den Zahlenwerten der Stichproben, wo \\(s_2^2\\) tatsächlich größer ist als \\(s_1^2\\). Die Formulierung ist immer die eines rechtsseitigen Tests, wo die größere Varianz links steht.16 Die Teststatistik \\(F_s\\) ist: \\[\\begin{equation} F_s=\\frac{s_2^2}{s_1^2}\\sim F_{n_1-1;n_2-1} \\tag{9.7} \\end{equation}\\] Die größere Stichprobenvarianz steht für den rechtsseitigen Test immer im Zähler.17 Sind die beiden Stichprobenvarianzen annähernd gleich dann ist \\(F_s\\) annähernd \\(1\\). Diese Teststatistik folgt bei wiederholtem Stichprobenziehen, falls die Nullhypothese wahr ist, einer sogenannten F-Verteilung mit den Parameterwerten \\(n_1-1\\) und \\(n_2-1\\) (auch hier genannt Freiheitsgrade, wobei es im Gegensatz zur t-Verteilung zwei gibt).18 Abbildung 9.5 zeigt die F-Verteilung für verschiedene Kombinationen der beiden Freiheitsgrade. Die F-Verteilung geht von \\(0\\) bis \\(+\\infty\\), d.h. ist nur für positive Werte definiert. Das passt für Varianzen, da diese nie negativ sind. Die F-Verteilung wird schmaler mit zunehmender Anzahl Freiheitsgrade, d.h. mit zunehmenden Stichprobengrößen \\(n_1-1\\) und \\(n_2-1\\) im Falle des F-Tests. Abbildung 9.5: Links: Dichtefunktion der F-Verteilung einer beliebigen Zufallsvariablen \\(Z\\) für verschiedene Kombinationen der beiden Freiheitsgrade (die beiden Parameter der F-Verteilung heißen Freiheitsgrade). Rechts: Verteilungsfunktion der entsprechenden F-Verteilungsvarianten. Setzen wir nun die Zahlenwerten der Stichproben in Formel (9.7) ein: # Varianzen var1 &lt;- var(t1, na.rm=TRUE) var1 ## [1] 352.2 var2 &lt;- var(t2, na.rm=TRUE) var2 ## [1] 384.4 # Stichprobenumfänge n1 &lt;- length(t1[!is.na(t1)]) n1 ## [1] 48 n2 &lt;- length(t2[!is.na(t2)]) n2 ## [1] 50 # Teststatistik fs &lt;- var2 / var1 fs ## [1] 1.092 Dieser Wert der Teststatistik liegt im Zentrum der F-Verteilung, die unter der Nullhypothese zu erwarten ist: plot(seq(0,5,0.01), pf(seq(0,5,0.01), n1-1, n2-1), ylim=c(0,1), type=&#39;l&#39;, xlab=&#39;Z=F_s&#39;, ylab=&#39;Verteilungsfunktion&#39;) lines(c(1, 1)*fs, c(0, pf(fs, n1-1, n2-1)), col=&#39;blue&#39;) text(fs,-0.2,&quot;F_s&quot;, col=&quot;blue&quot;, xpd=TRUE) Der p-Wert ist (vgl. rechtsseitiger t-Test): \\[\\Pr\\left(Z&gt;F_s\\right)=1-F_F\\left(F_s\\right)\\] Mit Zahlenwerten: 1-pf(fs, n1-1, n2-1) ## [1] 0.3806 Da der p-Wert viel größer ist als das konventionelle Signifikanzniveau von 0.01, ist sehr wahrscheinlich, dass dieser Wert der Teststatistik durch Zufall zustande kam falls die Nullhypothese wahr ist. Wir können demnach die Nullhypothese nicht ablehnen und schließen für Beispiel 6: Der Unterschied der beiden Varianzen ist statistisch nicht signifikant. D.h. wir schließen, dass die beiden Stichproben Grundgesamtheiten mit der selben Varianz entstammen. Zusammen mit Bsp. 4 habe wir so statistisch nachvollzogen, dass die beiden Stichproben der selben Grundgesamtheit entstammen. 9.5 Verteilungstest (Kolmogorow-Smirnow-Test) In Kapitel 8 haben wir den QQ-Plot als visuelles Maß der Übereinstimmung einer Stichprobe mit einer theoretischen Verteilungsannahme kennengelernt. Hier soll nun diese Frage als Test formalisiert werden. 9.5.1 Beispiel 7 (Einstichproben-Kolmogorow-Smirnow-Test) Siehe Abbildung 9.1: Entstammt die Stichprobe der Reisezeit einer normalverteilten Grundgesamtheit? Die Parameter dieser Normalverteilung werden anhand der Stichprobe geschätzt. Die Nullhypothese ist in diesem Fall, dass die Verteilungsfunktion der Zufallsvariable - nennen wir sie \\(X\\) - gleich einer bestimmten Verteilungsfunktion \\(F_0\\) - hier einer Normalverteilung - ist: \\[H_0:F_X(x)=F_0(x)\\] Die Alternativhypothese ist, dass die Verteilungsfunktion von \\(X\\) ungleich \\(F_0\\) ist: \\[H_0:F_X(x)\\ne F_0(x)\\] In diesem Beispiel testen wir auf eine Normalverteilung, grundsätzlich können wir aber auf jede andere Verteilung testen. Die Teststatistik - hier genannt \\(d_n\\) - ist im Falle des KS-Testes der maximale absolute Abstand zwischen empirischer und theoretischer Verteilungsfunktion: \\[\\begin{equation} d_n=\\sup\\left|F_X(x)-F_0(x)\\right| \\tag{9.8} \\end{equation}\\] Die Bezeichung \\(\\sup\\) steht für Supremum, also Maximum; sie bezeichnet hier den größten Wert aller vorliegenden absoluten Abstände \\(\\left|F_X(x)-F_0(x)\\right|\\) (Abbildung 9.6, links). Abbildung 9.6: Links: Empirische (schwarz) und theoretische (rot) Verteilungsfunktion des Merkmals Reisezeit vom WiSe 2019/20. Für die theoretische Verteilungsfunktion wurde eine Normalverteilung angenommen, deren Parameter anhand der Stichprobe geschätzt wurden. Der maximale absolute Abstand der beiden Verteilungsfunktionen, die Teststatistik \\(d_n\\) des KS-Testes, ist in blau markiert. Rechts: Histogramm derselben Daten mit geschätzter Dichtefunktion. Diese Teststatistik folgt bei wiederholtem Stichprobenziehen, falls die Nullhypothese wahr ist, einer sogenannten Kolmogorow-Verteilung. Diese ist in R nicht implementiert und generell aufwendig zu berechnen. Deshalb zeigen wir hier keine Darstellung, sondern benutzen sofort den in R implementierten KS-Test. Das Prinzip ist aber dasselbe wie bei t- und F-Test und allen anderen Tests. Die Teststatistik können wir weiterhin per Hand ausrechnen: # Mittelwertschätzer xbar &lt;- mean(reisedat19$t, na.rm=TRUE) # Schätzer der Standardabweichung s &lt;- sd(reisedat19$t, na.rm=TRUE) # Werte der empirischen und geschätzten theoretischen Verteilungsfunktionen # für jeden Merkmalswert der Stichprobe # Merkmalswerte sortieren t_sort &lt;- sort(reisedat19$t) # empirische Verteilungsfunktion cdf_e &lt;- seq(1/length(t_sort), 1, 1/length(t_sort)) # theoretische Verteilungsfunktion cdf_t &lt;- pnorm(t_sort, mean=xbar, sd=s) # absolute Differenzen diff &lt;- abs(cdf_e-cdf_t) # maximale Differenz = Teststatistik dn dn &lt;- max(diff) dn ## [1] 0.09932 # KS-Test ks.test(reisedat19$t, &quot;pnorm&quot;, xbar, s) ## Warning in ks.test(reisedat19$t, &quot;pnorm&quot;, xbar, s): ## ties should not be present for the Kolmogorov-Smirnov ## test ## ## One-sample Kolmogorov-Smirnov test ## ## data: reisedat19$t ## D = 0.099, p-value = 0.3 ## alternative hypothesis: two-sided Der Wert der Teststatistik \\(d_n\\) ist 0.1; wir haben ihn per Hand ausgerechnet also auch als Ausgabe D des KS-Testes erhalten. Die Warnung bezüglich der gleichen Stichprobenwerte (ties) ist unvermeidlich bei realen Stichproben und für unsere Zwecke zu vernachlässigen. Da der p-Wert viel größer ist als das konventionelle Signifikanzniveau von 0.01, ist sehr wahrscheinlich, dass dieser Wert der Teststatistik durch Zufall zustande kam falls die Nullhypothese wahr ist. Wir können demnach die Nullhypothese nicht ablehnen und schließen für Beispiel 7: Die Verteilungen sind statistisch gleich. D.h. wir können nicht ausschließen, dass die Stichprobe der Reisezeit einer normalverteilten Grundgesamtheit entstammt, obwohl die empirische Verteilung steiler aussieht. 9.5.2 Beispiel 8 (Zweistichproben-Kolmogorow-Smirnow-Test) Beim Zweistichproben-KS-Test werden zwei empirische Verteilungen miteinander verglichen. Siehe Abbildung 9.2: Entstammen die beiden Teilstichproben der Reisezeit einer gemeinsamen Verteilung? Beziehungsweise, können wir das statistisch nachvollziehen? Die Nullhypothese ist in diesem Fall, dass die Verteilungsfunktionen der beiden Zufallsvariablen - nennen wir sie \\(X_1\\) und \\(X_2\\) - gleich sind: \\[H_0:F_{X_1}(x)=F_{X_2}(x)\\] Die Alternativhypothese ist, dass die beiden Verteilungsfunktionen ungleich sind: \\[H_1:F_{X_1}(x)\\ne F_{X_2}(x)\\] Die Teststatistik \\(d_n\\) ist wieder ein maximaler absoluter Abstand, diesmal zwischen den beiden empirischen Verteilungsfunktionen (Abbildung 9.7): \\[\\begin{equation} d_n=\\sup\\left|F_{X_1}(x)-F_{X_2}(x)\\right| \\tag{9.9} \\end{equation}\\] Abbildung 9.7: Empirische Verteilungsfunktionen der beiden Teilstichproben des Merkmals Reisezeit vom WiSe 2019//20. Die Teststatistik folgt bei wiederholtem Stichprobenziehen, falls die Nullhypothese wahr ist, wiederum einer Kolmogorow-Verteilung. Wir benutzen direkt den in R implementierten KS-Test: # KS-Test ks.test(t1, t2) ## Warning in ks.test(t1, t2): cannot compute exact p- ## value with ties ## ## Two-sample Kolmogorov-Smirnov test ## ## data: t1 and t2 ## D = 0.074, p-value = 1 ## alternative hypothesis: two-sided Der Wert der Teststatistik \\(d_n\\) ist wieder etwa 0.1; diesmal können wir ihn wegen der unterschiedlichen Stichprobengröße aufgrund von NA Werten nicht so einfach per Hand ausrechnen und sparen uns das hier. Da der p-Wert wiederum viel größer ist als das konventionelle Signifikanzniveau von 0.01, können wir die Nullhypothese nicht ablehnen und schließen für Beispiel 8: Die beiden Teilstichproben entstammen einer gemeinsamen Verteilungen. Bzw., wir können diesen Tatbestand statistisch nachvollziehen. In der Tat wurden die beiden Teilstichproben ja der gleichen Stichprobe zufällig entnommen 9.6 Unabhängigkeitstest (Chi-Quadrat-Test) Der Chi-Quadrat-Test testet die Unabhängigkeit zweier nominal skalierter Merkmale mit Hilfe der Chi-Quadrat-Statistik \\(\\mathcal{X}^2\\), die wir bereits in Kapitel 5 anhand der Kontingenztabelle kennengelernt haben. Erinnern wir uns, dass \\(\\mathcal{X}^2\\) die Stärke des Zusammenhangs der Merkmale misst, ohne aber eine Aussage über Signifkanz zu treffen. 9.6.1 Beispiel 9 (Chi-Quadrat-Test) Siehe Kapitel 5: Gibt es einen Zusammenhang zwischen Bezirk und Votum beim Volksentscheid Tegel? Beziehungsweise, ist der geringe Zusammenhang, den wir bereits festgestellt haben, statistisch signifikant? Die Nullhypothese ist in diesem Fall, dass die Merkmale Bezirk \\(X\\) und Votum \\(Y\\) unabhängig sind. Die Alternativhypothese ist, dass die beiden Merkmale abhängig sind. In Kapitel 5 vermuteten wir, da \\(\\mathcal{X}^2=49895.1\\) klein ist im Vergleich zum maximal möglichen Wert von \\(1 732 940\\), dass die Abhängigkeit zwischen Bezirk und Votum gering ist. Aber ist sie trotzdem statistisch signifikant? Schauen Sie sich nochmal die Kontingenztabelle in Abbildung 5.5 an: In der linken Tabelle stehen die beobachteten absoluten Häufigkeiten \\(h_{ij}\\) für alle Kombinationen der beiden Merkmalsausprägungen. In der rechten Tabelle stehen die entsprechenden bei empirischer Unabhängigkeit erwarteten absoluten Häufigkeiten \\(\\tilde h_{ij}\\). Zu letzteren Werten kamen wir mit der Formel: \\[h_{ij}=\\frac{h_{i\\cdot}\\cdot h_{\\cdot j}}{n}:=\\tilde h_{ij}\\] Wobei \\(h_{i\\cdot}\\) und \\(h_{\\cdot j}\\) die absoluten Häufigkeiten der Randverteilungen von \\(X\\) und \\(Y\\) sind und \\(n\\) wie üblich der Stichprobenumfang ist (Abbildung 9.8). Abbildung 9.8: Kontingenztabelle zum Abstimmungsverhalten im Volksentscheid Tegel aus Kapitel 5 mit Legende. Seit Kapitel 6 haben wir jetzt auch das Handwerkszeug, diese Gleichung besser zu verstehen. Die Wahrscheinlichkeit, dass \\(X\\) und \\(Y\\) beide eintreten ist das Produkt der Einzelwahrscheinlichkeiten, falls \\(X\\) und \\(Y\\) unabhängig sind: \\[\\Pr(X\\cap Y)=\\Pr(X)\\cdot\\Pr(Y)\\] Das ist die Produktregel (Geichung (6.2)) für unabhängige Ereignisse. Übertragen auf die relativen Häufigkeiten der Kontingenztabelle heißt das: \\[\\frac{h_{ij}}{n}=\\frac{h_{i\\cdot}}{n}\\cdot\\frac{h_{\\cdot j}}{n}\\] Nach \\(h_{ij}\\) aufgelöst kommen wir auf: \\[h_{ij}=\\frac{h_{i\\cdot}\\cdot h_{\\cdot j}}{n}:=\\tilde h_{ij}\\] Die Chi-Quadrat-Statistik, die wir in Kapitel 5 ausgerechnet haben ist die Teststatistik des Chi-Quadrat-Testes: \\[\\begin{equation} \\mathcal{X}^2=\\sum_{i=1}^{k}\\sum_{j=1}^{m}\\frac{\\left(h_{ij}-\\tilde h_{ij}\\right)^2}{\\tilde h_{ij}} \\tag{9.10} \\end{equation}\\] In Worten: Die Summe der relativen quadratischen Abweichungen der beobachteten Häufigkeiten \\(h_{ij}\\) von den bei Unabhängigkeit erwarteten Häufigkeiten \\(\\tilde h_{ij}\\). In unserem Beispiel ist \\(\\mathcal{X}^2=49895.1\\). Diese Teststatistik folgt bei wiederholtem Stichprobenziehen, falls die Nullhypothese wahr ist, einer Chi-Quadrat-Verteilung mit Parameterwert \\((k-1)\\cdot(m-1)\\) (auch hier genannt Anzahl Freiheitsgrade).19 Abbildung 9.9 zeigt die Chi-Quadrat-Verteilung für verschiedene Anzahl Freiheitsgrade. Die Chi-Quadrat-Verteilung ist wie die F-Verteilung nur für positive Werte definiert. Das passt für \\(\\mathcal{X}^2\\), das nie negativ ist. Die Chi-Quadrat-Verteilung wird breiter mit zunehmender Anzahl Freiheitsgrade, d.h. mit zunehmender Anzahl Ausprägungen der beiden Merkmale. Das trägt dem mit zunehmender Anzahl Merkmalsausprägungen größer werdenden Maximalwert von \\(\\mathcal{X}^2\\) Rechnung. Abbildung 9.9: Links: Dichtefunktion der Chi-Quadrat-Verteilung einer beliebigen Zufallsvariablen \\(Z\\) für verschiedene Anzahl Freiheitsgrade (der einzige Parameter der Chi-Quadrat-Verteilung heißt Anzahl Freiheitsgrade). Rechts: Verteilungsfunktion der entsprechenden Chi-Quadrat-Verteilungsvarianten. Der Wert \\(\\mathcal{X}^2=49895.1\\) für unser Beispiel befindet sich am rechten Rand der Chi-Quadrat-Verteilung, weit jenseits des hier dargestellten Bereichs: plot(seq(0,100,1), pchisq(seq(0,100,1), (12-1)*(2-1)), ylim=c(0,1), type=&#39;l&#39;, xlab=&#39;Z=Chi2&#39;, ylab=&#39;Verteilungsfunktion&#39;) Der p-Wert ist damit viel kleiner als das konventionelle Signifikanzniveau von 0.01 (R rundet den Wert auf null): 1-pchisq(49895.1, (12-1)*(2-1)) ## [1] 0 Somit ist es sehr unwahrscheinlich, dass dieser Wert der Teststatistik durch Zufall zustande kam falls die Nullhypothese wahr ist. Wir lehnen demnach die Nullhypothese ab und schließen für Beispiel 9: Der geringe Abhängigkeit zwischen Bezirk und Votum ist tatsächlich statistisch signifikant. Literatur "],["10-regression.html", "Kapitel 10 Lineare Regression 10.1 Definitionen 10.2 Beschreibung vs. Vorhersage 10.3 Ausblick: Weiterführende lineare Modelle 10.4 Lineare Regression 10.5 Signifikanz der Regression 10.6 Konfidenzintervalle und Signifikanz der Parameter 10.7 Güte der Modellanpassung", " Kapitel 10 Lineare Regression Für die lineare Regression kehren wir zu einer Frage aus Kapitel 5 zurück: Kann man die Entfernung zu Ihrem Wohnort mit der Anzahl Stationen, die Sie bis Adlershof brauchen statistisch vorhersagen? plot(reisedat$stationen, reisedat$distanz, xlab=&quot;Anzahl Stationen&quot;, ylab=&quot;Entfernung (km)&quot;) Wir erinnern uns, dass der Korrelationskoeffizient nach Bravais-Pearson aus Kapitel 5 0.81 war. Das Ziel ist nun, eine Gerade durch die Punktwolke zu legen, die den Trend beschreibt, so dass der Abstand der Punkte von der Geraden minimal ist. Es geht um 2 Variablen (Merkmale): die abhängige Variable \\(y\\) (im Bsp. Entfernung) die unabhängige Variable \\(x\\) (im Bsp. Anzahl Stationen) Die Variablen müssen metrisch skaliert sein.20 Wir wollen das generelle Verhalten von \\(y\\) mit \\(x\\) beschreiben. Eine Gerade stellt dabei das einfachste lineare Modell dar. 10.1 Definitionen Im Falle einer einzigen unabhängigen Variable lautet die Gleichung des linearen Models: \\[\\begin{equation} y_i = \\beta_0 + \\beta_1 \\cdot x_i + \\epsilon_i \\quad \\text{mit} \\quad i=1,2,\\ldots,n \\tag{10.1} \\end{equation}\\] \\(y_i\\) bezeichnet den Wert der abhängigen Variable für Datenpunkt \\(i\\), und \\(x_i\\) den Wert der unabhängigen Variable für Datenpunkt \\(i\\). Der Parameter \\(\\beta_0\\) beschreibt den Achsenabschnitt der Geraden, also der Punkt an dem die Gerade die y-Achse schneidet. Der Parameter \\(\\beta_1\\) beschreibt die Steigung der Geraden. \\(\\epsilon_i\\) stellt das Residuum (also den Fehler) für Datenpunkt \\(i\\) dar (Abbildung 10.1). Abbildung 10.1: Lineare Regression: Definitionen. 10.2 Beschreibung vs. Vorhersage Der primäre Zweck einer Regressionsanalyse ist die Beschreibung (oder Erklärung) der Daten im Sinne einer allgemeinen Beziehung, die sich auf die Grundgesamtheit übertragen lässt, aus der diese Daten entnommen wurden. Da diese Beziehung eine Eigenschaft der Grundgesamtheit ist, sollte sie auch Vorhersagen ermöglichen. Hierbei ist jedoch Vorsicht geboten. Betrachten Sie den Zusammenhang von Jahr und Weltrekordzeit für die in Abbildung 10.2 dargestellten Daten (Meile, Herren). Wenn, wie hier, die Zeit die unabhängige Variable ist, wird die Regression zu einer Form der Trendanalyse, die in diesem Fall eine Verringerung der Rekordzeit mit den Jahren anzeigt. (Die lm() Funktion und ihren Output werden wir weiter unten kennenlernen, hier geht es um die Grafiken.) # Daten laden mile &lt;- read.csv(&quot;https://raw.githubusercontent.com/avehtari/ROS-Examples/master/Mile/data/mile.csv&quot;, header=TRUE) # lineares Modell an Daten aus 1. Hälfte des 20. Jahrh. anpassen mile_fit1 &lt;- lm(seconds ~ year, data = mile[mile$year&lt;1950,]) # Informationen zu Parameterschätzern extrahieren coef(summary(mile_fit1)) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 912.2340 67.90140 13.435 3.615e-08 ## year -0.3439 0.03509 -9.798 9.059e-07 # lineares Modell an kompletten Datensatz anpassen mile_fit2 &lt;- lm(seconds ~ year, data = mile) coef(summary(mile_fit2)) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1006.876 21.532 46.76 1.361e-29 ## year -0.393 0.011 -35.73 3.780e-26 # Modellanpassung für 1. Hälfte des 20. Jahrh. plotten plot(mile$year[mile$year&lt;1950], mile$seconds[mile$year&lt;1950], xlim = c(1900, 2000), ylim = c(200, 260), pch = 19, type = &#39;p&#39;, xlab = &quot;Jahr&quot;, ylab = &quot;Weltrekord, Meile, Herren (Sekunden)&quot;) abline(coef(mile_fit1), lwd = 3, col = &quot;red&quot;) # Extrapolation für 2. Hälfte des 20. Jahrh. plotten plot(mile$year, mile$seconds, xlim = c(1900, 2000), ylim = c(200, 260), pch = 19, type = &#39;p&#39;, xlab = &quot;Jahr&quot;, ylab = &quot;Weltrekord, Meile, Herren (Sekunden)&quot;) abline(coef(mile_fit1), lwd = 3, col = &quot;red&quot;) # Modellanpassung für Gesamtdaten bis 2050 plotten plot(mile$year, mile$seconds, xlim = c(1900, 2050), ylim = c(200, 260), pch = 19, type = &#39;p&#39;, xlab = &quot;Jahr&quot;, ylab = &quot;Weltrekord, Meile, Herren (Sekunden)&quot;) abline(coef(mile_fit2), lwd = 3, col = &quot;red&quot;) Abbildung 10.2: Links: Trend des Weltrekords Meile, Herren in der ersten Hälfte des 20. Jahrhunderts (Beschreibung). Mitte: Extrapolation des Trends für die zweite Hälfte des 20. Jahrhunderts (Vorhersage). Rechts: Extrapolation des Trends bis zum Jahr 2050 (längere Vorhersage). Nach: Wainer (2009) Wir sehen, dass sich der Weltrekord in der ersten Hälfte des 20. Jahrhunderts linear verbesserte (Abbildung 10.2, links). Dieser Trend passt auch für die zweite Hälfte des 20. Jahrhunderts bemerkenswert gut (Abbildung 10.2, Mitte). Wie lange kann sich der Weltrekord jedoch noch mit der gleichen Rate verbessern (Abbildung 10.2, rechts)? Dieses Beispiel zeigt deutlich die Anwendbarkeit von Regressionen für Vorhersagen innerhalb bestimmter Grenzen, zeigt jedoch gleichzeitig die Grenzen dieser einfachen Modelle für längere Vorhersagen (z.B. in Zeit und Raum). Im Falle des Weltrekords würden wir erwarten, dass die Verbesserungsrate mit der Zeit abnimmt, d.h. dass die Kurve abflacht, was ein nichtlineares Modell erfordert. 10.3 Ausblick: Weiterführende lineare Modelle Wenn wir über das lineare Modell sprechen, ist die abhängige Variable immer metrisch skaliert, während die unabhängigen Variablen metrisch, nominal/ordinal oder gemischt sein können. Im Prinzip kann jede dieser Varianten mathematisch gleich behandelt werden, d.h. alle können z.B. mit der lm() Funktion in R analysiert werden. Allerdings haben sich historisch gesehen unterschiedliche Bezeichnungen für diese Varianten etabliert, die hier erwähnt werden sollen, um Verwirrung zu vermeiden (Tabellen 10.1 und 10.2). Tabelle 10.1: Historische Namen für die Varianten des linearen Modells, je nachdem, ob die unabhängigen Variablen metrisch, nominal/ordinal oder gemischt sind. Die abhängige Variable ist immer metrisch skaliert. unabhängige Variable(n)metrisch unabhängige Variable(n)nominal/ordinal unabhängige Variable(n)gemischt Regression Varianzanalyse(ANOVA) Kovarianzanalyse(ANCOVA) Tabelle 10.2: Historische Namen für Regression, je nachdem, ob wir eine oder mehrere unabhängige Variablen und eine oder mehrere abhängige Variablen haben. 1 unabhängige Variable &gt;1 unabhängige Variable 1 abhängige Variable Regression Multiple Regression &gt;1 abhängige Variable Multivariate Regression Multivariate multiple Regression 10.4 Lineare Regression Wie soll nun die Gerade durch die Punktwolke gelegt werden, d.h. welche Werte sollen Achsenabschnitt \\(\\beta_0\\) und Steigung \\(\\beta_1\\) annehmen? Typischerweise werden Regressionsprobleme gelöst, indem die Summe der quadratischen Abweichungen zwischen der Regressionsgeraden und den Datenpunkten minimiert wird - die sogenannte Kleinste-Quadrate-Schätzung. Die Summe der quadratischen Abweichungen wird auch als \\(SSE\\) bezeichnet (Sum of Squared Errors). Grafisch gesehen probieren wir in Abbildung 10.1 verschiedene Geraden mit unterschiedlichen Achsenabschnitten \\(\\beta_0\\) und Steigungen \\(\\beta_1\\) aus und wählen diejenige, bei der die Summe aller vertikalen Abstände \\(\\epsilon_i\\) zum Quadrat am kleinsten ist. Mathematisch ist \\(SSE\\) definiert als: \\[\\begin{equation} SSE=\\sum_{i=1}^{n}\\left(\\epsilon_i\\right)^2=\\sum_{i=1}^{n}\\left(y_i-\\left(\\beta_0+\\beta_1 \\cdot x_i\\right)\\right)^2 \\tag{10.2} \\end{equation}\\] Das Residuum \\(\\epsilon_i\\) ist also gleich \\(y_i-\\left(\\beta_0+\\beta_1 \\cdot x_i\\right)\\), dem vertikalen Abstand zwischen Datenpunkt und Regressionsgerade. Im Fall der linearen Regression kann \\(SSE\\) analytisch minimiert werden, was z.B. bei nichtlinearen Modellen nicht der Fall ist. Analytisch finden wir das Minimum von \\(SSE\\) wo dessen partielle Ableitungen in Bezug auf die beiden Modellparameter beide Null sind: \\(\\frac{\\partial SSE}{\\partial \\beta_0}=0\\) und \\(\\frac{\\partial SSE}{\\partial \\beta_1}=0\\). Unter Anwendung der Definition von \\(SEE\\) aus Gleichung (10.2) und der Summenregel21 und der Kettenregel22, die Sie noch aus der Schule kennen werden, erhalten wir: \\[\\begin{equation} \\frac{\\partial SSE}{\\partial \\beta_0}=-2 \\cdot \\sum_{i=1}^{n}\\left(y_i-\\beta_0-\\beta_1 \\cdot x_i\\right)=0 \\tag{10.3} \\end{equation}\\] \\[\\begin{equation} \\frac{\\partial SSE}{\\partial \\beta_1}=-2 \\cdot \\sum_{i=1}^{n}x_i \\cdot \\left(y_i-\\beta_0-\\beta_1 \\cdot x_i\\right)=0 \\tag{10.4} \\end{equation}\\] Gleichungen (10.3) und (10.4) bilden ein Gleichungssystem mit zwei Gleichungen und zwei Unbekannten, das wir eindeutig lösen können. Zuerst lösen wir Gleichung (10.3) nach \\(\\beta_0\\) auf (nachdem wir durch -2 geteilt haben): \\[\\begin{equation} \\sum_{i=1}^{n}y_i-n \\cdot \\beta_0-\\beta_1 \\cdot \\sum_{i=1}^{n}x_i=0 \\tag{10.5} \\end{equation}\\] \\[\\begin{equation} n \\cdot \\beta_0=\\sum_{i=1}^{n}y_i-\\beta_1 \\cdot \\sum_{i=1}^{n}x_i \\tag{10.6} \\end{equation}\\] \\[\\begin{equation} \\beta_0=\\bar{y}-\\beta_1 \\cdot \\bar{x} \\tag{10.7} \\end{equation}\\] Formal sind das jetzt Parameterschätzer (das Dach-Symbol bezeichnet Schätzer): \\[\\begin{equation} \\hat\\beta_0=\\bar{y}-\\hat\\beta_1 \\cdot \\bar{x} \\tag{10.8} \\end{equation}\\] Sodann setzen wir Gleichung (10.8) in Gleichung (10.4) ein (nachdem wir durch -2 geteilt haben): \\[\\begin{equation} \\sum_{i=1}^{n}\\left(x_i \\cdot y_i-\\beta_0 \\cdot x_i-\\beta_1 \\cdot x_i^2\\right)=0 \\tag{10.9} \\end{equation}\\] \\[\\begin{equation} \\sum_{i=1}^{n}\\left(x_i \\cdot y_i-\\bar{y} \\cdot x_i+\\hat\\beta_1 \\cdot \\bar{x} \\cdot x_i-\\hat\\beta_1 \\cdot x_i^2\\right)=0 \\tag{10.10} \\end{equation}\\] Schließlich lösen wir Gleichung (10.10) nach \\(\\beta_1\\) auf: \\[\\begin{equation} \\sum_{i=1}^{n}\\left(x_i \\cdot y_i-\\bar{y} \\cdot x_i\\right)-\\hat\\beta_1 \\cdot \\sum_{i=1}^{n}\\left(x_i^2-\\bar{x} \\cdot x_i\\right)=0 \\tag{10.11} \\end{equation}\\] \\[\\begin{equation} \\hat\\beta_1=\\frac{\\sum_{i=1}^{n}\\left(x_i \\cdot y_i-\\bar{y} \\cdot x_i\\right)}{\\sum_{i=1}^{n}\\left(x_i^2-\\bar{x} \\cdot x_i\\right)} \\tag{10.12} \\end{equation}\\] Über eine Reihe von Schritten, die ich hier überspringe, erhalten wir: \\[\\begin{equation} \\hat\\beta_1=\\frac{SSXY}{SSX} \\tag{10.13} \\end{equation}\\] \\(SSX=\\sum_{i=1}^{n}\\left(x_i-\\bar{x}\\right)^2\\) ist ein Maß für die Varianz der Daten in \\(x\\)-Richtung. \\(SSXY=\\sum_{i=1}^{n}\\left(x_i-\\bar{x}\\right) \\cdot \\left(y_i-\\bar{y}\\right)\\) ist ein Maß für die Kovarianz der Daten. Es gibt auch \\(SSY=\\sum_{i=1}^{n}\\left(y_i-\\bar{y}\\right)^2\\), das entsprechend ein Maß für die Varianz der Daten in \\(y\\)-Richtung ist. Gleichung (10.13) ist eine exakte Lösung für \\(\\hat\\beta_1\\). Wir setzen nun Gleichung (10.13) in Gleichung (10.8) ein und haben eine exakte Lösung für \\(\\hat\\beta_0\\). Berechnen wir nun die Parameter für unsere Reisedaten mit der lm() Funktion: # lineare Regression der Reisedaten reise_fit &lt;- lm(distanz ~ stationen, data = reisedat) # Informationen über geschätzte Parameterwerte ausgeben coef(summary(reise_fit)) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.9232 1.01874 4.833 7.533e-06 ## stationen 0.6934 0.05976 11.604 4.641e-18 In der ersten Spalte (Estimate) dieses Outputs finden Sie die Werte der Parameterschätzer, wobei (Intercept) für \\(\\beta_0\\) steht und stationen für \\(\\beta_1\\). Anhand von \\(\\beta_1\\) können wir ablesen, dass die Entfernung zwischen zwei Stationen im Mittel 0.7 km beträgt. Der Achsenabschnitt \\(\\beta_0\\) hat keine direkte Entsprechung.23 Auf die anderen Spalten werden wir weiter unten zu sprechen kommen. Plotten wir nun die so ermittelte Regressionsgerade \\(y_i=4.9+0.7\\cdot x_i+\\epsilon_i\\): # Modellanpassung plotten plot(reisedat$stationen, reisedat$distanz, pch = 19, type = &#39;p&#39;, xlab = &quot;Anzahl Stationen&quot;, ylab = &quot;Entfernung (km)&quot;) abline(coef(reise_fit), lwd = 3, col = &quot;red&quot;) 10.5 Signifikanz der Regression Nun, da wir Werte für die Regressionsparameter haben, müssen wir uns fragen, ob diese Werte statistisch signifikant sind oder ob sie durch Zufall aus dem (angenommenen) Zufallsprozess der Stichprobenziehung entstanden sein könnten. Dazu testen wir formal, ob die vom Modell erklärte Varianz in den Daten signifikant größer als die nicht erklärte Varianz ist. Das ist ein F-Test-Problem (vgl. Kapitel 9.4), das wir über die sogenannte Varianzanalyse (ANOVA) angehen. ANOVA beginnt mit der Erstellung der ANOVA-Tabelle (Tabelle 10.3). Dies geschieht in R im Hintergrund und wird selten explizit betrachtet; tun wir es hier aber trotzdem, damit wir verstehen was passiert. Tabelle 10.3: ANOVA-Tabelle der linearen Regression. Varianz-quelle Quadrat-summe Freiheits-grad (\\(df\\)) Varianz F-Statistik (\\(F_s\\)) p-Wert Regression \\(SSR=\\\\SSY-SSE\\) \\(1\\) \\(\\frac{SSR}{df_{SSR}}\\) \\(\\frac{\\frac{SSR}{df_{SSR}}}{s^2}\\) \\(1-F\\left(F_s,1,n-2\\right)\\) Fehler \\(SSE\\) \\(n-2\\) \\(\\frac{SSE}{df_{SSE}}=s^2\\) Gesamt \\(SSY\\) \\(n-1\\) Schauen wir uns zunächst die zweiten Spalte der Tabelle 10.3 an: \\(SSY=\\sum_{i=1}^{n}\\left(y_i-\\bar{y}\\right)^2\\) ist ein Maß für die Gesamtvarianz der Daten (in \\(y\\)-Richtung), d.h. wie stark die Datenpunkte um den Gesamtmittelwert streuen (Abbildung 10.3, links). \\(SSE=\\sum_{i=1}^{n}\\left(\\epsilon_i\\right)^2=\\sum_{i=1}^{n}\\left(y_i-\\left(\\beta_0+\\beta_1 \\cdot x_i\\right)\\right)^2\\) ist ein Maß für die Fehlervarianz, d.h. wie stark die Datenpunkte um die Regressionsgerade streuen (Abbildung 10.3, rechts). Das ist die Varianz, die nach der Modellanpassung übrig ist (nicht erklärt). \\(SSR=SSY-SSE\\) ist folglich ein Maß für die vom Modell erklärte Varianz. Abbildung 10.3: Variation der Datenpunkte um den Mittelwert, zusammengefasst durch \\(SSY\\) (links), und um die Regressionsgerade, zusammengefasst durch \\(SSE\\) (rechts). In der dritten Spalte der Tabelle 10.3 stehen die Freiheitsgrade der drei Varianzterme. Diese können als Anzahl der Werte in einer Stichprobe, die für die Berechnung der jeweiligen Parameter frei zur Verfügung stehen, verstanden werden (vgl. Kapitel 4): In die Berechnung von \\(SSY\\) geht \\(\\bar y\\) ein, für dessen Berechnung die Werte der Stichprobe bereits einmal verwendet wurden; dadurch ist die Anzahl Freiheitsgrade \\(df_{SSY}=n-1\\). In die Berechnung von \\(SSE\\) gehen \\(\\beta_0\\) und \\(\\beta_1\\) ein (Gleichung (10.2)), d.h. die Anzahl Freiheitsgrade ist \\(df_{SSE}=n-2\\). Für \\(SSR\\) gilt dann einfach \\(df_{SSR}=df_{SSY}-df_{SSE}=1\\). Die Freiheitsgrade werden verwendet, um die Varianzterme in der vierten Spalte der Tabelle 10.3 zu normalisieren, wobei \\(s^2\\) Fehlervarianz genannt wird. In der fünften Spalte der Tabelle 10.3 finden wir das Verhältnis von zwei Varianzen; Regressionsvarianz über Fehlervarianz. Von einer signifikanten Regression erwarten wir, dass die (durch das Modell erklärte) Regressionsvarianz viel größer ist als die (durch das Modell nicht erklärte) Fehlervarianz. Dies ist ein F-Test Problem, bei dem getestet wird, ob sich die durch das Modell erklärte Varianz signifikant von der durch das Modell nicht erklärten Varianz unterscheidet. Das Verhältnis der beiden Varianzen dient als F-Statistik \\(F_s\\) (vgl. Kapitel 9.4). Die sechste Spalte der Tabelle 10.3 gibt dann den p-Wert des F-Tests an, d.h. die Wahrscheinlichkeit, \\(F_s\\) oder einen größeren Wert (d.h. ein noch besseres Modell) zufällig zu erhalten, wenn die Nullhypothese \\(H_0\\) wahr ist (vgl. Kapitel 9.4). Im Fall der linearen Regression ist \\(H_0:\\frac{SSR}{df_{SSR}}=s^2\\), d.h. die beiden Varianzen sind gleich, und \\(H_1:\\frac{SSR}{df_{SSR}}&gt;s^2\\), d.h. die erklärte Varianz ist größer als die nicht erklärte. Wie in Kapitel 9.4 bereits diskutiert folgt \\(F_s\\) einer F-Verteilung unter der Nullhypothese, hier mit den Parametern \\(1\\) und \\(n-2\\) (Abbildung 10.4). Die blaue Linie in Abbildung 10.4 markiert einen bestimmten Wert von \\(F_s\\) (hier 8) und den entsprechenden Wert der Verteilungsfunktion der F-Verteilung (\\(F\\left(F_s,1,n-2\\right)\\)). Der p-Wert ist \\(\\Pr\\left(Z&gt; F_s\\right)=1-F\\left(F_s,1,n-2\\right)\\) und beschreibt die Wahrscheinlichkeit, dieses oder ein größeres Varianzverhältnis zufällig (aufgrund der zufälligen Stichprobenziehung) zu erhalten, selbst wenn die beiden Varianzen tatsächlich gleich sind. Abbildung 10.4: Verteilungsfunktion der F-Verteilung der F-Statistik \\(F_s\\). Blau: Bestimmter Wert für \\(F_s\\) und entsprechender Wert der Verteilungsfunktion. Für die Regression der Reisedaten ist der p-Wert wesentlich kleiner als das konventionelle Signifikanzniveau \\(\\alpha=0.01\\), daher lehnen wir die die Nullhypothese ab und bezeichnen die Regression als statistisch signifikant. Schauen wir uns die ANOVA-Tabelle fuer das Beispiel an: anova(reise_fit) ## Analysis of Variance Table ## ## Response: distanz ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## stationen 1 3309 3309 135 &lt;2e-16 *** ## Residuals 71 1745 25 ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Im Vergleich zu Tabelle 10.3 lässt R die letzte Zeile (\\(SSY\\)) weg und tauscht die Spalten Quadratsumme und Freiheitsgrad. 10.6 Konfidenzintervalle und Signifikanz der Parameter Da die Modellanpassung nicht perfekt ist, haben die Parameterschätzer Standardfehler, d.h. sie werden wie andere statistische Kennzahlen als Realisationen eines Zufallsprozesses interpretiert. Das führt uns zu Konfidenzintervallen und t-Tests auf Signifikanz der einzelnen Parameter. Der Standardfehlern für \\(\\hat\\beta_0\\) ist: \\[\\begin{equation} s_{\\hat\\beta_0}=\\sqrt{\\frac{\\sum_{i=1}^{n}x_i^2}{n} \\cdot \\frac{s^2}{SSX}} \\tag{10.14} \\end{equation}\\] Wenn wir diese Formel in ihre einzelnen Teile zerlegen, sehen wir: Je mehr Datenpunkte \\(n\\) wir haben, desto kleiner ist der Standardfehler, d.h. desto mehr Vertrauen haben wir in die Schätzung. Außerdem gilt, je größer die Variation in \\(x\\) (\\(SSX\\)), desto kleiner der Standardfehler. Beide Effekte machen intuitiv Sinn: Je mehr Datenpunkte wir haben und je mehr Ausprägungen von \\(x\\) wir abgedeckt haben, desto sicherer können wir sein, dass unsere Stichprobe repräsentativ für die Grundgesamtheit ist. Umgekehrt gilt: Je größer die Fehlervarianz \\(s^2\\), d.h. je kleiner die Erklärungskraft unseres Modells, desto größer der Standardfehler. Und je mehr \\(x\\)-Datenpunkte von Null entfernt sind, d.h. je größer \\(\\sum_{i=1}^{n}x_i^2\\), desto geringer ist unser Vertrauen in den Achsenabschnitt (wo \\(x=0\\) ist) und damit steigt der Standardfehler. Der Standardfehler für \\(\\hat\\beta_1\\) ist: \\[\\begin{equation} s_{\\hat\\beta_1}=\\sqrt{\\frac{s^2}{SSX}} \\tag{10.15} \\end{equation}\\] Hier gilt die gleiche Interpretation wie zuvor, außer dass es keinen Einfluss der Größe der \\(x\\)-Datenpunkte gibt. Wir können auch einen Standardfehler für neue Vorhersagen \\(\\hat y\\) für gegebene \\(\\hat x\\) festlegen: \\[\\begin{equation} s_{\\hat y}=\\sqrt{s^2 \\cdot \\left(\\frac{1}{n}+\\frac{\\left(\\hat x-\\bar x\\right)^2}{SSX}\\right)} \\tag{10.16} \\end{equation}\\] Dieselbe Interpretation gilt auch hier, nur dass jetzt ein zusätzlicher Term \\(\\left(\\hat x-\\bar x\\right)^2\\) auftaucht, der besagt, je weiter der neue \\(x\\)-Wert vom Zentrum der ursprünglichen Daten (den Trainings- oder Kalibrierungsdaten) entfernt ist, desto größer ist der Standardfehler der neuen Vorhersage, d.h. desto geringer ist die Zuversicht, dass sie korrekt ist. Anmerkung: Die Formeln für die Standardfehler ergeben sich aus den grundlegenden Annahmen der linearen Regression, auf die wir weiter unten eingehen werden. Die mathematische Herleitung lassen wir hier aus. Aus den Standardfehlern können wir Konfidenzintervalle für die Parameterschätzer wie folgt berechnen (vgl. Konfidenzintervall des Mittelwertschätzers (Kapitel 8)): \\[\\begin{equation} \\Pr\\left(\\hat\\beta_0-t_{n-2;0.975} \\cdot s_{\\hat\\beta_0}\\leq \\beta_0\\leq \\hat\\beta_0+t_{n-2;0.975} \\cdot s_{\\hat\\beta_0}\\right)=0.95 \\tag{10.17} \\end{equation}\\] Gleichung (10.17) ist das zentrale 95%-Konfidenzintervall, in dem der wahre Parameterwert, hier \\(\\beta_0\\), mit einer Wahrscheinlichkeit von 0.95 liegt.24 Wir können das Intervall auch wie folgt schreiben: \\[\\begin{equation} KI=\\left[\\hat\\beta_0-t_{n-2;0.975} \\cdot s_{\\hat\\beta_0};\\hat\\beta_0+t_{n-2;0.975} \\cdot s_{\\hat\\beta_0}\\right] \\tag{10.18} \\end{equation}\\] Wie bei dem Konfidenzintervall des Mittelwertschätzers (Kapitel 8) liegt das Konfidenzintervall symmetrisch um den Parameterschätzwert \\(\\hat\\beta_0\\) und ergibt sich aus einer t-Verteilung mit dem Parameter \\(n-2\\), deren Breite durch den Standardfehler \\(s_{\\hat\\beta_0}\\) moduliert wird. Erinnern Sie sich, dass die Breite der t-Verteilung ebenfalls durch den Stichprobenumfang kontrolliert wird und mit zunehmendem \\(n\\) immer schmaler wird. Die gleichen Formeln gelten für \\(\\beta_1\\) und \\(y\\): \\[\\begin{equation} \\Pr\\left(\\hat\\beta_1-t_{n-2;0.975} \\cdot s_{\\hat\\beta_1}\\leq \\beta_1\\leq \\hat\\beta_1+t_{n-2;0.975} \\cdot s_{\\hat\\beta_1}\\right)=0.95 \\tag{10.19} \\end{equation}\\] \\[\\begin{equation} \\Pr\\left(\\hat y-t_{n-2;0.975} \\cdot s_{\\hat y}\\leq y\\leq \\hat y+t_{n-2;0.975} \\cdot s_{\\hat y}\\right)=0.95 \\tag{10.20} \\end{equation}\\] Die Formeln für die Konfidenzintervalle (Gleichungen (10.17), (10.19) und (10.20)) ergeben sich aus den Grundannahmen der linearen Regression (vgl. Kapitel 8): Die Residuen sind unabhängig identisch verteilt (u.i.v.) gemäß einer Normalverteilung, d.h. \\(\\epsilon_i\\sim N(0,\\sigma)\\), und das lineare Modell ist korrekt. Dann lässt sich mathematisch zeigen, dass \\(\\frac{\\hat\\beta_0-\\beta_0}{s_{\\hat\\beta_0}}\\), \\(\\frac{\\hat\\beta_1-\\beta_1}{s_{\\hat\\beta_1}}\\) und \\(\\frac{\\hat y-y}{s_{\\hat y}}\\) \\(t_{n-2}\\)-verteilt sind (t-Verteilung mit \\(n-2\\) Freiheitsgraden). Da das zentrale 95%-Konfidenzintervall einer \\(t_{n-2}\\)-verteilten Zufallsvariablen \\(Z\\) \\(\\Pr\\left(-t_{n-2;0.975}\\leq Z\\leq t_{n-2;0. 975}\\right)=0.95\\) ist (Abbildung 8.2), können wir jeden der oben genannten drei Terme für \\(Z\\) einsetzen und die Ungleichung umstellen, um zu den Gleichungen (10.17), (10.19) und (10.20) zu gelangen. Die Signifikanz der Parameterschätzer wird mit Hilfe eines t-Tests ermittelt (vgl. Kapitel 9.2). Die Nullhypothese ist, dass die wahren Parameterwerte gleich Null sind, d.h. die Parameterschätzer nicht signifikant sind: \\[\\begin{equation} H_0:\\beta_0=0 \\tag{10.21} \\end{equation}\\] \\[\\begin{equation} H_0:\\beta_1=0 \\tag{10.22} \\end{equation}\\] Diese Hypothese wird gegen die Alternativhypothese getestet, dass die wahren Parameterwerte ungleich Null sind, d.h. dass die Parameterschätzer signifikant sind: \\[\\begin{equation} H_1:\\beta_0\\neq 0 \\tag{10.23} \\end{equation}\\] \\[\\begin{equation} H_1:\\beta_1\\neq 0 \\tag{10.24} \\end{equation}\\] Die Teststatistiken sind: \\[\\begin{equation} t_s=\\frac{\\hat\\beta_0-0}{s_{\\hat\\beta_0}}\\sim t_{n-2} \\tag{10.25} \\end{equation}\\] \\[\\begin{equation} t_s=\\frac{\\hat\\beta_1-0}{s_{\\hat\\beta_1}}\\sim t_{n-2} \\tag{10.26} \\end{equation}\\] Die t-Verteilungen der Teststatistiken ergeben sich wiederum aus den oben erwähnten Regressionsannahmen. Die Annahmen sind die gleichen wie beim üblichen t-Test der Mittelwerte (Kapitel 9.2), außer dass im Fall der linearen Regression die Residuen als u.i.v. normal angenommen werden, während im Fall der Mittelwerte die tatsächlichen Datenpunkte \\(y\\) als u.i.v. normal angenommen werden. Analog zum üblichen 2-seitigen t-Test ist der p-Wert definiert als: \\[\\begin{equation} 2 \\cdot \\Pr\\left(t&gt;|t_s|\\right)=2 \\cdot \\left(1-F_t\\left(|t_s|\\right)\\right) \\tag{10.27} \\end{equation}\\] Mit einem konventionellen Signifikanzniveau von \\(\\alpha=0.01\\) gelangen wir zu einem kritischen Wert der Teststatistik \\(t_c=t_{n-2;0.995}\\), bei dessen Überschreitung, bzw. Unterschreitung von \\(-t_c\\), wir die Nullhypothese ablehnen und die Parameterschätzer als signifikant bezeichnen. Jetzt verstehen wir auch die restlichen Informationen des Outputs der lm() Funktion (s. oben): coef(summary(reise_fit)) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.9232 1.01874 4.833 7.533e-06 ## stationen 0.6934 0.05976 11.604 4.641e-18 Wie bereits oben erwähnt steht die Zeile (Intercept) für \\(\\beta_0\\) und die Zeile stationen für \\(\\beta_1\\). In Spalte Estimate stehen die Werte der Parameterschätzer. In Spalte Std. Error stehen deren Standardfehler (Gleichungen (10.14) und (10.15)). In Spalte t value stehen die entsprechenden Werte der Teststatistik (Gleichungen (10.25) und (10.26)). In Spalte Pr(&gt;|t|) stehen die p-Werte der t-Tests auf Signifikanz der Parameter (Gleichung (10.27)). Wir sehen, dass in unserem Beispiel beide Parameter signifikant sind (die p-Werte sind wesentlich kleiner als das konventionelle \\(\\alpha=0.01\\)). 10.7 Güte der Modellanpassung Die Signifikanz der Parameter der Regression ist eine Sache. Wie gut aber ist das Modell im Beschreiben der Daten? D.h. wieviel von der Varianz in den Daten wird vom Modell erklärt? Die Güte der Modellanpassung kann in erster Linie mit dem Bestimmtheitsmaß (\\(r^2\\)) begutachtet werden, welches als Anteil der Varianz (in \\(y\\)-Richtung) definiert ist, der durch das Modell erklärt wird: \\[\\begin{equation} r^2=\\frac{SSY-SSE}{SSY}=1-\\frac{SSE}{SSY} \\tag{10.28} \\end{equation}\\] Das Bestimmtheitsmaß ist der Korrelationskoeffizient nach Bravais-Pearson zum Quadrat (vgl. Kapitel 5). Wie wir an Gleichung (10.28) sehen, wenn das Modell nicht mehr Variation als die Gesamtvariation um den Mittelwert erklärt, d.h. \\(SSE=SSY\\) ist, dann ist \\(r^2=0\\). Umgekehrt, wenn das Modell perfekt zu den Daten passt, d.h. \\(SSE=0\\) ist, dann ist \\(r^2=1\\). Werte dazwischen stellen unterschiedliche Grade der Anpassungsgüte dar. Das kann wiederum mit Abbildung 10.3 veranschaulicht werden, wobei der linke Teil \\(SSY\\) und der rechte Teil \\(SSE\\) verdeutlicht. Wenn es darum geht, Modelle unterschiedlicher Komplexität (d.h. mit mehr oder weniger Parametern) mit \\(r^2\\) zu vergleichen, dann ist es sinnvoll, das Bestimmtheitsmaß mit der Anzahl der Modellparameter zu korrigieren, da komplexere Modelle (mehr Parameter) automatisch zu besseren Anpassungen führen, einfach aufgrund der größeren Freiheitsgrade, die komplexere Modelle bei der Anpassung der Daten haben. Dies führt zum korrigierten \\(r^2\\): \\[\\begin{equation} \\bar r^2=1-\\frac{\\frac{SSE}{df_{SSE}}}{\\frac{SSY}{df_{SSY}}}=1-\\frac{SSE}{SSY} \\cdot \\frac{df_{SSY}}{df_{SSE}} \\tag{10.29} \\end{equation}\\] Rechnen wir \\(r^2\\) und \\(\\bar r^2\\) für die Regression der Reisedaten aus: summary(reise_fit)$r.squared ## [1] 0.6548 summary(reise_fit)$adj.r.squared ## [1] 0.6499 D.h. rund 65% der Varianz in den Entfernungs-Daten wird durch das lineare Modell mit Anzahl Stationen als Prädiktor erklärt. Aber was heißt Güte der Modellanpassung? Sind alle Modellannahmen erfüllt? Folgende Annahmen ergeben sich aus der Maximum-Likelihood-Theorie (vgl. Schätzen von Verteilungsparametern, Kapitel 8): Die Residuen sind unabhängig, in diesem Fall gibt es keine serielle Korrelation in der Residuengrafik - dies kann mit dem Durbin-Watson-Test getestet werden Die Residuen sind normalverteilt - dies kann visuell mit Hilfe des Quantil-Quantil-Diagramms (QQ-Plot) und dem Residuen-Histogramm beurteilt werden, und kann mit dem Kolmogorov-Smirnov-Test (Kapitel 9.5) und dem Shapiro-Wilk-Test getestet werden Die Varianz ist für alle Residuen konstant (die Residuen sind homoskedastisch), d.h. es erfolgt kein Auffächern der Residuen Sind diese Annahmen nicht erfüllt, können wir auf Datentransformation, gewichtete Regression oder Generalisierte Lineare Modelle zurückgreifen. Letzteres ist wird im Master Global Change Geography unterrichtet. Eine erste nützliche diagnostische Darstellung ist die der Residuen in Serie, d.h. nach Index \\(i\\), um zu sehen, ob es ein Muster aufgrund des Datenerfassungsprozesses gibt. Für unser Beispiel: # Residuen gegen Index plotten plot(residuals(reise_fit), pch = 19, type = &#39;p&#39;, ylim = c(-25,25)) abline(h = 0, lwd = 3, col = &quot;red&quot;) Dieser Datensatz zeigt kein erkennbares Muster, was für eine Unabhängigkeit der Residuen spricht. Wir sollten auch die Residuen nach dem modellierten Wert von \\(y\\) plotten, um zu sehen, ob es ein Muster als Funktion der Größenordnung von \\(y\\) gibt: # Residuen gegen modellierte Werte von y plotten plot(fitted.values(reise_fit),residuals(reise_fit), pch = 19, type = &#39;p&#39;, ylim = c(-25,25)) abline(h = 0, lwd = 3, col = &quot;red&quot;) Diese Grafik zeigt eine leichte Zunahme der Residuenvarianz von links nach rechts, d.h. in Richtung größerer Werte - eine Form der Heteroskedastizität. Die Annahme, dass die Residuen normalverteilt sind, kann anhand des QQ-Plots beurteilt werden (vgl. Kapitel 8.4): qqnorm(residuals(reise_fit)) qqline(residuals(reise_fit)) In unserem Beispiel deutet der QQ-Plot darauf hin, dass die Flanken der Residuenverteilung flacher als bei einer Normalverteilung abfallen, vermutlich aufgrund der o.g. Heteroskedastizität. Das können wir beim Histogramm der Residuen nicht so gut sehen da die Vergleichsverteilung fehlt: # Histogramm der Residuen plotten hist(residuals(reise_fit), xlim = c(-25,25)) Die Heteroskedastizität würde man angehen, indem man mit einem Generalisierten Linearen Modell eine andere Residuenverteilung als die Normalverteilung annimmt. Das führt aber im Rahmen dieses Kurses zu weit. Es bleibt somit festzuhalten, dass das Regressionsproblem mit den hier behandelten Methoden noch nicht vollständig gelöst ist. Literatur "],["11-refs.html", "Literatur", " Literatur "]]
